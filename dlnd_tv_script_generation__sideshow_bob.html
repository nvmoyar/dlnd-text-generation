<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />
<title>dlnd_tv_script_generation</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    color: #000 !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.2.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.2.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.2.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.2.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.2.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.2.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=1);
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2);
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=3);
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1);
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1);
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
@media (max-width: 991px) {
  #ipython_notebook {
    margin-left: 10px;
  }
}
[dir="rtl"] #ipython_notebook {
  float: right !important;
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#login_widget {
  float: right;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  text-align: center;
  vertical-align: middle;
  display: inline;
  opacity: 0;
  z-index: 2;
  width: 12ex;
  margin-right: -12ex;
}
.alternate_upload .btn-upload {
  height: 22px;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
[dir="rtl"] #tabs li {
  float: right;
}
ul#tabs {
  margin-bottom: 4px;
}
[dir="rtl"] ul#tabs {
  margin-right: 0px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
[dir="rtl"] .list_toolbar .tree-buttons {
  float: left !important;
}
[dir="rtl"] .list_toolbar .pull-right {
  padding-top: 1px;
  float: left !important;
}
[dir="rtl"] .list_toolbar .pull-left {
  float: right !important;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: baseline;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
#tree-selector {
  padding-right: 0px;
}
[dir="rtl"] #tree-selector a {
  float: right;
}
#button-select-all {
  min-width: 50px;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
[dir="rtl"] #new-menu {
  text-align: right;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
[dir="rtl"] #running .col-sm-8 {
  float: right !important;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul {
  list-style: disc;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ul ul {
  list-style: square;
  margin: 0em 2em;
}
.rendered_html ul ul ul {
  list-style: circle;
  margin: 0em 2em;
}
.rendered_html ol {
  list-style: decimal;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
  margin: 0em 2em;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  background-color: #fff;
  color: #000;
  font-size: 100%;
  padding: 0px;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: 1px solid black;
  border-collapse: collapse;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  border: 1px solid black;
  border-collapse: collapse;
  margin: 1em 2em;
}
.rendered_html td,
.rendered_html th {
  text-align: left;
  vertical-align: middle;
  padding: 4px;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget {
  float: right !important;
  float: right;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 21ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  margin-top: 6px;
}
span.save_widget span.filename {
  height: 1em;
  line-height: 1em;
  padding: 3px;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  display: none;
}
.command-shortcut:before {
  content: "(command)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  width: 100%;
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal .xterm-rows {
  padding: 10px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>
<style type="text/css">
    
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}

@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="TV-Script-Generation">TV Script Generation<a class="anchor-link" href="#TV-Script-Generation">&#182;</a></h1><p>In this project, you'll generate your own <a href="https://en.wikipedia.org/wiki/The_Simpsons">Simpsons</a> TV scripts using RNNs.  You'll be using part of the <a href="https://www.kaggle.com/wcukierski/the-simpsons-by-the-data">Simpsons dataset</a> of scripts from 27 seasons.  The Neural Network you'll build will generate a new TV script for a scene at <a href="https://simpsonswiki.com/wiki/Moe&#39;s_Tavern">Moe's Tavern</a>.</p>
<h2 id="Get-the-Data">Get the Data<a class="anchor-link" href="#Get-the-Data">&#182;</a></h2><p>The data is already provided for you.  You'll be using a subset of the original dataset.  It consists of only the scenes in Moe's Tavern.  This doesn't include other versions of the tavern, like "Moe's Cavern", "Flaming Moe's", "Uncle Moe's Family Feed-Bag", etc..</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">helper</span>

<span class="n">data_dir</span> <span class="o">=</span> <span class="s1">&#39;./data/simpsons/dataset.txt&#39;</span>
<span class="c1"># data_dir = &#39;./data/simpsons/moes_tavern_lines.txt&#39;</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">data_dir</span><span class="p">)</span>
<span class="c1"># Ignore notice, since we don&#39;t use it for analysing the data</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="p">[</span><span class="mi">81</span><span class="p">:]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Explore-the-Data">Explore the Data<a class="anchor-link" href="#Explore-the-Data">&#182;</a></h2><p>Play around with <code>view_sentence_range</code> to view different parts of the data.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">view_sentence_range</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Dataset Stats&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Roughly the number of unique words: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">({</span><span class="n">word</span><span class="p">:</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()})))</span>

<span class="n">scenes</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of scenes: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">scenes</span><span class="p">)))</span>

<span class="n">sentence_count_scene</span> <span class="o">=</span> <span class="p">[</span><span class="n">scene</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">scene</span> <span class="ow">in</span> <span class="n">scenes</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Average number of sentences in each scene: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">sentence_count_scene</span><span class="p">)))</span>

<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentence</span> <span class="k">for</span> <span class="n">scene</span> <span class="ow">in</span> <span class="n">scenes</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">scene</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of lines: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">)))</span>

<span class="n">word_count_sentence</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Average number of words in each line: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">word_count_sentence</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The sentences </span><span class="si">{}</span><span class="s1"> to </span><span class="si">{}</span><span class="s1">:&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">*</span><span class="n">view_sentence_range</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)[</span><span class="n">view_sentence_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span><span class="n">view_sentence_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Dataset Stats
Roughly the number of unique words: 136109
Number of scenes: 568
Average number of sentences in each scene: 278.6021126760563
Number of lines: 158814
Average number of words in each line: 10.462925182918383

The sentences 0 to 50:
n:  Ooo, careful, Homer.
Homer_Simpson:  There&#39;s no time to be careful.
Homer_Simpson:  We&#39;re late.
Springfield_Elementary_School:  Ext. springfield elementary school - establishing - night)
Auditorium:  int. auditorium - night)
Marge_Simpson:  (HUSHED VOICE) Sorry, Excuse us. Pardon me...
Homer_Simpson:  (SIMULTANEOUSLY) Hey, Norman. How&#39;s it going? So you got dragged down here, too... heh, heh. How ya doing, Fred? Excuse me, Fred.
Homer_Simpson:  Pardon my galoshes. (CHUCKLES)
Seymour_Skinner:  (UNREHEARSED) Wasn&#39;t that wonderful? And now, &#34;Santas of Many Lands,&#34; as presented by the entire second grade class.
Marge_Simpson:  Oh... Lisa&#39;s class.
JANEY:  (SHY AND NERVOUS) Frohlich weihnachten -- that&#39;s German for Merry Christmas. In Germany, Santa&#39;s servant Ruprecht gives presents to good children and whipping rods to the parents of bad ones.
Todd_Flanders:  Meri Kurimasu. I am Hotseiosha, a Japanese priest who acts like Santa Claus. I have eyes in the back of my head so children better behave when I&#39;m nearby.
Dewey_Largo:  And now, presenting Lisa Simpson, as Tawanga, the Santa Claus of the South Seas.
Homer_Simpson:  Oh, it&#39;s Lisa. That&#39;s ours.
Seymour_Skinner:  The fourth grade will now favor us with a melody, medley of holiday flavorites.
Children:  (SING) DASHING THROUGH THE SNOW / IN A ONE-HORSE OPEN SLEIGH / O&#39;ER THE FIELDS WE GO / LAUGHING ALL THE WAY... HA HA HA... BELLS ON BOB-TAIL RING / MAKING SPIRITS BRIGHT / WHAT FUN IT IS TO RIDE AND SING THIS SLEIGHING SONG TONIGHT!
Marge_Simpson:  (WHISPERS) Isn&#39;t Bart sweet, Homer? He sings like a little angel.
Bart_Simpson:  (SINGS) &#34;JINGLE BELLS, BATMAN SMELLS, ROBIN LAID AN EGG / THE BATMOBILE BROKE ITS WHEEL, THE JOKER GOT AWAY.&#34;
Seymour_Skinner:  The fifth grade will now favor us with a scene from Charles Dickens&#39; &#34;A Christmas Carol&#34;.
Homer_Simpson:  (GROANS) How many grades does this school have?
Simpson_Home:  int. simpson house - living room - evening)
Marge_Simpson:  (READS) &#34;Dear friends of the Simpson family
Homer_Simpson:  (MAD) Marge, haven&#39;t you finished that stupid letter yet?
Marge_Simpson:  (CONTINUES) &#34;...Homer sends his love. Happy Holidays.&#34;
Homer_Simpson:  Marge!
Marge_Simpson:  (CONTINUES) &#34;The Simpsons.&#34;
Homer_Simpson:  (MAD) Marge, where&#39;s the extension cord?
Marge_Simpson:  Oh, for heavens sakes, Homer. It&#39;s in the utility drawer.
Homer_Simpson:  Sorry. I&#39;m just a big kid and I love Christmas so much.
Homer_Simpson:  (GRUMBLING) ...Every year.
Marge_Simpson:  All right, children. Let me have those letters. I&#39;ll send them to Santa&#39;s workshop at the North Pole.
Bart_Simpson:  Oh, please. (TO LISA) There&#39;s only one fat guy that brings us presents and his name ain&#39;t Santa.
Marge_Simpson:  A pony? Oh, Lisa. You&#39;ve asked for that for the last three years and I keep telling you Santa can&#39;t fit a pony into his sleigh. Can&#39;t you take a hint?
Lisa_Simpson:  But I really want a pony and I&#39;ve been really, really good this year.
Marge_Simpson:  (MURMURS) Oh, dear. Maybe Bart is a little more realistic.
Marge_Simpson:  (SHOCKED) A tattoo!
Homer_Simpson:  A what!
Bart_Simpson:  Yeah, they&#39;re cool and they last the rest of the your life.
Marge_Simpson:  (FIRM) You will not be getting a tattoo for Christmas.
Homer_Simpson:  Yeah. If you want one, you&#39;ll have to pay for it out of your own allowance.
Bart_Simpson:  All right!
Marge_Simpson:  Homer!
Homer_Simpson:  (into phone) Y&#39;ello.
Patty_Bouvier:  Marge, please.
Homer_Simpson:  (into phone) Who&#39;s this?
Patty_Bouvier:  May I please speak to Marge?
Homer_Simpson:  (into phone) This is her sister, isn&#39;t it?
Patty_Bouvier:  Is Marge there?
Homer_Simpson:  (into phone, GROWING MAD) Who shall I say is calling?
Patty_Bouvier:  Marge, please.
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Implement-Preprocessing-Functions">Implement Preprocessing Functions<a class="anchor-link" href="#Implement-Preprocessing-Functions">&#182;</a></h2><p>The first thing to do to any dataset is preprocessing.  Implement the following preprocessing functions below:</p>
<ul>
<li>Lookup Table</li>
<li>Tokenize Punctuation</li>
</ul>
<h3 id="Lookup-Table">Lookup Table<a class="anchor-link" href="#Lookup-Table">&#182;</a></h3><p>To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:</p>
<ul>
<li>Dictionary to go from the words to an id, we'll call <code>vocab_to_int</code></li>
<li>Dictionary to go from the id to word, we'll call <code>int_to_vocab</code></li>
</ul>
<p>Return these dictionaries in the following tuple <code>(vocab_to_int, int_to_vocab)</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">problem_unittests</span> <span class="k">as</span> <span class="nn">tests</span>

<span class="k">def</span> <span class="nf">create_lookup_tables</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create lookup tables for vocabulary</span>
<span class="sd">    :param text: The text of tv scripts split into words</span>
<span class="sd">    :return: A tuple of dicts (vocab_to_int, int_to_vocab)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
    <span class="c1"># print((text[:5])) #[&#39;moe_szyslak&#39;, &quot;moe&#39;s&quot;, &#39;tavern&#39;, &#39;where&#39;, &#39;the&#39;]</span>
    <span class="c1"># print(len(text)) # --&gt; 104 words</span>
    
    <span class="n">vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">vocab_to_int</span> <span class="o">=</span> <span class="p">{</span><span class="n">c</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
    <span class="n">int_to_vocab</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">vocab_to_int</span><span class="p">,</span> <span class="n">int_to_vocab</span>   

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_create_lookup_tables</span><span class="p">(</span><span class="n">create_lookup_tables</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Tokenize-Punctuation">Tokenize Punctuation<a class="anchor-link" href="#Tokenize-Punctuation">&#182;</a></h3><p>We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word "bye" and "bye!".</p>
<p>Implement the function <code>token_lookup</code> to return a dict that will be used to tokenize symbols like "!" into "||Exclamation_Mark||".  Create a dictionary for the following symbols where the symbol is the key and value is the token:</p>
<ul>
<li>Period ( . )</li>
<li>Comma ( , )</li>
<li>Quotation Mark ( " )</li>
<li>Semicolon ( ; )</li>
<li>Exclamation mark ( ! )</li>
<li>Question mark ( ? )</li>
<li>Left Parentheses ( ( )</li>
<li>Right Parentheses ( ) )</li>
<li>Dash ( -- )</li>
<li>Return ( \n )</li>
</ul>
<p>This dictionary will be used to token the symbols and add the delimiter (space) around it.  This separates the symbols as it's own word, making it easier for the neural network to predict on the next word. Make sure you don't use a token that could be confused as a word. Instead of using the token "dash", try using something like "||dash||".</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">token_lookup</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate a dict to turn punctuation into a token.</span>
<span class="sd">    :return: Tokenize dictionary where the key is the punctuation and the value is the token</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
        
    <span class="n">values</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;||Return||&#39;</span><span class="p">,</span> <span class="s1">&#39;||Exclamation_Mark||&#39;</span><span class="p">,</span> <span class="s1">&#39;||Quotation_Mark||&#39;</span><span class="p">,</span> <span class="s1">&#39;||Left_Parentheses||&#39;</span><span class="p">,</span> <span class="s1">&#39;||Right_Parentheses||&#39;</span><span class="p">,</span> <span class="s1">&#39;||Comma||&#39;</span><span class="p">,</span> <span class="s1">&#39;||Dash||&#39;</span><span class="p">,</span> <span class="s1">&#39;||Period||&#39;</span><span class="p">,</span> <span class="s1">&#39;||Semicolon||&#39;</span><span class="p">,</span> <span class="s1">&#39;||Question_Mark||&#39;</span><span class="p">}</span>
    
    <span class="n">keys</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="s1">&#39;&quot;&#39;</span><span class="p">,</span> <span class="s1">&#39;;&#39;</span><span class="p">,</span> <span class="s1">&#39;!&#39;</span><span class="p">,</span> <span class="s1">&#39;?&#39;</span><span class="p">,</span> <span class="s1">&#39;(&#39;</span><span class="p">,</span> <span class="s1">&#39;)&#39;</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">}</span>
     
    <span class="c1"># print(keys)</span>
        
    <span class="n">keys</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>
    
    <span class="c1"># print(values)</span>
    <span class="c1"># print(keys)</span>
   
    <span class="n">dict_punctuation</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">))</span>

    <span class="c1"># print(dict_punctuation)</span>
             
    <span class="k">return</span> <span class="n">dict_punctuation</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_tokenize</span><span class="p">(</span><span class="n">token_lookup</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Preprocess-all-the-data-and-save-it">Preprocess all the data and save it<a class="anchor-link" href="#Preprocess-all-the-data-and-save-it">&#182;</a></h2><p>Running the code cell below will preprocess all the data and save it to file.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="c1"># Preprocess Training, Validation, and Testing Data</span>
<span class="n">helper</span><span class="o">.</span><span class="n">preprocess_and_save_data</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">token_lookup</span><span class="p">,</span> <span class="n">create_lookup_tables</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Check-Point">Check Point<a class="anchor-link" href="#Check-Point">&#182;</a></h1><p>This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[20]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">helper</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">problem_unittests</span> <span class="k">as</span> <span class="nn">tests</span>

<span class="n">int_text</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">,</span> <span class="n">int_to_vocab</span><span class="p">,</span> <span class="n">token_dict</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">load_preprocess</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Build-the-Neural-Network">Build the Neural Network<a class="anchor-link" href="#Build-the-Neural-Network">&#182;</a></h2><p>You'll build the components necessary to build a RNN by implementing the following functions below:</p>
<ul>
<li>get_inputs</li>
<li>get_init_cell</li>
<li>get_embed</li>
<li>build_rnn</li>
<li>build_nn</li>
<li>get_batches</li>
</ul>
<h3 id="Check-the-Version-of-TensorFlow-and-Access-to-GPU">Check the Version of TensorFlow and Access to GPU<a class="anchor-link" href="#Check-the-Version-of-TensorFlow-and-Access-to-GPU">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">distutils.version</span> <span class="k">import</span> <span class="n">LooseVersion</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Check TensorFlow Version</span>
<span class="k">assert</span> <span class="n">LooseVersion</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">LooseVersion</span><span class="p">(</span><span class="s1">&#39;1.0&#39;</span><span class="p">),</span> <span class="s1">&#39;Please use TensorFlow version 1.0 or newer&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;TensorFlow Version: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>

<span class="c1"># Check for a GPU</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">tf</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">gpu_device_name</span><span class="p">():</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;No GPU found. Please use a GPU to train your neural network.&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Default GPU Device: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">gpu_device_name</span><span class="p">()))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>TensorFlow Version: 1.0.0
Default GPU Device: /gpu:0
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Input">Input<a class="anchor-link" href="#Input">&#182;</a></h3><p>Implement the <code>get_inputs()</code> function to create TF Placeholders for the Neural Network.  It should create the following placeholders:</p>
<ul>
<li>Input text placeholder named "input" using the <a href="https://www.tensorflow.org/api_docs/python/tf/placeholder">TF Placeholder</a> <code>name</code> parameter.</li>
<li>Targets placeholder</li>
<li>Learning Rate placeholder</li>
</ul>
<p>Return the placeholders in the following the tuple <code>(Input, Targets, LearingRate)</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[22]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_inputs</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create TF Placeholders for input, targets, and learning rate.</span>
<span class="sd">    :return: Tuple (input, targets, learning rate)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
    
    <span class="nb">input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input&#39;</span><span class="p">)</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;targets&#39;</span><span class="p">)</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="nb">input</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">learning_rate</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_get_inputs</span><span class="p">(</span><span class="n">get_inputs</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Build-RNN-Cell-and-Initialize">Build RNN Cell and Initialize<a class="anchor-link" href="#Build-RNN-Cell-and-Initialize">&#182;</a></h3><p>Stack one or more <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell"><code>BasicLSTMCells</code></a> in a <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell"><code>MultiRNNCell</code></a>.</p>
<ul>
<li>The Rnn size should be set using <code>rnn_size</code></li>
<li>Initalize Cell State using the MultiRNNCell's <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell#zero_state"><code>zero_state()</code></a> function<ul>
<li>Apply the name "initial_state" to the initial state using <a href="https://www.tensorflow.org/api_docs/python/tf/identity"><code>tf.identity()</code></a></li>
</ul>
</li>
</ul>
<p>Return the cell and initial state in the following tuple <code>(Cell, InitialState)</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[23]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_init_cell</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">rnn_size</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create an RNN Cell and initialize it.</span>
<span class="sd">    :param batch_size: Size of batches</span>
<span class="sd">    :param rnn_size: Size of RNNs</span>
<span class="sd">    :return: Tuple (cell, initialize state)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
    
    <span class="n">n_lstm_cell</span> <span class="o">=</span> <span class="mi">2</span>
    
    <span class="n">LSTM</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="n">rnn_size</span><span class="p">)</span>
    <span class="n">Cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">MultiRNNCell</span><span class="p">([</span><span class="n">LSTM</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_lstm_cell</span><span class="p">)</span>
    <span class="n">InitialState</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">Cell</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;initial_state&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Cell</span><span class="p">,</span> <span class="n">InitialState</span>
   

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_get_init_cell</span><span class="p">(</span><span class="n">get_init_cell</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Word-Embedding">Word Embedding<a class="anchor-link" href="#Word-Embedding">&#182;</a></h3><p>Apply embedding to <code>input_data</code> using TensorFlow.  Return the embedded sequence.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[24]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_embed</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create embedding for &lt;input_data&gt;.</span>
<span class="sd">    :param input_data: TF placeholder for text input.</span>
<span class="sd">    :param vocab_size: Number of words in vocabulary.</span>
<span class="sd">    :param embed_dim: Number of embedding dimensions</span>
<span class="sd">    :return: Embedded input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
    
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">input_data</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">embed</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_get_embed</span><span class="p">(</span><span class="n">get_embed</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Build-RNN">Build RNN<a class="anchor-link" href="#Build-RNN">&#182;</a></h3><p>You created a RNN Cell in the <code>get_init_cell()</code> function.  Time to use the cell to create a RNN.</p>
<ul>
<li>Build the RNN using the <a href="https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"><code>tf.nn.dynamic_rnn()</code></a><ul>
<li>Apply the name "final_state" to the final state using <a href="https://www.tensorflow.org/api_docs/python/tf/identity"><code>tf.identity()</code></a></li>
</ul>
</li>
</ul>
<p>Return the outputs and final_state state in the following tuple <code>(Outputs, FinalState)</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[25]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">build_rnn</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a RNN using a RNN Cell</span>
<span class="sd">    :param cell: RNN Cell</span>
<span class="sd">    :param inputs: Input text data</span>
<span class="sd">    :return: Tuple (Outputs, Final State)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
      
    <span class="n">outputs</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dynamic_rnn</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">final_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">final_state</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;final_state&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">final_state</span>
  

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_build_rnn</span><span class="p">(</span><span class="n">build_rnn</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Build-the-Neural-Network">Build the Neural Network<a class="anchor-link" href="#Build-the-Neural-Network">&#182;</a></h3><p>Apply the functions you implemented above to:</p>
<ul>
<li>Apply embedding to <code>input_data</code> using your <code>get_embed(input_data, vocab_size, embed_dim)</code> function.</li>
<li>Build RNN using <code>cell</code> and your <code>build_rnn(cell, inputs)</code> function.</li>
<li>Apply a fully connected layer with a linear activation and <code>vocab_size</code> as the number of outputs.</li>
</ul>
<p>Return the logits and final state in the following tuple (Logits, FinalState)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[26]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">build_nn</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">rnn_size</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Build part of the neural network</span>
<span class="sd">    :param cell: RNN cell</span>
<span class="sd">    :param rnn_size: Size of rnns</span>
<span class="sd">    :param input_data: Input data</span>
<span class="sd">    :param vocab_size: Vocabulary size</span>
<span class="sd">    :return: Tuple (Logits, FinalState)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
    
    <span class="n">embed</span> <span class="o">=</span> <span class="n">get_embed</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">rnn_size</span><span class="p">)</span>
    
    <span class="n">outputs</span><span class="p">,</span> <span class="n">finalstate</span> <span class="o">=</span> <span class="n">build_rnn</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">embed</span><span class="p">)</span>
    
    <span class="c1"># with no activation function, just a linear transformation</span>
    
    <span class="c1">#logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)</span>
    <span class="c1"># drop = tf.nn.dropout(x3, keep_prob=0.5)</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> 
                                           <span class="n">weights_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal_initializer</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span>
                                           <span class="n">biases_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">(),</span>
                                           <span class="n">activation_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">finalstate</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_build_nn</span><span class="p">(</span><span class="n">build_nn</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Batches">Batches<a class="anchor-link" href="#Batches">&#182;</a></h3><p>Implement <code>get_batches</code> to create batches of input and targets using <code>int_text</code>.  The batches should be a Numpy array with the shape <code>(number of batches, 2, batch size, sequence length)</code>. Each batch contains two elements:</p>
<ul>
<li>The first element is a single batch of <strong>input</strong> with the shape <code>[batch size, sequence length]</code></li>
<li>The second element is a single batch of <strong>targets</strong> with the shape <code>[batch size, sequence length]</code></li>
</ul>
<p>If you can't fill the last batch with enough data, drop the last batch.</p>
<p>For exmple, <code>get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 2, 3)</code> would return a Numpy array of the following:</p>

<pre><code>[
  # First Batch
  [
    # Batch of Input
    [[ 1  2  3], [ 7  8  9]],
    # Batch of targets
    [[ 2  3  4], [ 8  9 10]]
  ],

  # Second Batch
  [
    # Batch of Input
    [[ 4  5  6], [10 11 12]],
    # Batch of targets
    [[ 5  6  7], [11 12 13]]
  ]
]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[27]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_batches</span><span class="p">(</span><span class="n">int_text</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return batches of input and target</span>
<span class="sd">    :param int_text: Text with the words replaced by their ids</span>
<span class="sd">    :param batch_size: The size of batch</span>
<span class="sd">    :param seq_length: The length of sequence</span>
<span class="sd">    :return: Batches as a Numpy array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
    
    <span class="n">n_batches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">int_text</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">seq_length</span><span class="p">))</span>
    
    <span class="n">xdata</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">int_text</span><span class="p">[:</span> <span class="n">n_batches</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">seq_length</span><span class="p">])</span>
    <span class="n">ydata</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">int_text</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span> <span class="n">n_batches</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">seq_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Split an array into multiple sub-arrays</span>
    <span class="c1"># Gives a new shape to an array without changing its data. One shape dimension can be -1</span>
    
    <span class="n">x_batches</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">xdata</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">n_batches</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y_batches</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">ydata</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">n_batches</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># print(np.array(list(zip(x_batches, y_batches))))</span>
 
    <span class="c1"># The first element is a single batch of input with the shape [batch size, sequence length]</span>
    <span class="c1"># The second element is a single batch of targets with the shape [batch size, sequence length]</span>
        
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">x_batches</span><span class="p">,</span> <span class="n">y_batches</span><span class="p">)))</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_get_batches</span><span class="p">(</span><span class="n">get_batches</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Neural-Network-Training">Neural Network Training<a class="anchor-link" href="#Neural-Network-Training">&#182;</a></h2><h3 id="Hyperparameters">Hyperparameters<a class="anchor-link" href="#Hyperparameters">&#182;</a></h3><p>Tune the following parameters:</p>
<ul>
<li>Set <code>num_epochs</code> to the number of epochs.</li>
<li>Set <code>batch_size</code> to the batch size.</li>
<li>Set <code>rnn_size</code> to the size of the RNNs.</li>
<li>Set <code>seq_length</code> to the length of sequence.</li>
<li>Set <code>learning_rate</code> to the learning rate.</li>
<li>Set <code>show_every_n_batches</code> to the number of batches the neural network should print progress.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[28]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Number of Epochs</span>
<span class="c1"># num_epochs = 50 # for moe&#39;s tavern scenes</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">90</span>
<span class="c1"># Batch Size</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span>
<span class="c1"># RNN Size</span>
<span class="n">rnn_size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="c1"># Sequence Length</span>
<span class="n">seq_length</span> <span class="o">=</span> <span class="mi">12</span>
<span class="c1"># Learning Rate</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="c1"># Show stats for every n number of batches</span>
<span class="n">show_every_n_batches</span> <span class="o">=</span> <span class="mi">10</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">save_dir</span> <span class="o">=</span> <span class="s1">&#39;/save&#39;</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Build-the-Graph">Build the Graph<a class="anchor-link" href="#Build-the-Graph">&#182;</a></h3><p>Build the graph using the neural network you implemented.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[29]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">tensorflow.contrib</span> <span class="k">import</span> <span class="n">seq2seq</span>

<span class="n">train_graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="k">with</span> <span class="n">train_graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">int_to_vocab</span><span class="p">)</span>
    <span class="n">input_text</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">get_inputs</span><span class="p">()</span>
    <span class="n">input_data_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>
    <span class="n">cell</span><span class="p">,</span> <span class="n">initial_state</span> <span class="o">=</span> <span class="n">get_init_cell</span><span class="p">(</span><span class="n">input_data_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">rnn_size</span><span class="p">)</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">build_nn</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">rnn_size</span><span class="p">,</span> <span class="n">input_text</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

    <span class="c1"># Probabilities for generating words</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;probs&#39;</span><span class="p">)</span>

    <span class="c1"># Loss function</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">seq2seq</span><span class="o">.</span><span class="n">sequence_loss</span><span class="p">(</span>
        <span class="n">logits</span><span class="p">,</span>
        <span class="n">targets</span><span class="p">,</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">input_data_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_data_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]))</span>

    <span class="c1"># Optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>

    <span class="c1"># Gradient Clipping</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">compute_gradients</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
    <span class="n">capped_gradients</span> <span class="o">=</span> <span class="p">[(</span><span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">),</span> <span class="n">var</span><span class="p">)</span> <span class="k">for</span> <span class="n">grad</span><span class="p">,</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">gradients</span><span class="p">]</span>
    <span class="n">train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">capped_gradients</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Train">Train<a class="anchor-link" href="#Train">&#182;</a></h2><p>Train the neural network on the preprocessed data.  If you have a hard time getting a good loss, check the <a href="https://discussions.udacity.com/">forms</a> to see if anyone is having the same problem.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[30]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">get_batches</span><span class="p">(</span><span class="n">int_text</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">train_graph</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

    <span class="k">for</span> <span class="n">epoch_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">initial_state</span><span class="p">,</span> <span class="p">{</span><span class="n">input_text</span><span class="p">:</span> <span class="n">batches</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]})</span>

        <span class="k">for</span> <span class="n">batch_i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batches</span><span class="p">):</span>
            <span class="n">feed</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">input_text</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
                <span class="n">targets</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span>
                <span class="n">initial_state</span><span class="p">:</span> <span class="n">state</span><span class="p">,</span>
                <span class="n">lr</span><span class="p">:</span> <span class="n">learning_rate</span><span class="p">}</span>
            <span class="n">train_loss</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">cost</span><span class="p">,</span> <span class="n">final_state</span><span class="p">,</span> <span class="n">train_op</span><span class="p">],</span> <span class="n">feed</span><span class="p">)</span>

            <span class="c1"># Show every &lt;show_every_n_batches&gt; batches</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">epoch_i</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span> <span class="o">+</span> <span class="n">batch_i</span><span class="p">)</span> <span class="o">%</span> <span class="n">show_every_n_batches</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch </span><span class="si">{:&gt;3}</span><span class="s1"> Batch </span><span class="si">{:&gt;4}</span><span class="s1">/</span><span class="si">{}</span><span class="s1">   train_loss = </span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">epoch_i</span><span class="p">,</span>
                    <span class="n">batch_i</span><span class="p">,</span>
                    <span class="nb">len</span><span class="p">(</span><span class="n">batches</span><span class="p">),</span>
                    <span class="n">train_loss</span><span class="p">))</span>

    <span class="c1"># Save Model</span>
    <span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
    <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model Trained and Saved&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch   0 Batch    0/769   train_loss = 11.008
Epoch   0 Batch   10/769   train_loss = 7.321
Epoch   0 Batch   20/769   train_loss = 7.056
Epoch   0 Batch   30/769   train_loss = 7.041
Epoch   0 Batch   40/769   train_loss = 6.971
Epoch   0 Batch   50/769   train_loss = 6.859
Epoch   0 Batch   60/769   train_loss = 6.661
Epoch   0 Batch   70/769   train_loss = 6.594
Epoch   0 Batch   80/769   train_loss = 6.579
Epoch   0 Batch   90/769   train_loss = 6.536
Epoch   0 Batch  100/769   train_loss = 6.622
Epoch   0 Batch  110/769   train_loss = 6.503
Epoch   0 Batch  120/769   train_loss = 6.603
Epoch   0 Batch  130/769   train_loss = 6.666
Epoch   0 Batch  140/769   train_loss = 6.612
Epoch   0 Batch  150/769   train_loss = 6.651
Epoch   0 Batch  160/769   train_loss = 6.545
Epoch   0 Batch  170/769   train_loss = 6.577
Epoch   0 Batch  180/769   train_loss = 6.602
Epoch   0 Batch  190/769   train_loss = 6.651
Epoch   0 Batch  200/769   train_loss = 6.526
Epoch   0 Batch  210/769   train_loss = 6.642
Epoch   0 Batch  220/769   train_loss = 6.714
Epoch   0 Batch  230/769   train_loss = 6.609
Epoch   0 Batch  240/769   train_loss = 6.562
Epoch   0 Batch  250/769   train_loss = 6.489
Epoch   0 Batch  260/769   train_loss = 6.608
Epoch   0 Batch  270/769   train_loss = 6.566
Epoch   0 Batch  280/769   train_loss = 6.520
Epoch   0 Batch  290/769   train_loss = 6.580
Epoch   0 Batch  300/769   train_loss = 6.444
Epoch   0 Batch  310/769   train_loss = 6.394
Epoch   0 Batch  320/769   train_loss = 6.652
Epoch   0 Batch  330/769   train_loss = 6.464
Epoch   0 Batch  340/769   train_loss = 6.525
Epoch   0 Batch  350/769   train_loss = 6.579
Epoch   0 Batch  360/769   train_loss = 6.603
Epoch   0 Batch  370/769   train_loss = 6.527
Epoch   0 Batch  380/769   train_loss = 6.604
Epoch   0 Batch  390/769   train_loss = 6.556
Epoch   0 Batch  400/769   train_loss = 6.427
Epoch   0 Batch  410/769   train_loss = 6.567
Epoch   0 Batch  420/769   train_loss = 6.576
Epoch   0 Batch  430/769   train_loss = 6.512
Epoch   0 Batch  440/769   train_loss = 6.522
Epoch   0 Batch  450/769   train_loss = 6.532
Epoch   0 Batch  460/769   train_loss = 6.292
Epoch   0 Batch  470/769   train_loss = 6.527
Epoch   0 Batch  480/769   train_loss = 6.432
Epoch   0 Batch  490/769   train_loss = 6.494
Epoch   0 Batch  500/769   train_loss = 6.401
Epoch   0 Batch  510/769   train_loss = 6.386
Epoch   0 Batch  520/769   train_loss = 6.419
Epoch   0 Batch  530/769   train_loss = 6.422
Epoch   0 Batch  540/769   train_loss = 6.392
Epoch   0 Batch  550/769   train_loss = 6.484
Epoch   0 Batch  560/769   train_loss = 6.397
Epoch   0 Batch  570/769   train_loss = 6.367
Epoch   0 Batch  580/769   train_loss = 6.508
Epoch   0 Batch  590/769   train_loss = 6.403
Epoch   0 Batch  600/769   train_loss = 6.391
Epoch   0 Batch  610/769   train_loss = 6.462
Epoch   0 Batch  620/769   train_loss = 6.350
Epoch   0 Batch  630/769   train_loss = 6.391
Epoch   0 Batch  640/769   train_loss = 6.520
Epoch   0 Batch  650/769   train_loss = 6.330
Epoch   0 Batch  660/769   train_loss = 6.327
Epoch   0 Batch  670/769   train_loss = 6.214
Epoch   0 Batch  680/769   train_loss = 6.328
Epoch   0 Batch  690/769   train_loss = 6.384
Epoch   0 Batch  700/769   train_loss = 6.273
Epoch   0 Batch  710/769   train_loss = 6.292
Epoch   0 Batch  720/769   train_loss = 6.318
Epoch   0 Batch  730/769   train_loss = 6.188
Epoch   0 Batch  740/769   train_loss = 6.183
Epoch   0 Batch  750/769   train_loss = 6.254
Epoch   0 Batch  760/769   train_loss = 6.100
Epoch   1 Batch    1/769   train_loss = 6.338
Epoch   1 Batch   11/769   train_loss = 6.264
Epoch   1 Batch   21/769   train_loss = 6.131
Epoch   1 Batch   31/769   train_loss = 6.097
Epoch   1 Batch   41/769   train_loss = 6.054
Epoch   1 Batch   51/769   train_loss = 6.080
Epoch   1 Batch   61/769   train_loss = 6.059
Epoch   1 Batch   71/769   train_loss = 6.075
Epoch   1 Batch   81/769   train_loss = 5.922
Epoch   1 Batch   91/769   train_loss = 5.912
Epoch   1 Batch  101/769   train_loss = 6.043
Epoch   1 Batch  111/769   train_loss = 6.068
Epoch   1 Batch  121/769   train_loss = 5.988
Epoch   1 Batch  131/769   train_loss = 6.067
Epoch   1 Batch  141/769   train_loss = 5.843
Epoch   1 Batch  151/769   train_loss = 6.020
Epoch   1 Batch  161/769   train_loss = 5.832
Epoch   1 Batch  171/769   train_loss = 5.810
Epoch   1 Batch  181/769   train_loss = 5.869
Epoch   1 Batch  191/769   train_loss = 5.847
Epoch   1 Batch  201/769   train_loss = 5.787
Epoch   1 Batch  211/769   train_loss = 5.764
Epoch   1 Batch  221/769   train_loss = 5.669
Epoch   1 Batch  231/769   train_loss = 5.598
Epoch   1 Batch  241/769   train_loss = 5.541
Epoch   1 Batch  251/769   train_loss = 5.547
Epoch   1 Batch  261/769   train_loss = 5.398
Epoch   1 Batch  271/769   train_loss = 5.516
Epoch   1 Batch  281/769   train_loss = 5.429
Epoch   1 Batch  291/769   train_loss = 5.397
Epoch   1 Batch  301/769   train_loss = 5.304
Epoch   1 Batch  311/769   train_loss = 5.384
Epoch   1 Batch  321/769   train_loss = 5.338
Epoch   1 Batch  331/769   train_loss = 5.293
Epoch   1 Batch  341/769   train_loss = 5.293
Epoch   1 Batch  351/769   train_loss = 5.392
Epoch   1 Batch  361/769   train_loss = 4.976
Epoch   1 Batch  371/769   train_loss = 5.080
Epoch   1 Batch  381/769   train_loss = 5.244
Epoch   1 Batch  391/769   train_loss = 5.122
Epoch   1 Batch  401/769   train_loss = 5.135
Epoch   1 Batch  411/769   train_loss = 5.054
Epoch   1 Batch  421/769   train_loss = 5.055
Epoch   1 Batch  431/769   train_loss = 5.012
Epoch   1 Batch  441/769   train_loss = 4.880
Epoch   1 Batch  451/769   train_loss = 4.978
Epoch   1 Batch  461/769   train_loss = 4.964
Epoch   1 Batch  471/769   train_loss = 5.064
Epoch   1 Batch  481/769   train_loss = 5.017
Epoch   1 Batch  491/769   train_loss = 4.851
Epoch   1 Batch  501/769   train_loss = 4.910
Epoch   1 Batch  511/769   train_loss = 4.769
Epoch   1 Batch  521/769   train_loss = 4.787
Epoch   1 Batch  531/769   train_loss = 5.025
Epoch   1 Batch  541/769   train_loss = 4.831
Epoch   1 Batch  551/769   train_loss = 4.883
Epoch   1 Batch  561/769   train_loss = 4.835
Epoch   1 Batch  571/769   train_loss = 4.866
Epoch   1 Batch  581/769   train_loss = 5.013
Epoch   1 Batch  591/769   train_loss = 4.988
Epoch   1 Batch  601/769   train_loss = 4.829
Epoch   1 Batch  611/769   train_loss = 4.872
Epoch   1 Batch  621/769   train_loss = 4.915
Epoch   1 Batch  631/769   train_loss = 4.864
Epoch   1 Batch  641/769   train_loss = 4.837
Epoch   1 Batch  651/769   train_loss = 4.848
Epoch   1 Batch  661/769   train_loss = 4.832
Epoch   1 Batch  671/769   train_loss = 4.742
Epoch   1 Batch  681/769   train_loss = 4.741
Epoch   1 Batch  691/769   train_loss = 4.765
Epoch   1 Batch  701/769   train_loss = 4.704
Epoch   1 Batch  711/769   train_loss = 4.704
Epoch   1 Batch  721/769   train_loss = 4.875
Epoch   1 Batch  731/769   train_loss = 4.821
Epoch   1 Batch  741/769   train_loss = 4.764
Epoch   1 Batch  751/769   train_loss = 4.675
Epoch   1 Batch  761/769   train_loss = 4.667
Epoch   2 Batch    2/769   train_loss = 4.691
Epoch   2 Batch   12/769   train_loss = 4.775
Epoch   2 Batch   22/769   train_loss = 4.698
Epoch   2 Batch   32/769   train_loss = 4.739
Epoch   2 Batch   42/769   train_loss = 4.731
Epoch   2 Batch   52/769   train_loss = 4.721
Epoch   2 Batch   62/769   train_loss = 4.640
Epoch   2 Batch   72/769   train_loss = 4.556
Epoch   2 Batch   82/769   train_loss = 4.469
Epoch   2 Batch   92/769   train_loss = 4.665
Epoch   2 Batch  102/769   train_loss = 4.575
Epoch   2 Batch  112/769   train_loss = 4.620
Epoch   2 Batch  122/769   train_loss = 4.681
Epoch   2 Batch  132/769   train_loss = 4.725
Epoch   2 Batch  142/769   train_loss = 4.635
Epoch   2 Batch  152/769   train_loss = 4.683
Epoch   2 Batch  162/769   train_loss = 4.562
Epoch   2 Batch  172/769   train_loss = 4.578
Epoch   2 Batch  182/769   train_loss = 4.683
Epoch   2 Batch  192/769   train_loss = 4.579
Epoch   2 Batch  202/769   train_loss = 4.575
Epoch   2 Batch  212/769   train_loss = 4.684
Epoch   2 Batch  222/769   train_loss = 4.672
Epoch   2 Batch  232/769   train_loss = 4.639
Epoch   2 Batch  242/769   train_loss = 4.546
Epoch   2 Batch  252/769   train_loss = 4.588
Epoch   2 Batch  262/769   train_loss = 4.430
Epoch   2 Batch  272/769   train_loss = 4.531
Epoch   2 Batch  282/769   train_loss = 4.496
Epoch   2 Batch  292/769   train_loss = 4.565
Epoch   2 Batch  302/769   train_loss = 4.421
Epoch   2 Batch  312/769   train_loss = 4.514
Epoch   2 Batch  322/769   train_loss = 4.540
Epoch   2 Batch  332/769   train_loss = 4.468
Epoch   2 Batch  342/769   train_loss = 4.582
Epoch   2 Batch  352/769   train_loss = 4.553
Epoch   2 Batch  362/769   train_loss = 4.453
Epoch   2 Batch  372/769   train_loss = 4.482
Epoch   2 Batch  382/769   train_loss = 4.533
Epoch   2 Batch  392/769   train_loss = 4.556
Epoch   2 Batch  402/769   train_loss = 4.485
Epoch   2 Batch  412/769   train_loss = 4.553
Epoch   2 Batch  422/769   train_loss = 4.335
Epoch   2 Batch  432/769   train_loss = 4.458
Epoch   2 Batch  442/769   train_loss = 4.521
Epoch   2 Batch  452/769   train_loss = 4.446
Epoch   2 Batch  462/769   train_loss = 4.456
Epoch   2 Batch  472/769   train_loss = 4.594
Epoch   2 Batch  482/769   train_loss = 4.466
Epoch   2 Batch  492/769   train_loss = 4.629
Epoch   2 Batch  502/769   train_loss = 4.350
Epoch   2 Batch  512/769   train_loss = 4.256
Epoch   2 Batch  522/769   train_loss = 4.397
Epoch   2 Batch  532/769   train_loss = 4.545
Epoch   2 Batch  542/769   train_loss = 4.542
Epoch   2 Batch  552/769   train_loss = 4.437
Epoch   2 Batch  562/769   train_loss = 4.547
Epoch   2 Batch  572/769   train_loss = 4.555
Epoch   2 Batch  582/769   train_loss = 4.455
Epoch   2 Batch  592/769   train_loss = 4.414
Epoch   2 Batch  602/769   train_loss = 4.507
Epoch   2 Batch  612/769   train_loss = 4.511
Epoch   2 Batch  622/769   train_loss = 4.424
Epoch   2 Batch  632/769   train_loss = 4.466
Epoch   2 Batch  642/769   train_loss = 4.516
Epoch   2 Batch  652/769   train_loss = 4.335
Epoch   2 Batch  662/769   train_loss = 4.442
Epoch   2 Batch  672/769   train_loss = 4.585
Epoch   2 Batch  682/769   train_loss = 4.316
Epoch   2 Batch  692/769   train_loss = 4.646
Epoch   2 Batch  702/769   train_loss = 4.372
Epoch   2 Batch  712/769   train_loss = 4.331
Epoch   2 Batch  722/769   train_loss = 4.464
Epoch   2 Batch  732/769   train_loss = 4.440
Epoch   2 Batch  742/769   train_loss = 4.311
Epoch   2 Batch  752/769   train_loss = 4.532
Epoch   2 Batch  762/769   train_loss = 4.359
Epoch   3 Batch    3/769   train_loss = 4.318
Epoch   3 Batch   13/769   train_loss = 4.371
Epoch   3 Batch   23/769   train_loss = 4.447
Epoch   3 Batch   33/769   train_loss = 4.451
Epoch   3 Batch   43/769   train_loss = 4.343
Epoch   3 Batch   53/769   train_loss = 4.383
Epoch   3 Batch   63/769   train_loss = 4.309
Epoch   3 Batch   73/769   train_loss = 4.160
Epoch   3 Batch   83/769   train_loss = 4.331
Epoch   3 Batch   93/769   train_loss = 4.328
Epoch   3 Batch  103/769   train_loss = 4.331
Epoch   3 Batch  113/769   train_loss = 4.376
Epoch   3 Batch  123/769   train_loss = 4.279
Epoch   3 Batch  133/769   train_loss = 4.350
Epoch   3 Batch  143/769   train_loss = 4.327
Epoch   3 Batch  153/769   train_loss = 4.209
Epoch   3 Batch  163/769   train_loss = 4.367
Epoch   3 Batch  173/769   train_loss = 4.275
Epoch   3 Batch  183/769   train_loss = 4.396
Epoch   3 Batch  193/769   train_loss = 4.320
Epoch   3 Batch  203/769   train_loss = 4.294
Epoch   3 Batch  213/769   train_loss = 4.291
Epoch   3 Batch  223/769   train_loss = 4.374
Epoch   3 Batch  233/769   train_loss = 4.287
Epoch   3 Batch  243/769   train_loss = 4.328
Epoch   3 Batch  253/769   train_loss = 4.410
Epoch   3 Batch  263/769   train_loss = 4.249
Epoch   3 Batch  273/769   train_loss = 4.312
Epoch   3 Batch  283/769   train_loss = 4.277
Epoch   3 Batch  293/769   train_loss = 4.157
Epoch   3 Batch  303/769   train_loss = 4.276
Epoch   3 Batch  313/769   train_loss = 4.145
Epoch   3 Batch  323/769   train_loss = 4.253
Epoch   3 Batch  333/769   train_loss = 4.344
Epoch   3 Batch  343/769   train_loss = 4.237
Epoch   3 Batch  353/769   train_loss = 4.283
Epoch   3 Batch  363/769   train_loss = 4.366
Epoch   3 Batch  373/769   train_loss = 4.227
Epoch   3 Batch  383/769   train_loss = 4.240
Epoch   3 Batch  393/769   train_loss = 4.330
Epoch   3 Batch  403/769   train_loss = 4.197
Epoch   3 Batch  413/769   train_loss = 4.158
Epoch   3 Batch  423/769   train_loss = 4.256
Epoch   3 Batch  433/769   train_loss = 4.238
Epoch   3 Batch  443/769   train_loss = 4.348
Epoch   3 Batch  453/769   train_loss = 4.225
Epoch   3 Batch  463/769   train_loss = 4.277
Epoch   3 Batch  473/769   train_loss = 4.252
Epoch   3 Batch  483/769   train_loss = 4.196
Epoch   3 Batch  493/769   train_loss = 4.322
Epoch   3 Batch  503/769   train_loss = 4.270
Epoch   3 Batch  513/769   train_loss = 4.218
Epoch   3 Batch  523/769   train_loss = 4.339
Epoch   3 Batch  533/769   train_loss = 4.215
Epoch   3 Batch  543/769   train_loss = 4.371
Epoch   3 Batch  553/769   train_loss = 4.158
Epoch   3 Batch  563/769   train_loss = 4.182
Epoch   3 Batch  573/769   train_loss = 4.200
Epoch   3 Batch  583/769   train_loss = 4.360
Epoch   3 Batch  593/769   train_loss = 4.218
Epoch   3 Batch  603/769   train_loss = 4.133
Epoch   3 Batch  613/769   train_loss = 4.271
Epoch   3 Batch  623/769   train_loss = 4.196
Epoch   3 Batch  633/769   train_loss = 4.171
Epoch   3 Batch  643/769   train_loss = 4.210
Epoch   3 Batch  653/769   train_loss = 4.109
Epoch   3 Batch  663/769   train_loss = 4.269
Epoch   3 Batch  673/769   train_loss = 4.167
Epoch   3 Batch  683/769   train_loss = 4.254
Epoch   3 Batch  693/769   train_loss = 4.245
Epoch   3 Batch  703/769   train_loss = 4.124
Epoch   3 Batch  713/769   train_loss = 4.200
Epoch   3 Batch  723/769   train_loss = 4.142
Epoch   3 Batch  733/769   train_loss = 4.266
Epoch   3 Batch  743/769   train_loss = 4.248
Epoch   3 Batch  753/769   train_loss = 4.130
Epoch   3 Batch  763/769   train_loss = 4.351
Epoch   4 Batch    4/769   train_loss = 4.243
Epoch   4 Batch   14/769   train_loss = 4.128
Epoch   4 Batch   24/769   train_loss = 4.194
Epoch   4 Batch   34/769   train_loss = 4.213
Epoch   4 Batch   44/769   train_loss = 4.203
Epoch   4 Batch   54/769   train_loss = 4.205
Epoch   4 Batch   64/769   train_loss = 4.139
Epoch   4 Batch   74/769   train_loss = 4.049
Epoch   4 Batch   84/769   train_loss = 4.082
Epoch   4 Batch   94/769   train_loss = 4.091
Epoch   4 Batch  104/769   train_loss = 4.067
Epoch   4 Batch  114/769   train_loss = 4.237
Epoch   4 Batch  124/769   train_loss = 4.238
Epoch   4 Batch  134/769   train_loss = 4.173
Epoch   4 Batch  144/769   train_loss = 4.101
Epoch   4 Batch  154/769   train_loss = 4.120
Epoch   4 Batch  164/769   train_loss = 4.088
Epoch   4 Batch  174/769   train_loss = 4.143
Epoch   4 Batch  184/769   train_loss = 4.140
Epoch   4 Batch  194/769   train_loss = 4.137
Epoch   4 Batch  204/769   train_loss = 3.993
Epoch   4 Batch  214/769   train_loss = 4.051
Epoch   4 Batch  224/769   train_loss = 4.166
Epoch   4 Batch  234/769   train_loss = 4.229
Epoch   4 Batch  244/769   train_loss = 4.193
Epoch   4 Batch  254/769   train_loss = 4.169
Epoch   4 Batch  264/769   train_loss = 4.254
Epoch   4 Batch  274/769   train_loss = 4.075
Epoch   4 Batch  284/769   train_loss = 3.982
Epoch   4 Batch  294/769   train_loss = 4.084
Epoch   4 Batch  304/769   train_loss = 4.081
Epoch   4 Batch  314/769   train_loss = 4.101
Epoch   4 Batch  324/769   train_loss = 4.170
Epoch   4 Batch  334/769   train_loss = 4.075
Epoch   4 Batch  344/769   train_loss = 4.173
Epoch   4 Batch  354/769   train_loss = 4.156
Epoch   4 Batch  364/769   train_loss = 4.203
Epoch   4 Batch  374/769   train_loss = 4.097
Epoch   4 Batch  384/769   train_loss = 4.078
Epoch   4 Batch  394/769   train_loss = 3.979
Epoch   4 Batch  404/769   train_loss = 4.076
Epoch   4 Batch  414/769   train_loss = 3.955
Epoch   4 Batch  424/769   train_loss = 4.211
Epoch   4 Batch  434/769   train_loss = 4.148
Epoch   4 Batch  444/769   train_loss = 4.035
Epoch   4 Batch  454/769   train_loss = 4.026
Epoch   4 Batch  464/769   train_loss = 4.201
Epoch   4 Batch  474/769   train_loss = 4.093
Epoch   4 Batch  484/769   train_loss = 4.115
Epoch   4 Batch  494/769   train_loss = 4.042
Epoch   4 Batch  504/769   train_loss = 4.086
Epoch   4 Batch  514/769   train_loss = 4.016
Epoch   4 Batch  524/769   train_loss = 4.011
Epoch   4 Batch  534/769   train_loss = 4.195
Epoch   4 Batch  544/769   train_loss = 4.100
Epoch   4 Batch  554/769   train_loss = 4.203
Epoch   4 Batch  564/769   train_loss = 4.043
Epoch   4 Batch  574/769   train_loss = 4.015
Epoch   4 Batch  584/769   train_loss = 4.146
Epoch   4 Batch  594/769   train_loss = 4.049
Epoch   4 Batch  604/769   train_loss = 3.971
Epoch   4 Batch  614/769   train_loss = 4.151
Epoch   4 Batch  624/769   train_loss = 4.028
Epoch   4 Batch  634/769   train_loss = 4.200
Epoch   4 Batch  644/769   train_loss = 4.105
Epoch   4 Batch  654/769   train_loss = 4.071
Epoch   4 Batch  664/769   train_loss = 4.125
Epoch   4 Batch  674/769   train_loss = 4.094
Epoch   4 Batch  684/769   train_loss = 4.031
Epoch   4 Batch  694/769   train_loss = 4.068
Epoch   4 Batch  704/769   train_loss = 4.031
Epoch   4 Batch  714/769   train_loss = 4.010
Epoch   4 Batch  724/769   train_loss = 3.986
Epoch   4 Batch  734/769   train_loss = 4.077
Epoch   4 Batch  744/769   train_loss = 4.069
Epoch   4 Batch  754/769   train_loss = 4.159
Epoch   4 Batch  764/769   train_loss = 4.009
Epoch   5 Batch    5/769   train_loss = 4.033
Epoch   5 Batch   15/769   train_loss = 4.076
Epoch   5 Batch   25/769   train_loss = 3.984
Epoch   5 Batch   35/769   train_loss = 4.011
Epoch   5 Batch   45/769   train_loss = 3.967
Epoch   5 Batch   55/769   train_loss = 3.974
Epoch   5 Batch   65/769   train_loss = 3.944
Epoch   5 Batch   75/769   train_loss = 4.025
Epoch   5 Batch   85/769   train_loss = 3.993
Epoch   5 Batch   95/769   train_loss = 3.911
Epoch   5 Batch  105/769   train_loss = 4.034
Epoch   5 Batch  115/769   train_loss = 3.870
Epoch   5 Batch  125/769   train_loss = 4.057
Epoch   5 Batch  135/769   train_loss = 3.919
Epoch   5 Batch  145/769   train_loss = 3.922
Epoch   5 Batch  155/769   train_loss = 3.904
Epoch   5 Batch  165/769   train_loss = 4.070
Epoch   5 Batch  175/769   train_loss = 4.059
Epoch   5 Batch  185/769   train_loss = 3.984
Epoch   5 Batch  195/769   train_loss = 3.944
Epoch   5 Batch  205/769   train_loss = 3.867
Epoch   5 Batch  215/769   train_loss = 4.048
Epoch   5 Batch  225/769   train_loss = 4.123
Epoch   5 Batch  235/769   train_loss = 3.965
Epoch   5 Batch  245/769   train_loss = 4.020
Epoch   5 Batch  255/769   train_loss = 4.001
Epoch   5 Batch  265/769   train_loss = 4.048
Epoch   5 Batch  275/769   train_loss = 3.998
Epoch   5 Batch  285/769   train_loss = 3.903
Epoch   5 Batch  295/769   train_loss = 3.994
Epoch   5 Batch  305/769   train_loss = 3.987
Epoch   5 Batch  315/769   train_loss = 4.012
Epoch   5 Batch  325/769   train_loss = 4.004
Epoch   5 Batch  335/769   train_loss = 4.096
Epoch   5 Batch  345/769   train_loss = 3.946
Epoch   5 Batch  355/769   train_loss = 4.030
Epoch   5 Batch  365/769   train_loss = 4.008
Epoch   5 Batch  375/769   train_loss = 3.848
Epoch   5 Batch  385/769   train_loss = 3.983
Epoch   5 Batch  395/769   train_loss = 3.849
Epoch   5 Batch  405/769   train_loss = 4.096
Epoch   5 Batch  415/769   train_loss = 3.840
Epoch   5 Batch  425/769   train_loss = 3.992
Epoch   5 Batch  435/769   train_loss = 3.886
Epoch   5 Batch  445/769   train_loss = 3.892
Epoch   5 Batch  455/769   train_loss = 3.923
Epoch   5 Batch  465/769   train_loss = 4.027
Epoch   5 Batch  475/769   train_loss = 3.915
Epoch   5 Batch  485/769   train_loss = 4.009
Epoch   5 Batch  495/769   train_loss = 3.762
Epoch   5 Batch  505/769   train_loss = 3.827
Epoch   5 Batch  515/769   train_loss = 3.902
Epoch   5 Batch  525/769   train_loss = 3.888
Epoch   5 Batch  535/769   train_loss = 3.910
Epoch   5 Batch  545/769   train_loss = 3.946
Epoch   5 Batch  555/769   train_loss = 3.991
Epoch   5 Batch  565/769   train_loss = 4.018
Epoch   5 Batch  575/769   train_loss = 3.969
Epoch   5 Batch  585/769   train_loss = 3.983
Epoch   5 Batch  595/769   train_loss = 3.818
Epoch   5 Batch  605/769   train_loss = 3.865
Epoch   5 Batch  615/769   train_loss = 3.957
Epoch   5 Batch  625/769   train_loss = 3.867
Epoch   5 Batch  635/769   train_loss = 3.855
Epoch   5 Batch  645/769   train_loss = 3.917
Epoch   5 Batch  655/769   train_loss = 3.756
Epoch   5 Batch  665/769   train_loss = 3.916
Epoch   5 Batch  675/769   train_loss = 3.825
Epoch   5 Batch  685/769   train_loss = 3.885
Epoch   5 Batch  695/769   train_loss = 3.907
Epoch   5 Batch  705/769   train_loss = 3.907
Epoch   5 Batch  715/769   train_loss = 3.982
Epoch   5 Batch  725/769   train_loss = 3.863
Epoch   5 Batch  735/769   train_loss = 3.859
Epoch   5 Batch  745/769   train_loss = 3.922
Epoch   5 Batch  755/769   train_loss = 3.836
Epoch   5 Batch  765/769   train_loss = 3.887
Epoch   6 Batch    6/769   train_loss = 3.850
Epoch   6 Batch   16/769   train_loss = 3.933
Epoch   6 Batch   26/769   train_loss = 3.924
Epoch   6 Batch   36/769   train_loss = 3.849
Epoch   6 Batch   46/769   train_loss = 3.873
Epoch   6 Batch   56/769   train_loss = 3.839
Epoch   6 Batch   66/769   train_loss = 3.849
Epoch   6 Batch   76/769   train_loss = 3.750
Epoch   6 Batch   86/769   train_loss = 3.910
Epoch   6 Batch   96/769   train_loss = 3.840
Epoch   6 Batch  106/769   train_loss = 3.830
Epoch   6 Batch  116/769   train_loss = 3.907
Epoch   6 Batch  126/769   train_loss = 3.861
Epoch   6 Batch  136/769   train_loss = 3.934
Epoch   6 Batch  146/769   train_loss = 3.736
Epoch   6 Batch  156/769   train_loss = 3.702
Epoch   6 Batch  166/769   train_loss = 3.922
Epoch   6 Batch  176/769   train_loss = 3.920
Epoch   6 Batch  186/769   train_loss = 3.733
Epoch   6 Batch  196/769   train_loss = 3.989
Epoch   6 Batch  206/769   train_loss = 3.883
Epoch   6 Batch  216/769   train_loss = 3.767
Epoch   6 Batch  226/769   train_loss = 3.782
Epoch   6 Batch  236/769   train_loss = 3.877
Epoch   6 Batch  246/769   train_loss = 3.852
Epoch   6 Batch  256/769   train_loss = 3.707
Epoch   6 Batch  266/769   train_loss = 3.688
Epoch   6 Batch  276/769   train_loss = 3.740
Epoch   6 Batch  286/769   train_loss = 3.793
Epoch   6 Batch  296/769   train_loss = 3.851
Epoch   6 Batch  306/769   train_loss = 3.737
Epoch   6 Batch  316/769   train_loss = 3.821
Epoch   6 Batch  326/769   train_loss = 3.809
Epoch   6 Batch  336/769   train_loss = 3.772
Epoch   6 Batch  346/769   train_loss = 3.781
Epoch   6 Batch  356/769   train_loss = 3.690
Epoch   6 Batch  366/769   train_loss = 3.850
Epoch   6 Batch  376/769   train_loss = 3.745
Epoch   6 Batch  386/769   train_loss = 3.904
Epoch   6 Batch  396/769   train_loss = 3.737
Epoch   6 Batch  406/769   train_loss = 3.875
Epoch   6 Batch  416/769   train_loss = 3.804
Epoch   6 Batch  426/769   train_loss = 3.846
Epoch   6 Batch  436/769   train_loss = 3.709
Epoch   6 Batch  446/769   train_loss = 3.788
Epoch   6 Batch  456/769   train_loss = 3.813
Epoch   6 Batch  466/769   train_loss = 3.854
Epoch   6 Batch  476/769   train_loss = 3.704
Epoch   6 Batch  486/769   train_loss = 3.789
Epoch   6 Batch  496/769   train_loss = 3.799
Epoch   6 Batch  506/769   train_loss = 3.749
Epoch   6 Batch  516/769   train_loss = 3.827
Epoch   6 Batch  526/769   train_loss = 3.833
Epoch   6 Batch  536/769   train_loss = 3.872
Epoch   6 Batch  546/769   train_loss = 3.822
Epoch   6 Batch  556/769   train_loss = 3.776
Epoch   6 Batch  566/769   train_loss = 3.788
Epoch   6 Batch  576/769   train_loss = 3.716
Epoch   6 Batch  586/769   train_loss = 3.805
Epoch   6 Batch  596/769   train_loss = 3.765
Epoch   6 Batch  606/769   train_loss = 3.887
Epoch   6 Batch  616/769   train_loss = 3.658
Epoch   6 Batch  626/769   train_loss = 3.684
Epoch   6 Batch  636/769   train_loss = 3.732
Epoch   6 Batch  646/769   train_loss = 3.827
Epoch   6 Batch  656/769   train_loss = 3.771
Epoch   6 Batch  666/769   train_loss = 3.734
Epoch   6 Batch  676/769   train_loss = 3.767
Epoch   6 Batch  686/769   train_loss = 3.815
Epoch   6 Batch  696/769   train_loss = 3.677
Epoch   6 Batch  706/769   train_loss = 3.688
Epoch   6 Batch  716/769   train_loss = 3.753
Epoch   6 Batch  726/769   train_loss = 3.762
Epoch   6 Batch  736/769   train_loss = 3.633
Epoch   6 Batch  746/769   train_loss = 3.755
Epoch   6 Batch  756/769   train_loss = 3.674
Epoch   6 Batch  766/769   train_loss = 3.723
Epoch   7 Batch    7/769   train_loss = 3.754
Epoch   7 Batch   17/769   train_loss = 3.760
Epoch   7 Batch   27/769   train_loss = 3.794
Epoch   7 Batch   37/769   train_loss = 3.673
Epoch   7 Batch   47/769   train_loss = 3.682
Epoch   7 Batch   57/769   train_loss = 3.703
Epoch   7 Batch   67/769   train_loss = 3.679
Epoch   7 Batch   77/769   train_loss = 3.788
Epoch   7 Batch   87/769   train_loss = 3.662
Epoch   7 Batch   97/769   train_loss = 3.665
Epoch   7 Batch  107/769   train_loss = 3.693
Epoch   7 Batch  117/769   train_loss = 3.720
Epoch   7 Batch  127/769   train_loss = 3.635
Epoch   7 Batch  137/769   train_loss = 3.725
Epoch   7 Batch  147/769   train_loss = 3.708
Epoch   7 Batch  157/769   train_loss = 3.691
Epoch   7 Batch  167/769   train_loss = 3.815
Epoch   7 Batch  177/769   train_loss = 3.723
Epoch   7 Batch  187/769   train_loss = 3.715
Epoch   7 Batch  197/769   train_loss = 3.597
Epoch   7 Batch  207/769   train_loss = 3.756
Epoch   7 Batch  217/769   train_loss = 3.678
Epoch   7 Batch  227/769   train_loss = 3.725
Epoch   7 Batch  237/769   train_loss = 3.714
Epoch   7 Batch  247/769   train_loss = 3.774
Epoch   7 Batch  257/769   train_loss = 3.723
Epoch   7 Batch  267/769   train_loss = 3.584
Epoch   7 Batch  277/769   train_loss = 3.624
Epoch   7 Batch  287/769   train_loss = 3.714
Epoch   7 Batch  297/769   train_loss = 3.616
Epoch   7 Batch  307/769   train_loss = 3.688
Epoch   7 Batch  317/769   train_loss = 3.602
Epoch   7 Batch  327/769   train_loss = 3.662
Epoch   7 Batch  337/769   train_loss = 3.631
Epoch   7 Batch  347/769   train_loss = 3.708
Epoch   7 Batch  357/769   train_loss = 3.700
Epoch   7 Batch  367/769   train_loss = 3.681
Epoch   7 Batch  377/769   train_loss = 3.699
Epoch   7 Batch  387/769   train_loss = 3.774
Epoch   7 Batch  397/769   train_loss = 3.677
Epoch   7 Batch  407/769   train_loss = 3.657
Epoch   7 Batch  417/769   train_loss = 3.734
Epoch   7 Batch  427/769   train_loss = 3.583
Epoch   7 Batch  437/769   train_loss = 3.646
Epoch   7 Batch  447/769   train_loss = 3.593
Epoch   7 Batch  457/769   train_loss = 3.555
Epoch   7 Batch  467/769   train_loss = 3.699
Epoch   7 Batch  477/769   train_loss = 3.662
Epoch   7 Batch  487/769   train_loss = 3.634
Epoch   7 Batch  497/769   train_loss = 3.701
Epoch   7 Batch  507/769   train_loss = 3.664
Epoch   7 Batch  517/769   train_loss = 3.667
Epoch   7 Batch  527/769   train_loss = 3.688
Epoch   7 Batch  537/769   train_loss = 3.707
Epoch   7 Batch  547/769   train_loss = 3.537
Epoch   7 Batch  557/769   train_loss = 3.610
Epoch   7 Batch  567/769   train_loss = 3.708
Epoch   7 Batch  577/769   train_loss = 3.576
Epoch   7 Batch  587/769   train_loss = 3.606
Epoch   7 Batch  597/769   train_loss = 3.673
Epoch   7 Batch  607/769   train_loss = 3.663
Epoch   7 Batch  617/769   train_loss = 3.658
Epoch   7 Batch  627/769   train_loss = 3.613
Epoch   7 Batch  637/769   train_loss = 3.534
Epoch   7 Batch  647/769   train_loss = 3.601
Epoch   7 Batch  657/769   train_loss = 3.630
Epoch   7 Batch  667/769   train_loss = 3.625
Epoch   7 Batch  677/769   train_loss = 3.581
Epoch   7 Batch  687/769   train_loss = 3.623
Epoch   7 Batch  697/769   train_loss = 3.600
Epoch   7 Batch  707/769   train_loss = 3.575
Epoch   7 Batch  717/769   train_loss = 3.505
Epoch   7 Batch  727/769   train_loss = 3.552
Epoch   7 Batch  737/769   train_loss = 3.587
Epoch   7 Batch  747/769   train_loss = 3.626
Epoch   7 Batch  757/769   train_loss = 3.587
Epoch   7 Batch  767/769   train_loss = 3.619
Epoch   8 Batch    8/769   train_loss = 3.473
Epoch   8 Batch   18/769   train_loss = 3.593
Epoch   8 Batch   28/769   train_loss = 3.700
Epoch   8 Batch   38/769   train_loss = 3.701
Epoch   8 Batch   48/769   train_loss = 3.518
Epoch   8 Batch   58/769   train_loss = 3.595
Epoch   8 Batch   68/769   train_loss = 3.553
Epoch   8 Batch   78/769   train_loss = 3.592
Epoch   8 Batch   88/769   train_loss = 3.470
Epoch   8 Batch   98/769   train_loss = 3.546
Epoch   8 Batch  108/769   train_loss = 3.603
Epoch   8 Batch  118/769   train_loss = 3.558
Epoch   8 Batch  128/769   train_loss = 3.593
Epoch   8 Batch  138/769   train_loss = 3.566
Epoch   8 Batch  148/769   train_loss = 3.635
Epoch   8 Batch  158/769   train_loss = 3.479
Epoch   8 Batch  168/769   train_loss = 3.562
Epoch   8 Batch  178/769   train_loss = 3.593
Epoch   8 Batch  188/769   train_loss = 3.601
Epoch   8 Batch  198/769   train_loss = 3.583
Epoch   8 Batch  208/769   train_loss = 3.538
Epoch   8 Batch  218/769   train_loss = 3.543
Epoch   8 Batch  228/769   train_loss = 3.543
Epoch   8 Batch  238/769   train_loss = 3.580
Epoch   8 Batch  248/769   train_loss = 3.561
Epoch   8 Batch  258/769   train_loss = 3.547
Epoch   8 Batch  268/769   train_loss = 3.502
Epoch   8 Batch  278/769   train_loss = 3.512
Epoch   8 Batch  288/769   train_loss = 3.609
Epoch   8 Batch  298/769   train_loss = 3.570
Epoch   8 Batch  308/769   train_loss = 3.569
Epoch   8 Batch  318/769   train_loss = 3.421
Epoch   8 Batch  328/769   train_loss = 3.558
Epoch   8 Batch  338/769   train_loss = 3.531
Epoch   8 Batch  348/769   train_loss = 3.553
Epoch   8 Batch  358/769   train_loss = 3.501
Epoch   8 Batch  368/769   train_loss = 3.661
Epoch   8 Batch  378/769   train_loss = 3.568
Epoch   8 Batch  388/769   train_loss = 3.516
Epoch   8 Batch  398/769   train_loss = 3.601
Epoch   8 Batch  408/769   train_loss = 3.586
Epoch   8 Batch  418/769   train_loss = 3.625
Epoch   8 Batch  428/769   train_loss = 3.522
Epoch   8 Batch  438/769   train_loss = 3.425
Epoch   8 Batch  448/769   train_loss = 3.494
Epoch   8 Batch  458/769   train_loss = 3.514
Epoch   8 Batch  468/769   train_loss = 3.529
Epoch   8 Batch  478/769   train_loss = 3.495
Epoch   8 Batch  488/769   train_loss = 3.488
Epoch   8 Batch  498/769   train_loss = 3.586
Epoch   8 Batch  508/769   train_loss = 3.556
Epoch   8 Batch  518/769   train_loss = 3.491
Epoch   8 Batch  528/769   train_loss = 3.423
Epoch   8 Batch  538/769   train_loss = 3.465
Epoch   8 Batch  548/769   train_loss = 3.553
Epoch   8 Batch  558/769   train_loss = 3.523
Epoch   8 Batch  568/769   train_loss = 3.444
Epoch   8 Batch  578/769   train_loss = 3.553
Epoch   8 Batch  588/769   train_loss = 3.527
Epoch   8 Batch  598/769   train_loss = 3.464
Epoch   8 Batch  608/769   train_loss = 3.497
Epoch   8 Batch  618/769   train_loss = 3.426
Epoch   8 Batch  628/769   train_loss = 3.458
Epoch   8 Batch  638/769   train_loss = 3.427
Epoch   8 Batch  648/769   train_loss = 3.554
Epoch   8 Batch  658/769   train_loss = 3.531
Epoch   8 Batch  668/769   train_loss = 3.476
Epoch   8 Batch  678/769   train_loss = 3.503
Epoch   8 Batch  688/769   train_loss = 3.451
Epoch   8 Batch  698/769   train_loss = 3.479
Epoch   8 Batch  708/769   train_loss = 3.469
Epoch   8 Batch  718/769   train_loss = 3.436
Epoch   8 Batch  728/769   train_loss = 3.488
Epoch   8 Batch  738/769   train_loss = 3.449
Epoch   8 Batch  748/769   train_loss = 3.414
Epoch   8 Batch  758/769   train_loss = 3.478
Epoch   8 Batch  768/769   train_loss = 3.423
Epoch   9 Batch    9/769   train_loss = 3.416
Epoch   9 Batch   19/769   train_loss = 3.461
Epoch   9 Batch   29/769   train_loss = 3.472
Epoch   9 Batch   39/769   train_loss = 3.533
Epoch   9 Batch   49/769   train_loss = 3.484
Epoch   9 Batch   59/769   train_loss = 3.412
Epoch   9 Batch   69/769   train_loss = 3.373
Epoch   9 Batch   79/769   train_loss = 3.400
Epoch   9 Batch   89/769   train_loss = 3.419
Epoch   9 Batch   99/769   train_loss = 3.501
Epoch   9 Batch  109/769   train_loss = 3.421
Epoch   9 Batch  119/769   train_loss = 3.459
Epoch   9 Batch  129/769   train_loss = 3.417
Epoch   9 Batch  139/769   train_loss = 3.408
Epoch   9 Batch  149/769   train_loss = 3.451
Epoch   9 Batch  159/769   train_loss = 3.351
Epoch   9 Batch  169/769   train_loss = 3.465
Epoch   9 Batch  179/769   train_loss = 3.516
Epoch   9 Batch  189/769   train_loss = 3.424
Epoch   9 Batch  199/769   train_loss = 3.461
Epoch   9 Batch  209/769   train_loss = 3.427
Epoch   9 Batch  219/769   train_loss = 3.397
Epoch   9 Batch  229/769   train_loss = 3.540
Epoch   9 Batch  239/769   train_loss = 3.446
Epoch   9 Batch  249/769   train_loss = 3.444
Epoch   9 Batch  259/769   train_loss = 3.474
Epoch   9 Batch  269/769   train_loss = 3.410
Epoch   9 Batch  279/769   train_loss = 3.530
Epoch   9 Batch  289/769   train_loss = 3.459
Epoch   9 Batch  299/769   train_loss = 3.474
Epoch   9 Batch  309/769   train_loss = 3.398
Epoch   9 Batch  319/769   train_loss = 3.369
Epoch   9 Batch  329/769   train_loss = 3.394
Epoch   9 Batch  339/769   train_loss = 3.368
Epoch   9 Batch  349/769   train_loss = 3.436
Epoch   9 Batch  359/769   train_loss = 3.477
Epoch   9 Batch  369/769   train_loss = 3.431
Epoch   9 Batch  379/769   train_loss = 3.308
Epoch   9 Batch  389/769   train_loss = 3.340
Epoch   9 Batch  399/769   train_loss = 3.495
Epoch   9 Batch  409/769   train_loss = 3.489
Epoch   9 Batch  419/769   train_loss = 3.491
Epoch   9 Batch  429/769   train_loss = 3.291
Epoch   9 Batch  439/769   train_loss = 3.532
Epoch   9 Batch  449/769   train_loss = 3.363
Epoch   9 Batch  459/769   train_loss = 3.412
Epoch   9 Batch  469/769   train_loss = 3.434
Epoch   9 Batch  479/769   train_loss = 3.451
Epoch   9 Batch  489/769   train_loss = 3.389
Epoch   9 Batch  499/769   train_loss = 3.393
Epoch   9 Batch  509/769   train_loss = 3.424
Epoch   9 Batch  519/769   train_loss = 3.401
Epoch   9 Batch  529/769   train_loss = 3.447
Epoch   9 Batch  539/769   train_loss = 3.344
Epoch   9 Batch  549/769   train_loss = 3.379
Epoch   9 Batch  559/769   train_loss = 3.406
Epoch   9 Batch  569/769   train_loss = 3.274
Epoch   9 Batch  579/769   train_loss = 3.355
Epoch   9 Batch  589/769   train_loss = 3.428
Epoch   9 Batch  599/769   train_loss = 3.387
Epoch   9 Batch  609/769   train_loss = 3.400
Epoch   9 Batch  619/769   train_loss = 3.350
Epoch   9 Batch  629/769   train_loss = 3.346
Epoch   9 Batch  639/769   train_loss = 3.344
Epoch   9 Batch  649/769   train_loss = 3.374
Epoch   9 Batch  659/769   train_loss = 3.365
Epoch   9 Batch  669/769   train_loss = 3.321
Epoch   9 Batch  679/769   train_loss = 3.426
Epoch   9 Batch  689/769   train_loss = 3.369
Epoch   9 Batch  699/769   train_loss = 3.409
Epoch   9 Batch  709/769   train_loss = 3.333
Epoch   9 Batch  719/769   train_loss = 3.320
Epoch   9 Batch  729/769   train_loss = 3.372
Epoch   9 Batch  739/769   train_loss = 3.397
Epoch   9 Batch  749/769   train_loss = 3.297
Epoch   9 Batch  759/769   train_loss = 3.363
Epoch  10 Batch    0/769   train_loss = 3.341
Epoch  10 Batch   10/769   train_loss = 3.336
Epoch  10 Batch   20/769   train_loss = 3.326
Epoch  10 Batch   30/769   train_loss = 3.352
Epoch  10 Batch   40/769   train_loss = 3.409
Epoch  10 Batch   50/769   train_loss = 3.331
Epoch  10 Batch   60/769   train_loss = 3.368
Epoch  10 Batch   70/769   train_loss = 3.339
Epoch  10 Batch   80/769   train_loss = 3.314
Epoch  10 Batch   90/769   train_loss = 3.310
Epoch  10 Batch  100/769   train_loss = 3.280
Epoch  10 Batch  110/769   train_loss = 3.349
Epoch  10 Batch  120/769   train_loss = 3.321
Epoch  10 Batch  130/769   train_loss = 3.340
Epoch  10 Batch  140/769   train_loss = 3.369
Epoch  10 Batch  150/769   train_loss = 3.427
Epoch  10 Batch  160/769   train_loss = 3.283
Epoch  10 Batch  170/769   train_loss = 3.403
Epoch  10 Batch  180/769   train_loss = 3.346
Epoch  10 Batch  190/769   train_loss = 3.376
Epoch  10 Batch  200/769   train_loss = 3.366
Epoch  10 Batch  210/769   train_loss = 3.272
Epoch  10 Batch  220/769   train_loss = 3.331
Epoch  10 Batch  230/769   train_loss = 3.328
Epoch  10 Batch  240/769   train_loss = 3.291
Epoch  10 Batch  250/769   train_loss = 3.231
Epoch  10 Batch  260/769   train_loss = 3.343
Epoch  10 Batch  270/769   train_loss = 3.172
Epoch  10 Batch  280/769   train_loss = 3.236
Epoch  10 Batch  290/769   train_loss = 3.254
Epoch  10 Batch  300/769   train_loss = 3.299
Epoch  10 Batch  310/769   train_loss = 3.163
Epoch  10 Batch  320/769   train_loss = 3.341
Epoch  10 Batch  330/769   train_loss = 3.278
Epoch  10 Batch  340/769   train_loss = 3.298
Epoch  10 Batch  350/769   train_loss = 3.280
Epoch  10 Batch  360/769   train_loss = 3.280
Epoch  10 Batch  370/769   train_loss = 3.271
Epoch  10 Batch  380/769   train_loss = 3.277
Epoch  10 Batch  390/769   train_loss = 3.256
Epoch  10 Batch  400/769   train_loss = 3.278
Epoch  10 Batch  410/769   train_loss = 3.387
Epoch  10 Batch  420/769   train_loss = 3.251
Epoch  10 Batch  430/769   train_loss = 3.279
Epoch  10 Batch  440/769   train_loss = 3.321
Epoch  10 Batch  450/769   train_loss = 3.326
Epoch  10 Batch  460/769   train_loss = 3.283
Epoch  10 Batch  470/769   train_loss = 3.231
Epoch  10 Batch  480/769   train_loss = 3.238
Epoch  10 Batch  490/769   train_loss = 3.297
Epoch  10 Batch  500/769   train_loss = 3.242
Epoch  10 Batch  510/769   train_loss = 3.283
Epoch  10 Batch  520/769   train_loss = 3.134
Epoch  10 Batch  530/769   train_loss = 3.326
Epoch  10 Batch  540/769   train_loss = 3.353
Epoch  10 Batch  550/769   train_loss = 3.193
Epoch  10 Batch  560/769   train_loss = 3.308
Epoch  10 Batch  570/769   train_loss = 3.213
Epoch  10 Batch  580/769   train_loss = 3.296
Epoch  10 Batch  590/769   train_loss = 3.304
Epoch  10 Batch  600/769   train_loss = 3.279
Epoch  10 Batch  610/769   train_loss = 3.301
Epoch  10 Batch  620/769   train_loss = 3.273
Epoch  10 Batch  630/769   train_loss = 3.340
Epoch  10 Batch  640/769   train_loss = 3.310
Epoch  10 Batch  650/769   train_loss = 3.244
Epoch  10 Batch  660/769   train_loss = 3.269
Epoch  10 Batch  670/769   train_loss = 3.144
Epoch  10 Batch  680/769   train_loss = 3.249
Epoch  10 Batch  690/769   train_loss = 3.159
Epoch  10 Batch  700/769   train_loss = 3.235
Epoch  10 Batch  710/769   train_loss = 3.254
Epoch  10 Batch  720/769   train_loss = 3.209
Epoch  10 Batch  730/769   train_loss = 3.188
Epoch  10 Batch  740/769   train_loss = 3.130
Epoch  10 Batch  750/769   train_loss = 3.184
Epoch  10 Batch  760/769   train_loss = 3.142
Epoch  11 Batch    1/769   train_loss = 3.301
Epoch  11 Batch   11/769   train_loss = 3.221
Epoch  11 Batch   21/769   train_loss = 3.182
Epoch  11 Batch   31/769   train_loss = 3.242
Epoch  11 Batch   41/769   train_loss = 3.303
Epoch  11 Batch   51/769   train_loss = 3.244
Epoch  11 Batch   61/769   train_loss = 3.210
Epoch  11 Batch   71/769   train_loss = 3.214
Epoch  11 Batch   81/769   train_loss = 3.280
Epoch  11 Batch   91/769   train_loss = 3.167
Epoch  11 Batch  101/769   train_loss = 3.228
Epoch  11 Batch  111/769   train_loss = 3.199
Epoch  11 Batch  121/769   train_loss = 3.206
Epoch  11 Batch  131/769   train_loss = 3.192
Epoch  11 Batch  141/769   train_loss = 3.203
Epoch  11 Batch  151/769   train_loss = 3.224
Epoch  11 Batch  161/769   train_loss = 3.187
Epoch  11 Batch  171/769   train_loss = 3.249
Epoch  11 Batch  181/769   train_loss = 3.258
Epoch  11 Batch  191/769   train_loss = 3.208
Epoch  11 Batch  201/769   train_loss = 3.251
Epoch  11 Batch  211/769   train_loss = 3.202
Epoch  11 Batch  221/769   train_loss = 3.203
Epoch  11 Batch  231/769   train_loss = 3.118
Epoch  11 Batch  241/769   train_loss = 3.217
Epoch  11 Batch  251/769   train_loss = 3.200
Epoch  11 Batch  261/769   train_loss = 3.127
Epoch  11 Batch  271/769   train_loss = 3.152
Epoch  11 Batch  281/769   train_loss = 3.245
Epoch  11 Batch  291/769   train_loss = 3.242
Epoch  11 Batch  301/769   train_loss = 3.180
Epoch  11 Batch  311/769   train_loss = 3.175
Epoch  11 Batch  321/769   train_loss = 3.215
Epoch  11 Batch  331/769   train_loss = 3.242
Epoch  11 Batch  341/769   train_loss = 3.125
Epoch  11 Batch  351/769   train_loss = 3.274
Epoch  11 Batch  361/769   train_loss = 2.978
Epoch  11 Batch  371/769   train_loss = 3.078
Epoch  11 Batch  381/769   train_loss = 3.215
Epoch  11 Batch  391/769   train_loss = 3.131
Epoch  11 Batch  401/769   train_loss = 3.229
Epoch  11 Batch  411/769   train_loss = 3.078
Epoch  11 Batch  421/769   train_loss = 3.165
Epoch  11 Batch  431/769   train_loss = 3.197
Epoch  11 Batch  441/769   train_loss = 3.160
Epoch  11 Batch  451/769   train_loss = 3.215
Epoch  11 Batch  461/769   train_loss = 3.090
Epoch  11 Batch  471/769   train_loss = 3.188
Epoch  11 Batch  481/769   train_loss = 3.217
Epoch  11 Batch  491/769   train_loss = 3.147
Epoch  11 Batch  501/769   train_loss = 3.170
Epoch  11 Batch  511/769   train_loss = 3.054
Epoch  11 Batch  521/769   train_loss = 3.067
Epoch  11 Batch  531/769   train_loss = 3.177
Epoch  11 Batch  541/769   train_loss = 3.146
Epoch  11 Batch  551/769   train_loss = 3.178
Epoch  11 Batch  561/769   train_loss = 3.139
Epoch  11 Batch  571/769   train_loss = 3.190
Epoch  11 Batch  581/769   train_loss = 3.206
Epoch  11 Batch  591/769   train_loss = 3.218
Epoch  11 Batch  601/769   train_loss = 3.173
Epoch  11 Batch  611/769   train_loss = 3.147
Epoch  11 Batch  621/769   train_loss = 3.203
Epoch  11 Batch  631/769   train_loss = 3.178
Epoch  11 Batch  641/769   train_loss = 3.197
Epoch  11 Batch  651/769   train_loss = 3.171
Epoch  11 Batch  661/769   train_loss = 3.156
Epoch  11 Batch  671/769   train_loss = 3.098
Epoch  11 Batch  681/769   train_loss = 3.106
Epoch  11 Batch  691/769   train_loss = 3.086
Epoch  11 Batch  701/769   train_loss = 3.101
Epoch  11 Batch  711/769   train_loss = 3.121
Epoch  11 Batch  721/769   train_loss = 3.174
Epoch  11 Batch  731/769   train_loss = 3.174
Epoch  11 Batch  741/769   train_loss = 3.100
Epoch  11 Batch  751/769   train_loss = 3.047
Epoch  11 Batch  761/769   train_loss = 3.043
Epoch  12 Batch    2/769   train_loss = 3.125
Epoch  12 Batch   12/769   train_loss = 3.099
Epoch  12 Batch   22/769   train_loss = 3.175
Epoch  12 Batch   32/769   train_loss = 3.173
Epoch  12 Batch   42/769   train_loss = 3.222
Epoch  12 Batch   52/769   train_loss = 3.097
Epoch  12 Batch   62/769   train_loss = 3.168
Epoch  12 Batch   72/769   train_loss = 3.175
Epoch  12 Batch   82/769   train_loss = 3.015
Epoch  12 Batch   92/769   train_loss = 3.156
Epoch  12 Batch  102/769   train_loss = 3.109
Epoch  12 Batch  112/769   train_loss = 3.095
Epoch  12 Batch  122/769   train_loss = 3.099
Epoch  12 Batch  132/769   train_loss = 3.187
Epoch  12 Batch  142/769   train_loss = 3.115
Epoch  12 Batch  152/769   train_loss = 3.152
Epoch  12 Batch  162/769   train_loss = 3.102
Epoch  12 Batch  172/769   train_loss = 3.180
Epoch  12 Batch  182/769   train_loss = 3.120
Epoch  12 Batch  192/769   train_loss = 3.063
Epoch  12 Batch  202/769   train_loss = 3.097
Epoch  12 Batch  212/769   train_loss = 3.089
Epoch  12 Batch  222/769   train_loss = 3.138
Epoch  12 Batch  232/769   train_loss = 3.079
Epoch  12 Batch  242/769   train_loss = 3.077
Epoch  12 Batch  252/769   train_loss = 3.136
Epoch  12 Batch  262/769   train_loss = 2.985
Epoch  12 Batch  272/769   train_loss = 2.995
Epoch  12 Batch  282/769   train_loss = 3.055
Epoch  12 Batch  292/769   train_loss = 3.051
Epoch  12 Batch  302/769   train_loss = 3.036
Epoch  12 Batch  312/769   train_loss = 3.060
Epoch  12 Batch  322/769   train_loss = 3.092
Epoch  12 Batch  332/769   train_loss = 3.100
Epoch  12 Batch  342/769   train_loss = 3.049
Epoch  12 Batch  352/769   train_loss = 3.017
Epoch  12 Batch  362/769   train_loss = 3.065
Epoch  12 Batch  372/769   train_loss = 3.065
Epoch  12 Batch  382/769   train_loss = 3.056
Epoch  12 Batch  392/769   train_loss = 3.138
Epoch  12 Batch  402/769   train_loss = 3.201
Epoch  12 Batch  412/769   train_loss = 3.084
Epoch  12 Batch  422/769   train_loss = 2.983
Epoch  12 Batch  432/769   train_loss = 3.059
Epoch  12 Batch  442/769   train_loss = 3.030
Epoch  12 Batch  452/769   train_loss = 3.052
Epoch  12 Batch  462/769   train_loss = 3.044
Epoch  12 Batch  472/769   train_loss = 3.107
Epoch  12 Batch  482/769   train_loss = 3.076
Epoch  12 Batch  492/769   train_loss = 3.142
Epoch  12 Batch  502/769   train_loss = 3.030
Epoch  12 Batch  512/769   train_loss = 2.945
Epoch  12 Batch  522/769   train_loss = 2.994
Epoch  12 Batch  532/769   train_loss = 3.096
Epoch  12 Batch  542/769   train_loss = 3.100
Epoch  12 Batch  552/769   train_loss = 3.061
Epoch  12 Batch  562/769   train_loss = 3.024
Epoch  12 Batch  572/769   train_loss = 3.079
Epoch  12 Batch  582/769   train_loss = 3.047
Epoch  12 Batch  592/769   train_loss = 3.039
Epoch  12 Batch  602/769   train_loss = 3.100
Epoch  12 Batch  612/769   train_loss = 3.057
Epoch  12 Batch  622/769   train_loss = 3.021
Epoch  12 Batch  632/769   train_loss = 3.036
Epoch  12 Batch  642/769   train_loss = 3.069
Epoch  12 Batch  652/769   train_loss = 2.991
Epoch  12 Batch  662/769   train_loss = 3.013
Epoch  12 Batch  672/769   train_loss = 3.101
Epoch  12 Batch  682/769   train_loss = 3.003
Epoch  12 Batch  692/769   train_loss = 3.088
Epoch  12 Batch  702/769   train_loss = 3.100
Epoch  12 Batch  712/769   train_loss = 3.029
Epoch  12 Batch  722/769   train_loss = 3.024
Epoch  12 Batch  732/769   train_loss = 2.982
Epoch  12 Batch  742/769   train_loss = 2.955
Epoch  12 Batch  752/769   train_loss = 3.150
Epoch  12 Batch  762/769   train_loss = 3.073
Epoch  13 Batch    3/769   train_loss = 2.993
Epoch  13 Batch   13/769   train_loss = 3.064
Epoch  13 Batch   23/769   train_loss = 3.097
Epoch  13 Batch   33/769   train_loss = 3.068
Epoch  13 Batch   43/769   train_loss = 3.090
Epoch  13 Batch   53/769   train_loss = 3.032
Epoch  13 Batch   63/769   train_loss = 2.947
Epoch  13 Batch   73/769   train_loss = 3.026
Epoch  13 Batch   83/769   train_loss = 2.971
Epoch  13 Batch   93/769   train_loss = 3.005
Epoch  13 Batch  103/769   train_loss = 3.016
Epoch  13 Batch  113/769   train_loss = 3.024
Epoch  13 Batch  123/769   train_loss = 3.041
Epoch  13 Batch  133/769   train_loss = 3.014
Epoch  13 Batch  143/769   train_loss = 3.017
Epoch  13 Batch  153/769   train_loss = 2.975
Epoch  13 Batch  163/769   train_loss = 3.033
Epoch  13 Batch  173/769   train_loss = 3.026
Epoch  13 Batch  183/769   train_loss = 3.043
Epoch  13 Batch  193/769   train_loss = 2.991
Epoch  13 Batch  203/769   train_loss = 2.933
Epoch  13 Batch  213/769   train_loss = 3.032
Epoch  13 Batch  223/769   train_loss = 3.054
Epoch  13 Batch  233/769   train_loss = 3.002
Epoch  13 Batch  243/769   train_loss = 3.058
Epoch  13 Batch  253/769   train_loss = 3.044
Epoch  13 Batch  263/769   train_loss = 2.997
Epoch  13 Batch  273/769   train_loss = 3.023
Epoch  13 Batch  283/769   train_loss = 3.003
Epoch  13 Batch  293/769   train_loss = 2.917
Epoch  13 Batch  303/769   train_loss = 3.017
Epoch  13 Batch  313/769   train_loss = 2.964
Epoch  13 Batch  323/769   train_loss = 3.003
Epoch  13 Batch  333/769   train_loss = 3.019
Epoch  13 Batch  343/769   train_loss = 2.941
Epoch  13 Batch  353/769   train_loss = 2.975
Epoch  13 Batch  363/769   train_loss = 2.984
Epoch  13 Batch  373/769   train_loss = 3.028
Epoch  13 Batch  383/769   train_loss = 2.983
Epoch  13 Batch  393/769   train_loss = 3.027
Epoch  13 Batch  403/769   train_loss = 2.978
Epoch  13 Batch  413/769   train_loss = 2.992
Epoch  13 Batch  423/769   train_loss = 3.000
Epoch  13 Batch  433/769   train_loss = 2.954
Epoch  13 Batch  443/769   train_loss = 3.061
Epoch  13 Batch  453/769   train_loss = 2.988
Epoch  13 Batch  463/769   train_loss = 2.999
Epoch  13 Batch  473/769   train_loss = 2.999
Epoch  13 Batch  483/769   train_loss = 2.971
Epoch  13 Batch  493/769   train_loss = 2.984
Epoch  13 Batch  503/769   train_loss = 3.058
Epoch  13 Batch  513/769   train_loss = 3.011
Epoch  13 Batch  523/769   train_loss = 2.967
Epoch  13 Batch  533/769   train_loss = 2.938
Epoch  13 Batch  543/769   train_loss = 2.984
Epoch  13 Batch  553/769   train_loss = 2.966
Epoch  13 Batch  563/769   train_loss = 2.934
Epoch  13 Batch  573/769   train_loss = 2.945
Epoch  13 Batch  583/769   train_loss = 3.018
Epoch  13 Batch  593/769   train_loss = 2.983
Epoch  13 Batch  603/769   train_loss = 2.895
Epoch  13 Batch  613/769   train_loss = 2.971
Epoch  13 Batch  623/769   train_loss = 2.908
Epoch  13 Batch  633/769   train_loss = 2.857
Epoch  13 Batch  643/769   train_loss = 2.943
Epoch  13 Batch  653/769   train_loss = 2.897
Epoch  13 Batch  663/769   train_loss = 2.939
Epoch  13 Batch  673/769   train_loss = 2.967
Epoch  13 Batch  683/769   train_loss = 2.952
Epoch  13 Batch  693/769   train_loss = 2.968
Epoch  13 Batch  703/769   train_loss = 2.882
Epoch  13 Batch  713/769   train_loss = 2.921
Epoch  13 Batch  723/769   train_loss = 2.891
Epoch  13 Batch  733/769   train_loss = 2.935
Epoch  13 Batch  743/769   train_loss = 2.949
Epoch  13 Batch  753/769   train_loss = 2.947
Epoch  13 Batch  763/769   train_loss = 2.980
Epoch  14 Batch    4/769   train_loss = 2.940
Epoch  14 Batch   14/769   train_loss = 2.882
Epoch  14 Batch   24/769   train_loss = 2.941
Epoch  14 Batch   34/769   train_loss = 2.929
Epoch  14 Batch   44/769   train_loss = 2.990
Epoch  14 Batch   54/769   train_loss = 2.903
Epoch  14 Batch   64/769   train_loss = 2.863
Epoch  14 Batch   74/769   train_loss = 2.959
Epoch  14 Batch   84/769   train_loss = 2.884
Epoch  14 Batch   94/769   train_loss = 2.887
Epoch  14 Batch  104/769   train_loss = 2.925
Epoch  14 Batch  114/769   train_loss = 2.986
Epoch  14 Batch  124/769   train_loss = 2.925
Epoch  14 Batch  134/769   train_loss = 2.906
Epoch  14 Batch  144/769   train_loss = 2.896
Epoch  14 Batch  154/769   train_loss = 2.973
Epoch  14 Batch  164/769   train_loss = 2.865
Epoch  14 Batch  174/769   train_loss = 2.895
Epoch  14 Batch  184/769   train_loss = 2.904
Epoch  14 Batch  194/769   train_loss = 2.815
Epoch  14 Batch  204/769   train_loss = 2.833
Epoch  14 Batch  214/769   train_loss = 2.900
Epoch  14 Batch  224/769   train_loss = 2.947
Epoch  14 Batch  234/769   train_loss = 2.982
Epoch  14 Batch  244/769   train_loss = 3.000
Epoch  14 Batch  254/769   train_loss = 2.914
Epoch  14 Batch  264/769   train_loss = 2.944
Epoch  14 Batch  274/769   train_loss = 2.820
Epoch  14 Batch  284/769   train_loss = 2.805
Epoch  14 Batch  294/769   train_loss = 2.867
Epoch  14 Batch  304/769   train_loss = 2.927
Epoch  14 Batch  314/769   train_loss = 2.909
Epoch  14 Batch  324/769   train_loss = 2.933
Epoch  14 Batch  334/769   train_loss = 2.877
Epoch  14 Batch  344/769   train_loss = 2.920
Epoch  14 Batch  354/769   train_loss = 3.069
Epoch  14 Batch  364/769   train_loss = 2.897
Epoch  14 Batch  374/769   train_loss = 2.917
Epoch  14 Batch  384/769   train_loss = 2.812
Epoch  14 Batch  394/769   train_loss = 2.907
Epoch  14 Batch  404/769   train_loss = 2.972
Epoch  14 Batch  414/769   train_loss = 2.828
Epoch  14 Batch  424/769   train_loss = 2.910
Epoch  14 Batch  434/769   train_loss = 2.911
Epoch  14 Batch  444/769   train_loss = 2.927
Epoch  14 Batch  454/769   train_loss = 2.897
Epoch  14 Batch  464/769   train_loss = 2.999
Epoch  14 Batch  474/769   train_loss = 2.890
Epoch  14 Batch  484/769   train_loss = 2.884
Epoch  14 Batch  494/769   train_loss = 2.793
Epoch  14 Batch  504/769   train_loss = 2.953
Epoch  14 Batch  514/769   train_loss = 2.879
Epoch  14 Batch  524/769   train_loss = 2.906
Epoch  14 Batch  534/769   train_loss = 2.894
Epoch  14 Batch  544/769   train_loss = 2.911
Epoch  14 Batch  554/769   train_loss = 2.954
Epoch  14 Batch  564/769   train_loss = 2.916
Epoch  14 Batch  574/769   train_loss = 2.853
Epoch  14 Batch  584/769   train_loss = 2.868
Epoch  14 Batch  594/769   train_loss = 2.876
Epoch  14 Batch  604/769   train_loss = 2.837
Epoch  14 Batch  614/769   train_loss = 2.935
Epoch  14 Batch  624/769   train_loss = 2.938
Epoch  14 Batch  634/769   train_loss = 2.880
Epoch  14 Batch  644/769   train_loss = 2.829
Epoch  14 Batch  654/769   train_loss = 2.896
Epoch  14 Batch  664/769   train_loss = 2.868
Epoch  14 Batch  674/769   train_loss = 2.904
Epoch  14 Batch  684/769   train_loss = 2.882
Epoch  14 Batch  694/769   train_loss = 2.884
Epoch  14 Batch  704/769   train_loss = 2.846
Epoch  14 Batch  714/769   train_loss = 2.784
Epoch  14 Batch  724/769   train_loss = 2.877
Epoch  14 Batch  734/769   train_loss = 2.930
Epoch  14 Batch  744/769   train_loss = 2.803
Epoch  14 Batch  754/769   train_loss = 2.965
Epoch  14 Batch  764/769   train_loss = 2.863
Epoch  15 Batch    5/769   train_loss = 2.897
Epoch  15 Batch   15/769   train_loss = 2.861
Epoch  15 Batch   25/769   train_loss = 2.921
Epoch  15 Batch   35/769   train_loss = 2.905
Epoch  15 Batch   45/769   train_loss = 2.897
Epoch  15 Batch   55/769   train_loss = 2.866
Epoch  15 Batch   65/769   train_loss = 2.807
Epoch  15 Batch   75/769   train_loss = 2.917
Epoch  15 Batch   85/769   train_loss = 2.917
Epoch  15 Batch   95/769   train_loss = 2.796
Epoch  15 Batch  105/769   train_loss = 2.881
Epoch  15 Batch  115/769   train_loss = 2.796
Epoch  15 Batch  125/769   train_loss = 2.800
Epoch  15 Batch  135/769   train_loss = 2.846
Epoch  15 Batch  145/769   train_loss = 2.844
Epoch  15 Batch  155/769   train_loss = 2.856
Epoch  15 Batch  165/769   train_loss = 2.860
Epoch  15 Batch  175/769   train_loss = 2.813
Epoch  15 Batch  185/769   train_loss = 2.759
Epoch  15 Batch  195/769   train_loss = 2.829
Epoch  15 Batch  205/769   train_loss = 2.864
Epoch  15 Batch  215/769   train_loss = 2.890
Epoch  15 Batch  225/769   train_loss = 2.845
Epoch  15 Batch  235/769   train_loss = 2.837
Epoch  15 Batch  245/769   train_loss = 2.872
Epoch  15 Batch  255/769   train_loss = 2.843
Epoch  15 Batch  265/769   train_loss = 2.775
Epoch  15 Batch  275/769   train_loss = 2.806
Epoch  15 Batch  285/769   train_loss = 2.859
Epoch  15 Batch  295/769   train_loss = 2.817
Epoch  15 Batch  305/769   train_loss = 2.868
Epoch  15 Batch  315/769   train_loss = 2.811
Epoch  15 Batch  325/769   train_loss = 2.850
Epoch  15 Batch  335/769   train_loss = 2.898
Epoch  15 Batch  345/769   train_loss = 2.772
Epoch  15 Batch  355/769   train_loss = 2.828
Epoch  15 Batch  365/769   train_loss = 2.848
Epoch  15 Batch  375/769   train_loss = 2.842
Epoch  15 Batch  385/769   train_loss = 2.739
Epoch  15 Batch  395/769   train_loss = 2.818
Epoch  15 Batch  405/769   train_loss = 2.897
Epoch  15 Batch  415/769   train_loss = 2.680
Epoch  15 Batch  425/769   train_loss = 2.834
Epoch  15 Batch  435/769   train_loss = 2.798
Epoch  15 Batch  445/769   train_loss = 2.856
Epoch  15 Batch  455/769   train_loss = 2.866
Epoch  15 Batch  465/769   train_loss = 2.912
Epoch  15 Batch  475/769   train_loss = 2.766
Epoch  15 Batch  485/769   train_loss = 2.894
Epoch  15 Batch  495/769   train_loss = 2.739
Epoch  15 Batch  505/769   train_loss = 2.787
Epoch  15 Batch  515/769   train_loss = 2.833
Epoch  15 Batch  525/769   train_loss = 2.805
Epoch  15 Batch  535/769   train_loss = 2.792
Epoch  15 Batch  545/769   train_loss = 2.810
Epoch  15 Batch  555/769   train_loss = 2.841
Epoch  15 Batch  565/769   train_loss = 2.928
Epoch  15 Batch  575/769   train_loss = 2.858
Epoch  15 Batch  585/769   train_loss = 2.876
Epoch  15 Batch  595/769   train_loss = 2.777
Epoch  15 Batch  605/769   train_loss = 2.748
Epoch  15 Batch  615/769   train_loss = 2.868
Epoch  15 Batch  625/769   train_loss = 2.826
Epoch  15 Batch  635/769   train_loss = 2.741
Epoch  15 Batch  645/769   train_loss = 2.790
Epoch  15 Batch  655/769   train_loss = 2.734
Epoch  15 Batch  665/769   train_loss = 2.778
Epoch  15 Batch  675/769   train_loss = 2.789
Epoch  15 Batch  685/769   train_loss = 2.787
Epoch  15 Batch  695/769   train_loss = 2.869
Epoch  15 Batch  705/769   train_loss = 2.770
Epoch  15 Batch  715/769   train_loss = 2.838
Epoch  15 Batch  725/769   train_loss = 2.794
Epoch  15 Batch  735/769   train_loss = 2.793
Epoch  15 Batch  745/769   train_loss = 2.864
Epoch  15 Batch  755/769   train_loss = 2.705
Epoch  15 Batch  765/769   train_loss = 2.838
Epoch  16 Batch    6/769   train_loss = 2.719
Epoch  16 Batch   16/769   train_loss = 2.782
Epoch  16 Batch   26/769   train_loss = 2.811
Epoch  16 Batch   36/769   train_loss = 2.856
Epoch  16 Batch   46/769   train_loss = 2.820
Epoch  16 Batch   56/769   train_loss = 2.764
Epoch  16 Batch   66/769   train_loss = 2.784
Epoch  16 Batch   76/769   train_loss = 2.782
Epoch  16 Batch   86/769   train_loss = 2.882
Epoch  16 Batch   96/769   train_loss = 2.708
Epoch  16 Batch  106/769   train_loss = 2.842
Epoch  16 Batch  116/769   train_loss = 2.761
Epoch  16 Batch  126/769   train_loss = 2.745
Epoch  16 Batch  136/769   train_loss = 2.802
Epoch  16 Batch  146/769   train_loss = 2.742
Epoch  16 Batch  156/769   train_loss = 2.751
Epoch  16 Batch  166/769   train_loss = 2.772
Epoch  16 Batch  176/769   train_loss = 2.730
Epoch  16 Batch  186/769   train_loss = 2.653
Epoch  16 Batch  196/769   train_loss = 2.825
Epoch  16 Batch  206/769   train_loss = 2.790
Epoch  16 Batch  216/769   train_loss = 2.709
Epoch  16 Batch  226/769   train_loss = 2.737
Epoch  16 Batch  236/769   train_loss = 2.762
Epoch  16 Batch  246/769   train_loss = 2.779
Epoch  16 Batch  256/769   train_loss = 2.686
Epoch  16 Batch  266/769   train_loss = 2.736
Epoch  16 Batch  276/769   train_loss = 2.706
Epoch  16 Batch  286/769   train_loss = 2.714
Epoch  16 Batch  296/769   train_loss = 2.740
Epoch  16 Batch  306/769   train_loss = 2.707
Epoch  16 Batch  316/769   train_loss = 2.705
Epoch  16 Batch  326/769   train_loss = 2.794
Epoch  16 Batch  336/769   train_loss = 2.790
Epoch  16 Batch  346/769   train_loss = 2.736
Epoch  16 Batch  356/769   train_loss = 2.705
Epoch  16 Batch  366/769   train_loss = 2.816
Epoch  16 Batch  376/769   train_loss = 2.708
Epoch  16 Batch  386/769   train_loss = 2.813
Epoch  16 Batch  396/769   train_loss = 2.740
Epoch  16 Batch  406/769   train_loss = 2.757
Epoch  16 Batch  416/769   train_loss = 2.817
Epoch  16 Batch  426/769   train_loss = 2.832
Epoch  16 Batch  436/769   train_loss = 2.701
Epoch  16 Batch  446/769   train_loss = 2.782
Epoch  16 Batch  456/769   train_loss = 2.803
Epoch  16 Batch  466/769   train_loss = 2.786
Epoch  16 Batch  476/769   train_loss = 2.718
Epoch  16 Batch  486/769   train_loss = 2.782
Epoch  16 Batch  496/769   train_loss = 2.720
Epoch  16 Batch  506/769   train_loss = 2.767
Epoch  16 Batch  516/769   train_loss = 2.752
Epoch  16 Batch  526/769   train_loss = 2.709
Epoch  16 Batch  536/769   train_loss = 2.764
Epoch  16 Batch  546/769   train_loss = 2.814
Epoch  16 Batch  556/769   train_loss = 2.746
Epoch  16 Batch  566/769   train_loss = 2.818
Epoch  16 Batch  576/769   train_loss = 2.678
Epoch  16 Batch  586/769   train_loss = 2.707
Epoch  16 Batch  596/769   train_loss = 2.806
Epoch  16 Batch  606/769   train_loss = 2.785
Epoch  16 Batch  616/769   train_loss = 2.650
Epoch  16 Batch  626/769   train_loss = 2.753
Epoch  16 Batch  636/769   train_loss = 2.675
Epoch  16 Batch  646/769   train_loss = 2.818
Epoch  16 Batch  656/769   train_loss = 2.726
Epoch  16 Batch  666/769   train_loss = 2.755
Epoch  16 Batch  676/769   train_loss = 2.774
Epoch  16 Batch  686/769   train_loss = 2.762
Epoch  16 Batch  696/769   train_loss = 2.732
Epoch  16 Batch  706/769   train_loss = 2.778
Epoch  16 Batch  716/769   train_loss = 2.784
Epoch  16 Batch  726/769   train_loss = 2.727
Epoch  16 Batch  736/769   train_loss = 2.749
Epoch  16 Batch  746/769   train_loss = 2.708
Epoch  16 Batch  756/769   train_loss = 2.642
Epoch  16 Batch  766/769   train_loss = 2.731
Epoch  17 Batch    7/769   train_loss = 2.706
Epoch  17 Batch   17/769   train_loss = 2.698
Epoch  17 Batch   27/769   train_loss = 2.723
Epoch  17 Batch   37/769   train_loss = 2.631
Epoch  17 Batch   47/769   train_loss = 2.635
Epoch  17 Batch   57/769   train_loss = 2.751
Epoch  17 Batch   67/769   train_loss = 2.725
Epoch  17 Batch   77/769   train_loss = 2.798
Epoch  17 Batch   87/769   train_loss = 2.666
Epoch  17 Batch   97/769   train_loss = 2.641
Epoch  17 Batch  107/769   train_loss = 2.695
Epoch  17 Batch  117/769   train_loss = 2.709
Epoch  17 Batch  127/769   train_loss = 2.729
Epoch  17 Batch  137/769   train_loss = 2.700
Epoch  17 Batch  147/769   train_loss = 2.732
Epoch  17 Batch  157/769   train_loss = 2.763
Epoch  17 Batch  167/769   train_loss = 2.688
Epoch  17 Batch  177/769   train_loss = 2.604
Epoch  17 Batch  187/769   train_loss = 2.738
Epoch  17 Batch  197/769   train_loss = 2.616
Epoch  17 Batch  207/769   train_loss = 2.698
Epoch  17 Batch  217/769   train_loss = 2.729
Epoch  17 Batch  227/769   train_loss = 2.679
Epoch  17 Batch  237/769   train_loss = 2.679
Epoch  17 Batch  247/769   train_loss = 2.730
Epoch  17 Batch  257/769   train_loss = 2.682
Epoch  17 Batch  267/769   train_loss = 2.652
Epoch  17 Batch  277/769   train_loss = 2.685
Epoch  17 Batch  287/769   train_loss = 2.669
Epoch  17 Batch  297/769   train_loss = 2.668
Epoch  17 Batch  307/769   train_loss = 2.749
Epoch  17 Batch  317/769   train_loss = 2.565
Epoch  17 Batch  327/769   train_loss = 2.689
Epoch  17 Batch  337/769   train_loss = 2.686
Epoch  17 Batch  347/769   train_loss = 2.701
Epoch  17 Batch  357/769   train_loss = 2.716
Epoch  17 Batch  367/769   train_loss = 2.681
Epoch  17 Batch  377/769   train_loss = 2.715
Epoch  17 Batch  387/769   train_loss = 2.787
Epoch  17 Batch  397/769   train_loss = 2.677
Epoch  17 Batch  407/769   train_loss = 2.686
Epoch  17 Batch  417/769   train_loss = 2.734
Epoch  17 Batch  427/769   train_loss = 2.667
Epoch  17 Batch  437/769   train_loss = 2.726
Epoch  17 Batch  447/769   train_loss = 2.728
Epoch  17 Batch  457/769   train_loss = 2.616
Epoch  17 Batch  467/769   train_loss = 2.671
Epoch  17 Batch  477/769   train_loss = 2.709
Epoch  17 Batch  487/769   train_loss = 2.665
Epoch  17 Batch  497/769   train_loss = 2.742
Epoch  17 Batch  507/769   train_loss = 2.649
Epoch  17 Batch  517/769   train_loss = 2.668
Epoch  17 Batch  527/769   train_loss = 2.655
Epoch  17 Batch  537/769   train_loss = 2.720
Epoch  17 Batch  547/769   train_loss = 2.668
Epoch  17 Batch  557/769   train_loss = 2.675
Epoch  17 Batch  567/769   train_loss = 2.707
Epoch  17 Batch  577/769   train_loss = 2.728
Epoch  17 Batch  587/769   train_loss = 2.606
Epoch  17 Batch  597/769   train_loss = 2.660
Epoch  17 Batch  607/769   train_loss = 2.701
Epoch  17 Batch  617/769   train_loss = 2.708
Epoch  17 Batch  627/769   train_loss = 2.750
Epoch  17 Batch  637/769   train_loss = 2.618
Epoch  17 Batch  647/769   train_loss = 2.681
Epoch  17 Batch  657/769   train_loss = 2.616
Epoch  17 Batch  667/769   train_loss = 2.702
Epoch  17 Batch  677/769   train_loss = 2.639
Epoch  17 Batch  687/769   train_loss = 2.714
Epoch  17 Batch  697/769   train_loss = 2.676
Epoch  17 Batch  707/769   train_loss = 2.646
Epoch  17 Batch  717/769   train_loss = 2.614
Epoch  17 Batch  727/769   train_loss = 2.654
Epoch  17 Batch  737/769   train_loss = 2.715
Epoch  17 Batch  747/769   train_loss = 2.604
Epoch  17 Batch  757/769   train_loss = 2.697
Epoch  17 Batch  767/769   train_loss = 2.684
Epoch  18 Batch    8/769   train_loss = 2.560
Epoch  18 Batch   18/769   train_loss = 2.675
Epoch  18 Batch   28/769   train_loss = 2.688
Epoch  18 Batch   38/769   train_loss = 2.722
Epoch  18 Batch   48/769   train_loss = 2.637
Epoch  18 Batch   58/769   train_loss = 2.638
Epoch  18 Batch   68/769   train_loss = 2.667
Epoch  18 Batch   78/769   train_loss = 2.685
Epoch  18 Batch   88/769   train_loss = 2.562
Epoch  18 Batch   98/769   train_loss = 2.551
Epoch  18 Batch  108/769   train_loss = 2.681
Epoch  18 Batch  118/769   train_loss = 2.589
Epoch  18 Batch  128/769   train_loss = 2.674
Epoch  18 Batch  138/769   train_loss = 2.607
Epoch  18 Batch  148/769   train_loss = 2.713
Epoch  18 Batch  158/769   train_loss = 2.665
Epoch  18 Batch  168/769   train_loss = 2.642
Epoch  18 Batch  178/769   train_loss = 2.642
Epoch  18 Batch  188/769   train_loss = 2.642
Epoch  18 Batch  198/769   train_loss = 2.647
Epoch  18 Batch  208/769   train_loss = 2.625
Epoch  18 Batch  218/769   train_loss = 2.646
Epoch  18 Batch  228/769   train_loss = 2.610
Epoch  18 Batch  238/769   train_loss = 2.669
Epoch  18 Batch  248/769   train_loss = 2.669
Epoch  18 Batch  258/769   train_loss = 2.613
Epoch  18 Batch  268/769   train_loss = 2.592
Epoch  18 Batch  278/769   train_loss = 2.666
Epoch  18 Batch  288/769   train_loss = 2.669
Epoch  18 Batch  298/769   train_loss = 2.676
Epoch  18 Batch  308/769   train_loss = 2.650
Epoch  18 Batch  318/769   train_loss = 2.581
Epoch  18 Batch  328/769   train_loss = 2.628
Epoch  18 Batch  338/769   train_loss = 2.652
Epoch  18 Batch  348/769   train_loss = 2.631
Epoch  18 Batch  358/769   train_loss = 2.645
Epoch  18 Batch  368/769   train_loss = 2.657
Epoch  18 Batch  378/769   train_loss = 2.658
Epoch  18 Batch  388/769   train_loss = 2.605
Epoch  18 Batch  398/769   train_loss = 2.718
Epoch  18 Batch  408/769   train_loss = 2.628
Epoch  18 Batch  418/769   train_loss = 2.659
Epoch  18 Batch  428/769   train_loss = 2.618
Epoch  18 Batch  438/769   train_loss = 2.527
Epoch  18 Batch  448/769   train_loss = 2.622
Epoch  18 Batch  458/769   train_loss = 2.656
Epoch  18 Batch  468/769   train_loss = 2.638
Epoch  18 Batch  478/769   train_loss = 2.560
Epoch  18 Batch  488/769   train_loss = 2.606
Epoch  18 Batch  498/769   train_loss = 2.621
Epoch  18 Batch  508/769   train_loss = 2.624
Epoch  18 Batch  518/769   train_loss = 2.622
Epoch  18 Batch  528/769   train_loss = 2.542
Epoch  18 Batch  538/769   train_loss = 2.597
Epoch  18 Batch  548/769   train_loss = 2.623
Epoch  18 Batch  558/769   train_loss = 2.597
Epoch  18 Batch  568/769   train_loss = 2.544
Epoch  18 Batch  578/769   train_loss = 2.508
Epoch  18 Batch  588/769   train_loss = 2.562
Epoch  18 Batch  598/769   train_loss = 2.564
Epoch  18 Batch  608/769   train_loss = 2.562
Epoch  18 Batch  618/769   train_loss = 2.543
Epoch  18 Batch  628/769   train_loss = 2.566
Epoch  18 Batch  638/769   train_loss = 2.521
Epoch  18 Batch  648/769   train_loss = 2.673
Epoch  18 Batch  658/769   train_loss = 2.681
Epoch  18 Batch  668/769   train_loss = 2.554
Epoch  18 Batch  678/769   train_loss = 2.584
Epoch  18 Batch  688/769   train_loss = 2.577
Epoch  18 Batch  698/769   train_loss = 2.622
Epoch  18 Batch  708/769   train_loss = 2.584
Epoch  18 Batch  718/769   train_loss = 2.548
Epoch  18 Batch  728/769   train_loss = 2.648
Epoch  18 Batch  738/769   train_loss = 2.583
Epoch  18 Batch  748/769   train_loss = 2.616
Epoch  18 Batch  758/769   train_loss = 2.616
Epoch  18 Batch  768/769   train_loss = 2.613
Epoch  19 Batch    9/769   train_loss = 2.601
Epoch  19 Batch   19/769   train_loss = 2.628
Epoch  19 Batch   29/769   train_loss = 2.634
Epoch  19 Batch   39/769   train_loss = 2.627
Epoch  19 Batch   49/769   train_loss = 2.546
Epoch  19 Batch   59/769   train_loss = 2.510
Epoch  19 Batch   69/769   train_loss = 2.568
Epoch  19 Batch   79/769   train_loss = 2.561
Epoch  19 Batch   89/769   train_loss = 2.485
Epoch  19 Batch   99/769   train_loss = 2.580
Epoch  19 Batch  109/769   train_loss = 2.553
Epoch  19 Batch  119/769   train_loss = 2.553
Epoch  19 Batch  129/769   train_loss = 2.534
Epoch  19 Batch  139/769   train_loss = 2.548
Epoch  19 Batch  149/769   train_loss = 2.550
Epoch  19 Batch  159/769   train_loss = 2.472
Epoch  19 Batch  169/769   train_loss = 2.641
Epoch  19 Batch  179/769   train_loss = 2.561
Epoch  19 Batch  189/769   train_loss = 2.574
Epoch  19 Batch  199/769   train_loss = 2.539
Epoch  19 Batch  209/769   train_loss = 2.583
Epoch  19 Batch  219/769   train_loss = 2.581
Epoch  19 Batch  229/769   train_loss = 2.636
Epoch  19 Batch  239/769   train_loss = 2.529
Epoch  19 Batch  249/769   train_loss = 2.587
Epoch  19 Batch  259/769   train_loss = 2.587
Epoch  19 Batch  269/769   train_loss = 2.551
Epoch  19 Batch  279/769   train_loss = 2.622
Epoch  19 Batch  289/769   train_loss = 2.571
Epoch  19 Batch  299/769   train_loss = 2.687
Epoch  19 Batch  309/769   train_loss = 2.596
Epoch  19 Batch  319/769   train_loss = 2.608
Epoch  19 Batch  329/769   train_loss = 2.572
Epoch  19 Batch  339/769   train_loss = 2.521
Epoch  19 Batch  349/769   train_loss = 2.569
Epoch  19 Batch  359/769   train_loss = 2.643
Epoch  19 Batch  369/769   train_loss = 2.513
Epoch  19 Batch  379/769   train_loss = 2.463
Epoch  19 Batch  389/769   train_loss = 2.476
Epoch  19 Batch  399/769   train_loss = 2.654
Epoch  19 Batch  409/769   train_loss = 2.658
Epoch  19 Batch  419/769   train_loss = 2.677
Epoch  19 Batch  429/769   train_loss = 2.523
Epoch  19 Batch  439/769   train_loss = 2.625
Epoch  19 Batch  449/769   train_loss = 2.593
Epoch  19 Batch  459/769   train_loss = 2.547
Epoch  19 Batch  469/769   train_loss = 2.492
Epoch  19 Batch  479/769   train_loss = 2.593
Epoch  19 Batch  489/769   train_loss = 2.576
Epoch  19 Batch  499/769   train_loss = 2.549
Epoch  19 Batch  509/769   train_loss = 2.513
Epoch  19 Batch  519/769   train_loss = 2.584
Epoch  19 Batch  529/769   train_loss = 2.613
Epoch  19 Batch  539/769   train_loss = 2.528
Epoch  19 Batch  549/769   train_loss = 2.588
Epoch  19 Batch  559/769   train_loss = 2.575
Epoch  19 Batch  569/769   train_loss = 2.468
Epoch  19 Batch  579/769   train_loss = 2.495
Epoch  19 Batch  589/769   train_loss = 2.597
Epoch  19 Batch  599/769   train_loss = 2.543
Epoch  19 Batch  609/769   train_loss = 2.573
Epoch  19 Batch  619/769   train_loss = 2.552
Epoch  19 Batch  629/769   train_loss = 2.518
Epoch  19 Batch  639/769   train_loss = 2.539
Epoch  19 Batch  649/769   train_loss = 2.481
Epoch  19 Batch  659/769   train_loss = 2.507
Epoch  19 Batch  669/769   train_loss = 2.547
Epoch  19 Batch  679/769   train_loss = 2.551
Epoch  19 Batch  689/769   train_loss = 2.554
Epoch  19 Batch  699/769   train_loss = 2.519
Epoch  19 Batch  709/769   train_loss = 2.561
Epoch  19 Batch  719/769   train_loss = 2.445
Epoch  19 Batch  729/769   train_loss = 2.533
Epoch  19 Batch  739/769   train_loss = 2.511
Epoch  19 Batch  749/769   train_loss = 2.455
Epoch  19 Batch  759/769   train_loss = 2.513
Epoch  20 Batch    0/769   train_loss = 2.471
Epoch  20 Batch   10/769   train_loss = 2.579
Epoch  20 Batch   20/769   train_loss = 2.493
Epoch  20 Batch   30/769   train_loss = 2.517
Epoch  20 Batch   40/769   train_loss = 2.486
Epoch  20 Batch   50/769   train_loss = 2.459
Epoch  20 Batch   60/769   train_loss = 2.618
Epoch  20 Batch   70/769   train_loss = 2.539
Epoch  20 Batch   80/769   train_loss = 2.461
Epoch  20 Batch   90/769   train_loss = 2.509
Epoch  20 Batch  100/769   train_loss = 2.419
Epoch  20 Batch  110/769   train_loss = 2.500
Epoch  20 Batch  120/769   train_loss = 2.429
Epoch  20 Batch  130/769   train_loss = 2.463
Epoch  20 Batch  140/769   train_loss = 2.543
Epoch  20 Batch  150/769   train_loss = 2.528
Epoch  20 Batch  160/769   train_loss = 2.464
Epoch  20 Batch  170/769   train_loss = 2.532
Epoch  20 Batch  180/769   train_loss = 2.495
Epoch  20 Batch  190/769   train_loss = 2.496
Epoch  20 Batch  200/769   train_loss = 2.556
Epoch  20 Batch  210/769   train_loss = 2.451
Epoch  20 Batch  220/769   train_loss = 2.465
Epoch  20 Batch  230/769   train_loss = 2.499
Epoch  20 Batch  240/769   train_loss = 2.467
Epoch  20 Batch  250/769   train_loss = 2.459
Epoch  20 Batch  260/769   train_loss = 2.582
Epoch  20 Batch  270/769   train_loss = 2.409
Epoch  20 Batch  280/769   train_loss = 2.469
Epoch  20 Batch  290/769   train_loss = 2.470
Epoch  20 Batch  300/769   train_loss = 2.528
Epoch  20 Batch  310/769   train_loss = 2.396
Epoch  20 Batch  320/769   train_loss = 2.485
Epoch  20 Batch  330/769   train_loss = 2.523
Epoch  20 Batch  340/769   train_loss = 2.524
Epoch  20 Batch  350/769   train_loss = 2.451
Epoch  20 Batch  360/769   train_loss = 2.465
Epoch  20 Batch  370/769   train_loss = 2.468
Epoch  20 Batch  380/769   train_loss = 2.473
Epoch  20 Batch  390/769   train_loss = 2.444
Epoch  20 Batch  400/769   train_loss = 2.496
Epoch  20 Batch  410/769   train_loss = 2.621
Epoch  20 Batch  420/769   train_loss = 2.458
Epoch  20 Batch  430/769   train_loss = 2.486
Epoch  20 Batch  440/769   train_loss = 2.516
Epoch  20 Batch  450/769   train_loss = 2.540
Epoch  20 Batch  460/769   train_loss = 2.563
Epoch  20 Batch  470/769   train_loss = 2.412
Epoch  20 Batch  480/769   train_loss = 2.453
Epoch  20 Batch  490/769   train_loss = 2.495
Epoch  20 Batch  500/769   train_loss = 2.519
Epoch  20 Batch  510/769   train_loss = 2.500
Epoch  20 Batch  520/769   train_loss = 2.357
Epoch  20 Batch  530/769   train_loss = 2.555
Epoch  20 Batch  540/769   train_loss = 2.587
Epoch  20 Batch  550/769   train_loss = 2.428
Epoch  20 Batch  560/769   train_loss = 2.538
Epoch  20 Batch  570/769   train_loss = 2.460
Epoch  20 Batch  580/769   train_loss = 2.475
Epoch  20 Batch  590/769   train_loss = 2.498
Epoch  20 Batch  600/769   train_loss = 2.469
Epoch  20 Batch  610/769   train_loss = 2.479
Epoch  20 Batch  620/769   train_loss = 2.512
Epoch  20 Batch  630/769   train_loss = 2.542
Epoch  20 Batch  640/769   train_loss = 2.487
Epoch  20 Batch  650/769   train_loss = 2.494
Epoch  20 Batch  660/769   train_loss = 2.512
Epoch  20 Batch  670/769   train_loss = 2.421
Epoch  20 Batch  680/769   train_loss = 2.491
Epoch  20 Batch  690/769   train_loss = 2.380
Epoch  20 Batch  700/769   train_loss = 2.506
Epoch  20 Batch  710/769   train_loss = 2.508
Epoch  20 Batch  720/769   train_loss = 2.413
Epoch  20 Batch  730/769   train_loss = 2.435
Epoch  20 Batch  740/769   train_loss = 2.419
Epoch  20 Batch  750/769   train_loss = 2.425
Epoch  20 Batch  760/769   train_loss = 2.436
Epoch  21 Batch    1/769   train_loss = 2.501
Epoch  21 Batch   11/769   train_loss = 2.386
Epoch  21 Batch   21/769   train_loss = 2.426
Epoch  21 Batch   31/769   train_loss = 2.422
Epoch  21 Batch   41/769   train_loss = 2.519
Epoch  21 Batch   51/769   train_loss = 2.465
Epoch  21 Batch   61/769   train_loss = 2.487
Epoch  21 Batch   71/769   train_loss = 2.472
Epoch  21 Batch   81/769   train_loss = 2.605
Epoch  21 Batch   91/769   train_loss = 2.478
Epoch  21 Batch  101/769   train_loss = 2.430
Epoch  21 Batch  111/769   train_loss = 2.406
Epoch  21 Batch  121/769   train_loss = 2.454
Epoch  21 Batch  131/769   train_loss = 2.376
Epoch  21 Batch  141/769   train_loss = 2.467
Epoch  21 Batch  151/769   train_loss = 2.459
Epoch  21 Batch  161/769   train_loss = 2.474
Epoch  21 Batch  171/769   train_loss = 2.454
Epoch  21 Batch  181/769   train_loss = 2.495
Epoch  21 Batch  191/769   train_loss = 2.409
Epoch  21 Batch  201/769   train_loss = 2.474
Epoch  21 Batch  211/769   train_loss = 2.442
Epoch  21 Batch  221/769   train_loss = 2.431
Epoch  21 Batch  231/769   train_loss = 2.362
Epoch  21 Batch  241/769   train_loss = 2.473
Epoch  21 Batch  251/769   train_loss = 2.446
Epoch  21 Batch  261/769   train_loss = 2.363
Epoch  21 Batch  271/769   train_loss = 2.399
Epoch  21 Batch  281/769   train_loss = 2.484
Epoch  21 Batch  291/769   train_loss = 2.480
Epoch  21 Batch  301/769   train_loss = 2.436
Epoch  21 Batch  311/769   train_loss = 2.424
Epoch  21 Batch  321/769   train_loss = 2.462
Epoch  21 Batch  331/769   train_loss = 2.493
Epoch  21 Batch  341/769   train_loss = 2.375
Epoch  21 Batch  351/769   train_loss = 2.507
Epoch  21 Batch  361/769   train_loss = 2.313
Epoch  21 Batch  371/769   train_loss = 2.366
Epoch  21 Batch  381/769   train_loss = 2.437
Epoch  21 Batch  391/769   train_loss = 2.358
Epoch  21 Batch  401/769   train_loss = 2.453
Epoch  21 Batch  411/769   train_loss = 2.396
Epoch  21 Batch  421/769   train_loss = 2.464
Epoch  21 Batch  431/769   train_loss = 2.465
Epoch  21 Batch  441/769   train_loss = 2.430
Epoch  21 Batch  451/769   train_loss = 2.433
Epoch  21 Batch  461/769   train_loss = 2.351
Epoch  21 Batch  471/769   train_loss = 2.461
Epoch  21 Batch  481/769   train_loss = 2.460
Epoch  21 Batch  491/769   train_loss = 2.477
Epoch  21 Batch  501/769   train_loss = 2.472
Epoch  21 Batch  511/769   train_loss = 2.391
Epoch  21 Batch  521/769   train_loss = 2.387
Epoch  21 Batch  531/769   train_loss = 2.443
Epoch  21 Batch  541/769   train_loss = 2.426
Epoch  21 Batch  551/769   train_loss = 2.500
Epoch  21 Batch  561/769   train_loss = 2.428
Epoch  21 Batch  571/769   train_loss = 2.456
Epoch  21 Batch  581/769   train_loss = 2.421
Epoch  21 Batch  591/769   train_loss = 2.462
Epoch  21 Batch  601/769   train_loss = 2.444
Epoch  21 Batch  611/769   train_loss = 2.377
Epoch  21 Batch  621/769   train_loss = 2.471
Epoch  21 Batch  631/769   train_loss = 2.462
Epoch  21 Batch  641/769   train_loss = 2.451
Epoch  21 Batch  651/769   train_loss = 2.400
Epoch  21 Batch  661/769   train_loss = 2.428
Epoch  21 Batch  671/769   train_loss = 2.457
Epoch  21 Batch  681/769   train_loss = 2.380
Epoch  21 Batch  691/769   train_loss = 2.414
Epoch  21 Batch  701/769   train_loss = 2.386
Epoch  21 Batch  711/769   train_loss = 2.424
Epoch  21 Batch  721/769   train_loss = 2.450
Epoch  21 Batch  731/769   train_loss = 2.426
Epoch  21 Batch  741/769   train_loss = 2.397
Epoch  21 Batch  751/769   train_loss = 2.376
Epoch  21 Batch  761/769   train_loss = 2.328
Epoch  22 Batch    2/769   train_loss = 2.414
Epoch  22 Batch   12/769   train_loss = 2.381
Epoch  22 Batch   22/769   train_loss = 2.429
Epoch  22 Batch   32/769   train_loss = 2.450
Epoch  22 Batch   42/769   train_loss = 2.503
Epoch  22 Batch   52/769   train_loss = 2.357
Epoch  22 Batch   62/769   train_loss = 2.451
Epoch  22 Batch   72/769   train_loss = 2.506
Epoch  22 Batch   82/769   train_loss = 2.380
Epoch  22 Batch   92/769   train_loss = 2.486
Epoch  22 Batch  102/769   train_loss = 2.413
Epoch  22 Batch  112/769   train_loss = 2.357
Epoch  22 Batch  122/769   train_loss = 2.369
Epoch  22 Batch  132/769   train_loss = 2.404
Epoch  22 Batch  142/769   train_loss = 2.397
Epoch  22 Batch  152/769   train_loss = 2.438
Epoch  22 Batch  162/769   train_loss = 2.438
Epoch  22 Batch  172/769   train_loss = 2.492
Epoch  22 Batch  182/769   train_loss = 2.374
Epoch  22 Batch  192/769   train_loss = 2.361
Epoch  22 Batch  202/769   train_loss = 2.358
Epoch  22 Batch  212/769   train_loss = 2.375
Epoch  22 Batch  222/769   train_loss = 2.431
Epoch  22 Batch  232/769   train_loss = 2.371
Epoch  22 Batch  242/769   train_loss = 2.379
Epoch  22 Batch  252/769   train_loss = 2.445
Epoch  22 Batch  262/769   train_loss = 2.357
Epoch  22 Batch  272/769   train_loss = 2.319
Epoch  22 Batch  282/769   train_loss = 2.388
Epoch  22 Batch  292/769   train_loss = 2.314
Epoch  22 Batch  302/769   train_loss = 2.389
Epoch  22 Batch  312/769   train_loss = 2.391
Epoch  22 Batch  322/769   train_loss = 2.380
Epoch  22 Batch  332/769   train_loss = 2.392
Epoch  22 Batch  342/769   train_loss = 2.327
Epoch  22 Batch  352/769   train_loss = 2.324
Epoch  22 Batch  362/769   train_loss = 2.376
Epoch  22 Batch  372/769   train_loss = 2.379
Epoch  22 Batch  382/769   train_loss = 2.314
Epoch  22 Batch  392/769   train_loss = 2.414
Epoch  22 Batch  402/769   train_loss = 2.530
Epoch  22 Batch  412/769   train_loss = 2.388
Epoch  22 Batch  422/769   train_loss = 2.314
Epoch  22 Batch  432/769   train_loss = 2.404
Epoch  22 Batch  442/769   train_loss = 2.338
Epoch  22 Batch  452/769   train_loss = 2.373
Epoch  22 Batch  462/769   train_loss = 2.394
Epoch  22 Batch  472/769   train_loss = 2.361
Epoch  22 Batch  482/769   train_loss = 2.377
Epoch  22 Batch  492/769   train_loss = 2.432
Epoch  22 Batch  502/769   train_loss = 2.377
Epoch  22 Batch  512/769   train_loss = 2.360
Epoch  22 Batch  522/769   train_loss = 2.375
Epoch  22 Batch  532/769   train_loss = 2.388
Epoch  22 Batch  542/769   train_loss = 2.384
Epoch  22 Batch  552/769   train_loss = 2.403
Epoch  22 Batch  562/769   train_loss = 2.337
Epoch  22 Batch  572/769   train_loss = 2.376
Epoch  22 Batch  582/769   train_loss = 2.374
Epoch  22 Batch  592/769   train_loss = 2.349
Epoch  22 Batch  602/769   train_loss = 2.422
Epoch  22 Batch  612/769   train_loss = 2.371
Epoch  22 Batch  622/769   train_loss = 2.330
Epoch  22 Batch  632/769   train_loss = 2.358
Epoch  22 Batch  642/769   train_loss = 2.385
Epoch  22 Batch  652/769   train_loss = 2.377
Epoch  22 Batch  662/769   train_loss = 2.356
Epoch  22 Batch  672/769   train_loss = 2.399
Epoch  22 Batch  682/769   train_loss = 2.380
Epoch  22 Batch  692/769   train_loss = 2.379
Epoch  22 Batch  702/769   train_loss = 2.475
Epoch  22 Batch  712/769   train_loss = 2.392
Epoch  22 Batch  722/769   train_loss = 2.329
Epoch  22 Batch  732/769   train_loss = 2.291
Epoch  22 Batch  742/769   train_loss = 2.360
Epoch  22 Batch  752/769   train_loss = 2.471
Epoch  22 Batch  762/769   train_loss = 2.392
Epoch  23 Batch    3/769   train_loss = 2.363
Epoch  23 Batch   13/769   train_loss = 2.413
Epoch  23 Batch   23/769   train_loss = 2.417
Epoch  23 Batch   33/769   train_loss = 2.354
Epoch  23 Batch   43/769   train_loss = 2.391
Epoch  23 Batch   53/769   train_loss = 2.371
Epoch  23 Batch   63/769   train_loss = 2.201
Epoch  23 Batch   73/769   train_loss = 2.444
Epoch  23 Batch   83/769   train_loss = 2.272
Epoch  23 Batch   93/769   train_loss = 2.364
Epoch  23 Batch  103/769   train_loss = 2.348
Epoch  23 Batch  113/769   train_loss = 2.366
Epoch  23 Batch  123/769   train_loss = 2.390
Epoch  23 Batch  133/769   train_loss = 2.336
Epoch  23 Batch  143/769   train_loss = 2.358
Epoch  23 Batch  153/769   train_loss = 2.349
Epoch  23 Batch  163/769   train_loss = 2.389
Epoch  23 Batch  173/769   train_loss = 2.373
Epoch  23 Batch  183/769   train_loss = 2.404
Epoch  23 Batch  193/769   train_loss = 2.290
Epoch  23 Batch  203/769   train_loss = 2.296
Epoch  23 Batch  213/769   train_loss = 2.334
Epoch  23 Batch  223/769   train_loss = 2.397
Epoch  23 Batch  233/769   train_loss = 2.309
Epoch  23 Batch  243/769   train_loss = 2.388
Epoch  23 Batch  253/769   train_loss = 2.374
Epoch  23 Batch  263/769   train_loss = 2.384
Epoch  23 Batch  273/769   train_loss = 2.384
Epoch  23 Batch  283/769   train_loss = 2.403
Epoch  23 Batch  293/769   train_loss = 2.330
Epoch  23 Batch  303/769   train_loss = 2.390
Epoch  23 Batch  313/769   train_loss = 2.379
Epoch  23 Batch  323/769   train_loss = 2.359
Epoch  23 Batch  333/769   train_loss = 2.369
Epoch  23 Batch  343/769   train_loss = 2.357
Epoch  23 Batch  353/769   train_loss = 2.373
Epoch  23 Batch  363/769   train_loss = 2.323
Epoch  23 Batch  373/769   train_loss = 2.415
Epoch  23 Batch  383/769   train_loss = 2.322
Epoch  23 Batch  393/769   train_loss = 2.378
Epoch  23 Batch  403/769   train_loss = 2.377
Epoch  23 Batch  413/769   train_loss = 2.373
Epoch  23 Batch  423/769   train_loss = 2.351
Epoch  23 Batch  433/769   train_loss = 2.317
Epoch  23 Batch  443/769   train_loss = 2.348
Epoch  23 Batch  453/769   train_loss = 2.353
Epoch  23 Batch  463/769   train_loss = 2.375
Epoch  23 Batch  473/769   train_loss = 2.376
Epoch  23 Batch  483/769   train_loss = 2.346
Epoch  23 Batch  493/769   train_loss = 2.337
Epoch  23 Batch  503/769   train_loss = 2.404
Epoch  23 Batch  513/769   train_loss = 2.409
Epoch  23 Batch  523/769   train_loss = 2.313
Epoch  23 Batch  533/769   train_loss = 2.346
Epoch  23 Batch  543/769   train_loss = 2.287
Epoch  23 Batch  553/769   train_loss = 2.328
Epoch  23 Batch  563/769   train_loss = 2.308
Epoch  23 Batch  573/769   train_loss = 2.317
Epoch  23 Batch  583/769   train_loss = 2.349
Epoch  23 Batch  593/769   train_loss = 2.390
Epoch  23 Batch  603/769   train_loss = 2.294
Epoch  23 Batch  613/769   train_loss = 2.375
Epoch  23 Batch  623/769   train_loss = 2.269
Epoch  23 Batch  633/769   train_loss = 2.254
Epoch  23 Batch  643/769   train_loss = 2.266
Epoch  23 Batch  653/769   train_loss = 2.293
Epoch  23 Batch  663/769   train_loss = 2.308
Epoch  23 Batch  673/769   train_loss = 2.355
Epoch  23 Batch  683/769   train_loss = 2.313
Epoch  23 Batch  693/769   train_loss = 2.349
Epoch  23 Batch  703/769   train_loss = 2.285
Epoch  23 Batch  713/769   train_loss = 2.317
Epoch  23 Batch  723/769   train_loss = 2.258
Epoch  23 Batch  733/769   train_loss = 2.325
Epoch  23 Batch  743/769   train_loss = 2.352
Epoch  23 Batch  753/769   train_loss = 2.329
Epoch  23 Batch  763/769   train_loss = 2.347
Epoch  24 Batch    4/769   train_loss = 2.324
Epoch  24 Batch   14/769   train_loss = 2.300
Epoch  24 Batch   24/769   train_loss = 2.326
Epoch  24 Batch   34/769   train_loss = 2.299
Epoch  24 Batch   44/769   train_loss = 2.347
Epoch  24 Batch   54/769   train_loss = 2.230
Epoch  24 Batch   64/769   train_loss = 2.236
Epoch  24 Batch   74/769   train_loss = 2.344
Epoch  24 Batch   84/769   train_loss = 2.265
Epoch  24 Batch   94/769   train_loss = 2.285
Epoch  24 Batch  104/769   train_loss = 2.378
Epoch  24 Batch  114/769   train_loss = 2.346
Epoch  24 Batch  124/769   train_loss = 2.291
Epoch  24 Batch  134/769   train_loss = 2.268
Epoch  24 Batch  144/769   train_loss = 2.348
Epoch  24 Batch  154/769   train_loss = 2.322
Epoch  24 Batch  164/769   train_loss = 2.295
Epoch  24 Batch  174/769   train_loss = 2.287
Epoch  24 Batch  184/769   train_loss = 2.290
Epoch  24 Batch  194/769   train_loss = 2.171
Epoch  24 Batch  204/769   train_loss = 2.216
Epoch  24 Batch  214/769   train_loss = 2.287
Epoch  24 Batch  224/769   train_loss = 2.314
Epoch  24 Batch  234/769   train_loss = 2.332
Epoch  24 Batch  244/769   train_loss = 2.367
Epoch  24 Batch  254/769   train_loss = 2.246
Epoch  24 Batch  264/769   train_loss = 2.295
Epoch  24 Batch  274/769   train_loss = 2.216
Epoch  24 Batch  284/769   train_loss = 2.232
Epoch  24 Batch  294/769   train_loss = 2.278
Epoch  24 Batch  304/769   train_loss = 2.333
Epoch  24 Batch  314/769   train_loss = 2.368
Epoch  24 Batch  324/769   train_loss = 2.314
Epoch  24 Batch  334/769   train_loss = 2.282
Epoch  24 Batch  344/769   train_loss = 2.328
Epoch  24 Batch  354/769   train_loss = 2.501
Epoch  24 Batch  364/769   train_loss = 2.255
Epoch  24 Batch  374/769   train_loss = 2.322
Epoch  24 Batch  384/769   train_loss = 2.182
Epoch  24 Batch  394/769   train_loss = 2.341
Epoch  24 Batch  404/769   train_loss = 2.413
Epoch  24 Batch  414/769   train_loss = 2.258
Epoch  24 Batch  424/769   train_loss = 2.259
Epoch  24 Batch  434/769   train_loss = 2.277
Epoch  24 Batch  444/769   train_loss = 2.355
Epoch  24 Batch  454/769   train_loss = 2.297
Epoch  24 Batch  464/769   train_loss = 2.398
Epoch  24 Batch  474/769   train_loss = 2.309
Epoch  24 Batch  484/769   train_loss = 2.278
Epoch  24 Batch  494/769   train_loss = 2.238
Epoch  24 Batch  504/769   train_loss = 2.370
Epoch  24 Batch  514/769   train_loss = 2.288
Epoch  24 Batch  524/769   train_loss = 2.352
Epoch  24 Batch  534/769   train_loss = 2.281
Epoch  24 Batch  544/769   train_loss = 2.300
Epoch  24 Batch  554/769   train_loss = 2.331
Epoch  24 Batch  564/769   train_loss = 2.303
Epoch  24 Batch  574/769   train_loss = 2.278
Epoch  24 Batch  584/769   train_loss = 2.219
Epoch  24 Batch  594/769   train_loss = 2.280
Epoch  24 Batch  604/769   train_loss = 2.267
Epoch  24 Batch  614/769   train_loss = 2.303
Epoch  24 Batch  624/769   train_loss = 2.363
Epoch  24 Batch  634/769   train_loss = 2.256
Epoch  24 Batch  644/769   train_loss = 2.188
Epoch  24 Batch  654/769   train_loss = 2.316
Epoch  24 Batch  664/769   train_loss = 2.330
Epoch  24 Batch  674/769   train_loss = 2.303
Epoch  24 Batch  684/769   train_loss = 2.299
Epoch  24 Batch  694/769   train_loss = 2.336
Epoch  24 Batch  704/769   train_loss = 2.290
Epoch  24 Batch  714/769   train_loss = 2.221
Epoch  24 Batch  724/769   train_loss = 2.277
Epoch  24 Batch  734/769   train_loss = 2.377
Epoch  24 Batch  744/769   train_loss = 2.241
Epoch  24 Batch  754/769   train_loss = 2.362
Epoch  24 Batch  764/769   train_loss = 2.262
Epoch  25 Batch    5/769   train_loss = 2.341
Epoch  25 Batch   15/769   train_loss = 2.280
Epoch  25 Batch   25/769   train_loss = 2.333
Epoch  25 Batch   35/769   train_loss = 2.312
Epoch  25 Batch   45/769   train_loss = 2.287
Epoch  25 Batch   55/769   train_loss = 2.266
Epoch  25 Batch   65/769   train_loss = 2.256
Epoch  25 Batch   75/769   train_loss = 2.342
Epoch  25 Batch   85/769   train_loss = 2.374
Epoch  25 Batch   95/769   train_loss = 2.224
Epoch  25 Batch  105/769   train_loss = 2.312
Epoch  25 Batch  115/769   train_loss = 2.256
Epoch  25 Batch  125/769   train_loss = 2.172
Epoch  25 Batch  135/769   train_loss = 2.270
Epoch  25 Batch  145/769   train_loss = 2.292
Epoch  25 Batch  155/769   train_loss = 2.291
Epoch  25 Batch  165/769   train_loss = 2.282
Epoch  25 Batch  175/769   train_loss = 2.197
Epoch  25 Batch  185/769   train_loss = 2.218
Epoch  25 Batch  195/769   train_loss = 2.219
Epoch  25 Batch  205/769   train_loss = 2.308
Epoch  25 Batch  215/769   train_loss = 2.291
Epoch  25 Batch  225/769   train_loss = 2.224
Epoch  25 Batch  235/769   train_loss = 2.221
Epoch  25 Batch  245/769   train_loss = 2.297
Epoch  25 Batch  255/769   train_loss = 2.274
Epoch  25 Batch  265/769   train_loss = 2.168
Epoch  25 Batch  275/769   train_loss = 2.216
Epoch  25 Batch  285/769   train_loss = 2.305
Epoch  25 Batch  295/769   train_loss = 2.241
Epoch  25 Batch  305/769   train_loss = 2.290
Epoch  25 Batch  315/769   train_loss = 2.207
Epoch  25 Batch  325/769   train_loss = 2.246
Epoch  25 Batch  335/769   train_loss = 2.310
Epoch  25 Batch  345/769   train_loss = 2.201
Epoch  25 Batch  355/769   train_loss = 2.239
Epoch  25 Batch  365/769   train_loss = 2.249
Epoch  25 Batch  375/769   train_loss = 2.310
Epoch  25 Batch  385/769   train_loss = 2.124
Epoch  25 Batch  395/769   train_loss = 2.300
Epoch  25 Batch  405/769   train_loss = 2.259
Epoch  25 Batch  415/769   train_loss = 2.117
Epoch  25 Batch  425/769   train_loss = 2.231
Epoch  25 Batch  435/769   train_loss = 2.172
Epoch  25 Batch  445/769   train_loss = 2.272
Epoch  25 Batch  455/769   train_loss = 2.327
Epoch  25 Batch  465/769   train_loss = 2.328
Epoch  25 Batch  475/769   train_loss = 2.174
Epoch  25 Batch  485/769   train_loss = 2.312
Epoch  25 Batch  495/769   train_loss = 2.231
Epoch  25 Batch  505/769   train_loss = 2.235
Epoch  25 Batch  515/769   train_loss = 2.275
Epoch  25 Batch  525/769   train_loss = 2.237
Epoch  25 Batch  535/769   train_loss = 2.227
Epoch  25 Batch  545/769   train_loss = 2.265
Epoch  25 Batch  555/769   train_loss = 2.275
Epoch  25 Batch  565/769   train_loss = 2.340
Epoch  25 Batch  575/769   train_loss = 2.308
Epoch  25 Batch  585/769   train_loss = 2.265
Epoch  25 Batch  595/769   train_loss = 2.258
Epoch  25 Batch  605/769   train_loss = 2.202
Epoch  25 Batch  615/769   train_loss = 2.338
Epoch  25 Batch  625/769   train_loss = 2.277
Epoch  25 Batch  635/769   train_loss = 2.209
Epoch  25 Batch  645/769   train_loss = 2.202
Epoch  25 Batch  655/769   train_loss = 2.192
Epoch  25 Batch  665/769   train_loss = 2.231
Epoch  25 Batch  675/769   train_loss = 2.266
Epoch  25 Batch  685/769   train_loss = 2.251
Epoch  25 Batch  695/769   train_loss = 2.323
Epoch  25 Batch  705/769   train_loss = 2.275
Epoch  25 Batch  715/769   train_loss = 2.280
Epoch  25 Batch  725/769   train_loss = 2.220
Epoch  25 Batch  735/769   train_loss = 2.275
Epoch  25 Batch  745/769   train_loss = 2.322
Epoch  25 Batch  755/769   train_loss = 2.135
Epoch  25 Batch  765/769   train_loss = 2.285
Epoch  26 Batch    6/769   train_loss = 2.193
Epoch  26 Batch   16/769   train_loss = 2.201
Epoch  26 Batch   26/769   train_loss = 2.214
Epoch  26 Batch   36/769   train_loss = 2.309
Epoch  26 Batch   46/769   train_loss = 2.300
Epoch  26 Batch   56/769   train_loss = 2.194
Epoch  26 Batch   66/769   train_loss = 2.248
Epoch  26 Batch   76/769   train_loss = 2.267
Epoch  26 Batch   86/769   train_loss = 2.312
Epoch  26 Batch   96/769   train_loss = 2.135
Epoch  26 Batch  106/769   train_loss = 2.306
Epoch  26 Batch  116/769   train_loss = 2.190
Epoch  26 Batch  126/769   train_loss = 2.185
Epoch  26 Batch  136/769   train_loss = 2.221
Epoch  26 Batch  146/769   train_loss = 2.232
Epoch  26 Batch  156/769   train_loss = 2.242
Epoch  26 Batch  166/769   train_loss = 2.203
Epoch  26 Batch  176/769   train_loss = 2.175
Epoch  26 Batch  186/769   train_loss = 2.147
Epoch  26 Batch  196/769   train_loss = 2.215
Epoch  26 Batch  206/769   train_loss = 2.229
Epoch  26 Batch  216/769   train_loss = 2.202
Epoch  26 Batch  226/769   train_loss = 2.217
Epoch  26 Batch  236/769   train_loss = 2.247
Epoch  26 Batch  246/769   train_loss = 2.228
Epoch  26 Batch  256/769   train_loss = 2.177
Epoch  26 Batch  266/769   train_loss = 2.202
Epoch  26 Batch  276/769   train_loss = 2.167
Epoch  26 Batch  286/769   train_loss = 2.163
Epoch  26 Batch  296/769   train_loss = 2.182
Epoch  26 Batch  306/769   train_loss = 2.157
Epoch  26 Batch  316/769   train_loss = 2.198
Epoch  26 Batch  326/769   train_loss = 2.280
Epoch  26 Batch  336/769   train_loss = 2.243
Epoch  26 Batch  346/769   train_loss = 2.210
Epoch  26 Batch  356/769   train_loss = 2.191
Epoch  26 Batch  366/769   train_loss = 2.237
Epoch  26 Batch  376/769   train_loss = 2.166
Epoch  26 Batch  386/769   train_loss = 2.273
Epoch  26 Batch  396/769   train_loss = 2.233
Epoch  26 Batch  406/769   train_loss = 2.187
Epoch  26 Batch  416/769   train_loss = 2.242
Epoch  26 Batch  426/769   train_loss = 2.286
Epoch  26 Batch  436/769   train_loss = 2.183
Epoch  26 Batch  446/769   train_loss = 2.227
Epoch  26 Batch  456/769   train_loss = 2.250
Epoch  26 Batch  466/769   train_loss = 2.237
Epoch  26 Batch  476/769   train_loss = 2.200
Epoch  26 Batch  486/769   train_loss = 2.260
Epoch  26 Batch  496/769   train_loss = 2.190
Epoch  26 Batch  506/769   train_loss = 2.258
Epoch  26 Batch  516/769   train_loss = 2.209
Epoch  26 Batch  526/769   train_loss = 2.130
Epoch  26 Batch  536/769   train_loss = 2.227
Epoch  26 Batch  546/769   train_loss = 2.234
Epoch  26 Batch  556/769   train_loss = 2.231
Epoch  26 Batch  566/769   train_loss = 2.277
Epoch  26 Batch  576/769   train_loss = 2.169
Epoch  26 Batch  586/769   train_loss = 2.159
Epoch  26 Batch  596/769   train_loss = 2.309
Epoch  26 Batch  606/769   train_loss = 2.280
Epoch  26 Batch  616/769   train_loss = 2.187
Epoch  26 Batch  626/769   train_loss = 2.247
Epoch  26 Batch  636/769   train_loss = 2.161
Epoch  26 Batch  646/769   train_loss = 2.311
Epoch  26 Batch  656/769   train_loss = 2.179
Epoch  26 Batch  666/769   train_loss = 2.211
Epoch  26 Batch  676/769   train_loss = 2.263
Epoch  26 Batch  686/769   train_loss = 2.233
Epoch  26 Batch  696/769   train_loss = 2.220
Epoch  26 Batch  706/769   train_loss = 2.272
Epoch  26 Batch  716/769   train_loss = 2.261
Epoch  26 Batch  726/769   train_loss = 2.227
Epoch  26 Batch  736/769   train_loss = 2.266
Epoch  26 Batch  746/769   train_loss = 2.195
Epoch  26 Batch  756/769   train_loss = 2.157
Epoch  26 Batch  766/769   train_loss = 2.212
Epoch  27 Batch    7/769   train_loss = 2.237
Epoch  27 Batch   17/769   train_loss = 2.223
Epoch  27 Batch   27/769   train_loss = 2.221
Epoch  27 Batch   37/769   train_loss = 2.130
Epoch  27 Batch   47/769   train_loss = 2.126
Epoch  27 Batch   57/769   train_loss = 2.211
Epoch  27 Batch   67/769   train_loss = 2.195
Epoch  27 Batch   77/769   train_loss = 2.248
Epoch  27 Batch   87/769   train_loss = 2.176
Epoch  27 Batch   97/769   train_loss = 2.115
Epoch  27 Batch  107/769   train_loss = 2.192
Epoch  27 Batch  117/769   train_loss = 2.158
Epoch  27 Batch  127/769   train_loss = 2.219
Epoch  27 Batch  137/769   train_loss = 2.165
Epoch  27 Batch  147/769   train_loss = 2.204
Epoch  27 Batch  157/769   train_loss = 2.269
Epoch  27 Batch  167/769   train_loss = 2.159
Epoch  27 Batch  177/769   train_loss = 2.090
Epoch  27 Batch  187/769   train_loss = 2.236
Epoch  27 Batch  197/769   train_loss = 2.124
Epoch  27 Batch  207/769   train_loss = 2.138
Epoch  27 Batch  217/769   train_loss = 2.208
Epoch  27 Batch  227/769   train_loss = 2.164
Epoch  27 Batch  237/769   train_loss = 2.134
Epoch  27 Batch  247/769   train_loss = 2.187
Epoch  27 Batch  257/769   train_loss = 2.131
Epoch  27 Batch  267/769   train_loss = 2.118
Epoch  27 Batch  277/769   train_loss = 2.185
Epoch  27 Batch  287/769   train_loss = 2.115
Epoch  27 Batch  297/769   train_loss = 2.130
Epoch  27 Batch  307/769   train_loss = 2.243
Epoch  27 Batch  317/769   train_loss = 2.062
Epoch  27 Batch  327/769   train_loss = 2.207
Epoch  27 Batch  337/769   train_loss = 2.205
Epoch  27 Batch  347/769   train_loss = 2.176
Epoch  27 Batch  357/769   train_loss = 2.220
Epoch  27 Batch  367/769   train_loss = 2.130
Epoch  27 Batch  377/769   train_loss = 2.200
Epoch  27 Batch  387/769   train_loss = 2.248
Epoch  27 Batch  397/769   train_loss = 2.133
Epoch  27 Batch  407/769   train_loss = 2.196
Epoch  27 Batch  417/769   train_loss = 2.231
Epoch  27 Batch  427/769   train_loss = 2.186
Epoch  27 Batch  437/769   train_loss = 2.209
Epoch  27 Batch  447/769   train_loss = 2.222
Epoch  27 Batch  457/769   train_loss = 2.114
Epoch  27 Batch  467/769   train_loss = 2.147
Epoch  27 Batch  477/769   train_loss = 2.215
Epoch  27 Batch  487/769   train_loss = 2.157
Epoch  27 Batch  497/769   train_loss = 2.211
Epoch  27 Batch  507/769   train_loss = 2.179
Epoch  27 Batch  517/769   train_loss = 2.161
Epoch  27 Batch  527/769   train_loss = 2.137
Epoch  27 Batch  537/769   train_loss = 2.237
Epoch  27 Batch  547/769   train_loss = 2.237
Epoch  27 Batch  557/769   train_loss = 2.170
Epoch  27 Batch  567/769   train_loss = 2.194
Epoch  27 Batch  577/769   train_loss = 2.253
Epoch  27 Batch  587/769   train_loss = 2.159
Epoch  27 Batch  597/769   train_loss = 2.116
Epoch  27 Batch  607/769   train_loss = 2.219
Epoch  27 Batch  617/769   train_loss = 2.214
Epoch  27 Batch  627/769   train_loss = 2.297
Epoch  27 Batch  637/769   train_loss = 2.156
Epoch  27 Batch  647/769   train_loss = 2.234
Epoch  27 Batch  657/769   train_loss = 2.115
Epoch  27 Batch  667/769   train_loss = 2.239
Epoch  27 Batch  677/769   train_loss = 2.156
Epoch  27 Batch  687/769   train_loss = 2.238
Epoch  27 Batch  697/769   train_loss = 2.239
Epoch  27 Batch  707/769   train_loss = 2.168
Epoch  27 Batch  717/769   train_loss = 2.159
Epoch  27 Batch  727/769   train_loss = 2.176
Epoch  27 Batch  737/769   train_loss = 2.243
Epoch  27 Batch  747/769   train_loss = 2.136
Epoch  27 Batch  757/769   train_loss = 2.211
Epoch  27 Batch  767/769   train_loss = 2.207
Epoch  28 Batch    8/769   train_loss = 2.137
Epoch  28 Batch   18/769   train_loss = 2.176
Epoch  28 Batch   28/769   train_loss = 2.208
Epoch  28 Batch   38/769   train_loss = 2.242
Epoch  28 Batch   48/769   train_loss = 2.142
Epoch  28 Batch   58/769   train_loss = 2.109
Epoch  28 Batch   68/769   train_loss = 2.196
Epoch  28 Batch   78/769   train_loss = 2.198
Epoch  28 Batch   88/769   train_loss = 2.133
Epoch  28 Batch   98/769   train_loss = 2.074
Epoch  28 Batch  108/769   train_loss = 2.189
Epoch  28 Batch  118/769   train_loss = 2.120
Epoch  28 Batch  128/769   train_loss = 2.182
Epoch  28 Batch  138/769   train_loss = 2.114
Epoch  28 Batch  148/769   train_loss = 2.216
Epoch  28 Batch  158/769   train_loss = 2.187
Epoch  28 Batch  168/769   train_loss = 2.126
Epoch  28 Batch  178/769   train_loss = 2.172
Epoch  28 Batch  188/769   train_loss = 2.137
Epoch  28 Batch  198/769   train_loss = 2.132
Epoch  28 Batch  208/769   train_loss = 2.148
Epoch  28 Batch  218/769   train_loss = 2.192
Epoch  28 Batch  228/769   train_loss = 2.141
Epoch  28 Batch  238/769   train_loss = 2.159
Epoch  28 Batch  248/769   train_loss = 2.167
Epoch  28 Batch  258/769   train_loss = 2.084
Epoch  28 Batch  268/769   train_loss = 2.134
Epoch  28 Batch  278/769   train_loss = 2.200
Epoch  28 Batch  288/769   train_loss = 2.200
Epoch  28 Batch  298/769   train_loss = 2.197
Epoch  28 Batch  308/769   train_loss = 2.159
Epoch  28 Batch  318/769   train_loss = 2.111
Epoch  28 Batch  328/769   train_loss = 2.122
Epoch  28 Batch  338/769   train_loss = 2.176
Epoch  28 Batch  348/769   train_loss = 2.171
Epoch  28 Batch  358/769   train_loss = 2.179
Epoch  28 Batch  368/769   train_loss = 2.169
Epoch  28 Batch  378/769   train_loss = 2.165
Epoch  28 Batch  388/769   train_loss = 2.082
Epoch  28 Batch  398/769   train_loss = 2.195
Epoch  28 Batch  408/769   train_loss = 2.145
Epoch  28 Batch  418/769   train_loss = 2.183
Epoch  28 Batch  428/769   train_loss = 2.154
Epoch  28 Batch  438/769   train_loss = 2.072
Epoch  28 Batch  448/769   train_loss = 2.168
Epoch  28 Batch  458/769   train_loss = 2.220
Epoch  28 Batch  468/769   train_loss = 2.145
Epoch  28 Batch  478/769   train_loss = 2.056
Epoch  28 Batch  488/769   train_loss = 2.119
Epoch  28 Batch  498/769   train_loss = 2.127
Epoch  28 Batch  508/769   train_loss = 2.146
Epoch  28 Batch  518/769   train_loss = 2.171
Epoch  28 Batch  528/769   train_loss = 2.085
Epoch  28 Batch  538/769   train_loss = 2.164
Epoch  28 Batch  548/769   train_loss = 2.125
Epoch  28 Batch  558/769   train_loss = 2.137
Epoch  28 Batch  568/769   train_loss = 2.074
Epoch  28 Batch  578/769   train_loss = 1.986
Epoch  28 Batch  588/769   train_loss = 2.070
Epoch  28 Batch  598/769   train_loss = 2.125
Epoch  28 Batch  608/769   train_loss = 2.100
Epoch  28 Batch  618/769   train_loss = 2.101
Epoch  28 Batch  628/769   train_loss = 2.137
Epoch  28 Batch  638/769   train_loss = 2.079
Epoch  28 Batch  648/769   train_loss = 2.247
Epoch  28 Batch  658/769   train_loss = 2.246
Epoch  28 Batch  668/769   train_loss = 2.098
Epoch  28 Batch  678/769   train_loss = 2.121
Epoch  28 Batch  688/769   train_loss = 2.113
Epoch  28 Batch  698/769   train_loss = 2.173
Epoch  28 Batch  708/769   train_loss = 2.165
Epoch  28 Batch  718/769   train_loss = 2.075
Epoch  28 Batch  728/769   train_loss = 2.163
Epoch  28 Batch  738/769   train_loss = 2.105
Epoch  28 Batch  748/769   train_loss = 2.169
Epoch  28 Batch  758/769   train_loss = 2.171
Epoch  28 Batch  768/769   train_loss = 2.174
Epoch  29 Batch    9/769   train_loss = 2.180
Epoch  29 Batch   19/769   train_loss = 2.173
Epoch  29 Batch   29/769   train_loss = 2.178
Epoch  29 Batch   39/769   train_loss = 2.153
Epoch  29 Batch   49/769   train_loss = 2.096
Epoch  29 Batch   59/769   train_loss = 2.053
Epoch  29 Batch   69/769   train_loss = 2.137
Epoch  29 Batch   79/769   train_loss = 2.106
Epoch  29 Batch   89/769   train_loss = 2.017
Epoch  29 Batch   99/769   train_loss = 2.129
Epoch  29 Batch  109/769   train_loss = 2.105
Epoch  29 Batch  119/769   train_loss = 2.058
Epoch  29 Batch  129/769   train_loss = 2.034
Epoch  29 Batch  139/769   train_loss = 2.125
Epoch  29 Batch  149/769   train_loss = 2.088
Epoch  29 Batch  159/769   train_loss = 2.033
Epoch  29 Batch  169/769   train_loss = 2.135
Epoch  29 Batch  179/769   train_loss = 2.087
Epoch  29 Batch  189/769   train_loss = 2.114
Epoch  29 Batch  199/769   train_loss = 2.081
Epoch  29 Batch  209/769   train_loss = 2.111
Epoch  29 Batch  219/769   train_loss = 2.114
Epoch  29 Batch  229/769   train_loss = 2.145
Epoch  29 Batch  239/769   train_loss = 2.046
Epoch  29 Batch  249/769   train_loss = 2.114
Epoch  29 Batch  259/769   train_loss = 2.100
Epoch  29 Batch  269/769   train_loss = 2.096
Epoch  29 Batch  279/769   train_loss = 2.155
Epoch  29 Batch  289/769   train_loss = 2.048
Epoch  29 Batch  299/769   train_loss = 2.200
Epoch  29 Batch  309/769   train_loss = 2.130
Epoch  29 Batch  319/769   train_loss = 2.161
Epoch  29 Batch  329/769   train_loss = 2.124
Epoch  29 Batch  339/769   train_loss = 2.073
Epoch  29 Batch  349/769   train_loss = 2.110
Epoch  29 Batch  359/769   train_loss = 2.187
Epoch  29 Batch  369/769   train_loss = 2.077
Epoch  29 Batch  379/769   train_loss = 2.042
Epoch  29 Batch  389/769   train_loss = 2.009
Epoch  29 Batch  399/769   train_loss = 2.141
Epoch  29 Batch  409/769   train_loss = 2.184
Epoch  29 Batch  419/769   train_loss = 2.208
Epoch  29 Batch  429/769   train_loss = 2.074
Epoch  29 Batch  439/769   train_loss = 2.164
Epoch  29 Batch  449/769   train_loss = 2.196
Epoch  29 Batch  459/769   train_loss = 2.082
Epoch  29 Batch  469/769   train_loss = 2.060
Epoch  29 Batch  479/769   train_loss = 2.119
Epoch  29 Batch  489/769   train_loss = 2.145
Epoch  29 Batch  499/769   train_loss = 2.130
Epoch  29 Batch  509/769   train_loss = 2.065
Epoch  29 Batch  519/769   train_loss = 2.146
Epoch  29 Batch  529/769   train_loss = 2.170
Epoch  29 Batch  539/769   train_loss = 2.139
Epoch  29 Batch  549/769   train_loss = 2.167
Epoch  29 Batch  559/769   train_loss = 2.158
Epoch  29 Batch  569/769   train_loss = 2.011
Epoch  29 Batch  579/769   train_loss = 2.017
Epoch  29 Batch  589/769   train_loss = 2.131
Epoch  29 Batch  599/769   train_loss = 2.085
Epoch  29 Batch  609/769   train_loss = 2.122
Epoch  29 Batch  619/769   train_loss = 2.146
Epoch  29 Batch  629/769   train_loss = 2.112
Epoch  29 Batch  639/769   train_loss = 2.104
Epoch  29 Batch  649/769   train_loss = 2.041
Epoch  29 Batch  659/769   train_loss = 2.097
Epoch  29 Batch  669/769   train_loss = 2.130
Epoch  29 Batch  679/769   train_loss = 2.081
Epoch  29 Batch  689/769   train_loss = 2.161
Epoch  29 Batch  699/769   train_loss = 2.123
Epoch  29 Batch  709/769   train_loss = 2.146
Epoch  29 Batch  719/769   train_loss = 2.016
Epoch  29 Batch  729/769   train_loss = 2.097
Epoch  29 Batch  739/769   train_loss = 2.071
Epoch  29 Batch  749/769   train_loss = 2.013
Epoch  29 Batch  759/769   train_loss = 2.113
Epoch  30 Batch    0/769   train_loss = 2.030
Epoch  30 Batch   10/769   train_loss = 2.158
Epoch  30 Batch   20/769   train_loss = 2.083
Epoch  30 Batch   30/769   train_loss = 2.078
Epoch  30 Batch   40/769   train_loss = 2.042
Epoch  30 Batch   50/769   train_loss = 2.025
Epoch  30 Batch   60/769   train_loss = 2.160
Epoch  30 Batch   70/769   train_loss = 2.089
Epoch  30 Batch   80/769   train_loss = 2.049
Epoch  30 Batch   90/769   train_loss = 2.115
Epoch  30 Batch  100/769   train_loss = 2.006
Epoch  30 Batch  110/769   train_loss = 2.068
Epoch  30 Batch  120/769   train_loss = 1.955
Epoch  30 Batch  130/769   train_loss = 2.012
Epoch  30 Batch  140/769   train_loss = 2.141
Epoch  30 Batch  150/769   train_loss = 2.073
Epoch  30 Batch  160/769   train_loss = 2.062
Epoch  30 Batch  170/769   train_loss = 2.050
Epoch  30 Batch  180/769   train_loss = 2.056
Epoch  30 Batch  190/769   train_loss = 2.055
Epoch  30 Batch  200/769   train_loss = 2.148
Epoch  30 Batch  210/769   train_loss = 2.018
Epoch  30 Batch  220/769   train_loss = 1.978
Epoch  30 Batch  230/769   train_loss = 2.040
Epoch  30 Batch  240/769   train_loss = 2.029
Epoch  30 Batch  250/769   train_loss = 1.999
Epoch  30 Batch  260/769   train_loss = 2.095
Epoch  30 Batch  270/769   train_loss = 2.008
Epoch  30 Batch  280/769   train_loss = 2.038
Epoch  30 Batch  290/769   train_loss = 2.042
Epoch  30 Batch  300/769   train_loss = 2.131
Epoch  30 Batch  310/769   train_loss = 2.000
Epoch  30 Batch  320/769   train_loss = 2.030
Epoch  30 Batch  330/769   train_loss = 2.092
Epoch  30 Batch  340/769   train_loss = 2.071
Epoch  30 Batch  350/769   train_loss = 2.015
Epoch  30 Batch  360/769   train_loss = 2.030
Epoch  30 Batch  370/769   train_loss = 2.048
Epoch  30 Batch  380/769   train_loss = 2.060
Epoch  30 Batch  390/769   train_loss = 2.029
Epoch  30 Batch  400/769   train_loss = 2.040
Epoch  30 Batch  410/769   train_loss = 2.159
Epoch  30 Batch  420/769   train_loss = 2.053
Epoch  30 Batch  430/769   train_loss = 2.058
Epoch  30 Batch  440/769   train_loss = 2.118
Epoch  30 Batch  450/769   train_loss = 2.108
Epoch  30 Batch  460/769   train_loss = 2.178
Epoch  30 Batch  470/769   train_loss = 2.036
Epoch  30 Batch  480/769   train_loss = 2.041
Epoch  30 Batch  490/769   train_loss = 2.041
Epoch  30 Batch  500/769   train_loss = 2.093
Epoch  30 Batch  510/769   train_loss = 2.089
Epoch  30 Batch  520/769   train_loss = 1.951
Epoch  30 Batch  530/769   train_loss = 2.079
Epoch  30 Batch  540/769   train_loss = 2.164
Epoch  30 Batch  550/769   train_loss = 2.030
Epoch  30 Batch  560/769   train_loss = 2.120
Epoch  30 Batch  570/769   train_loss = 2.036
Epoch  30 Batch  580/769   train_loss = 2.038
Epoch  30 Batch  590/769   train_loss = 2.054
Epoch  30 Batch  600/769   train_loss = 2.033
Epoch  30 Batch  610/769   train_loss = 2.046
Epoch  30 Batch  620/769   train_loss = 2.090
Epoch  30 Batch  630/769   train_loss = 2.150
Epoch  30 Batch  640/769   train_loss = 2.063
Epoch  30 Batch  650/769   train_loss = 2.081
Epoch  30 Batch  660/769   train_loss = 2.106
Epoch  30 Batch  670/769   train_loss = 2.042
Epoch  30 Batch  680/769   train_loss = 2.110
Epoch  30 Batch  690/769   train_loss = 1.959
Epoch  30 Batch  700/769   train_loss = 2.125
Epoch  30 Batch  710/769   train_loss = 2.086
Epoch  30 Batch  720/769   train_loss = 2.026
Epoch  30 Batch  730/769   train_loss = 2.033
Epoch  30 Batch  740/769   train_loss = 2.022
Epoch  30 Batch  750/769   train_loss = 2.001
Epoch  30 Batch  760/769   train_loss = 2.073
Epoch  31 Batch    1/769   train_loss = 2.120
Epoch  31 Batch   11/769   train_loss = 2.020
Epoch  31 Batch   21/769   train_loss = 2.065
Epoch  31 Batch   31/769   train_loss = 2.047
Epoch  31 Batch   41/769   train_loss = 2.121
Epoch  31 Batch   51/769   train_loss = 2.075
Epoch  31 Batch   61/769   train_loss = 2.088
Epoch  31 Batch   71/769   train_loss = 2.025
Epoch  31 Batch   81/769   train_loss = 2.193
Epoch  31 Batch   91/769   train_loss = 2.054
Epoch  31 Batch  101/769   train_loss = 2.017
Epoch  31 Batch  111/769   train_loss = 1.991
Epoch  31 Batch  121/769   train_loss = 2.065
Epoch  31 Batch  131/769   train_loss = 1.988
Epoch  31 Batch  141/769   train_loss = 2.066
Epoch  31 Batch  151/769   train_loss = 2.036
Epoch  31 Batch  161/769   train_loss = 2.107
Epoch  31 Batch  171/769   train_loss = 2.038
Epoch  31 Batch  181/769   train_loss = 2.110
Epoch  31 Batch  191/769   train_loss = 2.009
Epoch  31 Batch  201/769   train_loss = 2.074
Epoch  31 Batch  211/769   train_loss = 2.019
Epoch  31 Batch  221/769   train_loss = 1.976
Epoch  31 Batch  231/769   train_loss = 1.958
Epoch  31 Batch  241/769   train_loss = 2.057
Epoch  31 Batch  251/769   train_loss = 2.032
Epoch  31 Batch  261/769   train_loss = 1.956
Epoch  31 Batch  271/769   train_loss = 1.991
Epoch  31 Batch  281/769   train_loss = 2.077
Epoch  31 Batch  291/769   train_loss = 2.074
Epoch  31 Batch  301/769   train_loss = 2.031
Epoch  31 Batch  311/769   train_loss = 2.036
Epoch  31 Batch  321/769   train_loss = 2.061
Epoch  31 Batch  331/769   train_loss = 2.075
Epoch  31 Batch  341/769   train_loss = 1.981
Epoch  31 Batch  351/769   train_loss = 2.092
Epoch  31 Batch  361/769   train_loss = 1.960
Epoch  31 Batch  371/769   train_loss = 1.994
Epoch  31 Batch  381/769   train_loss = 2.031
Epoch  31 Batch  391/769   train_loss = 1.965
Epoch  31 Batch  401/769   train_loss = 2.038
Epoch  31 Batch  411/769   train_loss = 1.942
Epoch  31 Batch  421/769   train_loss = 2.066
Epoch  31 Batch  431/769   train_loss = 2.060
Epoch  31 Batch  441/769   train_loss = 2.035
Epoch  31 Batch  451/769   train_loss = 2.026
Epoch  31 Batch  461/769   train_loss = 1.990
Epoch  31 Batch  471/769   train_loss = 2.037
Epoch  31 Batch  481/769   train_loss = 2.049
Epoch  31 Batch  491/769   train_loss = 2.094
Epoch  31 Batch  501/769   train_loss = 2.073
Epoch  31 Batch  511/769   train_loss = 1.982
Epoch  31 Batch  521/769   train_loss = 2.015
Epoch  31 Batch  531/769   train_loss = 2.011
Epoch  31 Batch  541/769   train_loss = 2.014
Epoch  31 Batch  551/769   train_loss = 2.115
Epoch  31 Batch  561/769   train_loss = 2.076
Epoch  31 Batch  571/769   train_loss = 2.077
Epoch  31 Batch  581/769   train_loss = 1.936
Epoch  31 Batch  591/769   train_loss = 2.004
Epoch  31 Batch  601/769   train_loss = 2.052
Epoch  31 Batch  611/769   train_loss = 1.997
Epoch  31 Batch  621/769   train_loss = 2.068
Epoch  31 Batch  631/769   train_loss = 2.078
Epoch  31 Batch  641/769   train_loss = 2.089
Epoch  31 Batch  651/769   train_loss = 2.005
Epoch  31 Batch  661/769   train_loss = 2.032
Epoch  31 Batch  671/769   train_loss = 2.088
Epoch  31 Batch  681/769   train_loss = 2.015
Epoch  31 Batch  691/769   train_loss = 2.030
Epoch  31 Batch  701/769   train_loss = 2.018
Epoch  31 Batch  711/769   train_loss = 2.075
Epoch  31 Batch  721/769   train_loss = 2.097
Epoch  31 Batch  731/769   train_loss = 2.049
Epoch  31 Batch  741/769   train_loss = 1.964
Epoch  31 Batch  751/769   train_loss = 1.994
Epoch  31 Batch  761/769   train_loss = 1.944
Epoch  32 Batch    2/769   train_loss = 2.065
Epoch  32 Batch   12/769   train_loss = 1.985
Epoch  32 Batch   22/769   train_loss = 2.056
Epoch  32 Batch   32/769   train_loss = 2.118
Epoch  32 Batch   42/769   train_loss = 2.107
Epoch  32 Batch   52/769   train_loss = 1.985
Epoch  32 Batch   62/769   train_loss = 2.062
Epoch  32 Batch   72/769   train_loss = 2.123
Epoch  32 Batch   82/769   train_loss = 1.988
Epoch  32 Batch   92/769   train_loss = 2.085
Epoch  32 Batch  102/769   train_loss = 2.059
Epoch  32 Batch  112/769   train_loss = 1.976
Epoch  32 Batch  122/769   train_loss = 1.963
Epoch  32 Batch  132/769   train_loss = 1.980
Epoch  32 Batch  142/769   train_loss = 2.011
Epoch  32 Batch  152/769   train_loss = 2.011
Epoch  32 Batch  162/769   train_loss = 2.053
Epoch  32 Batch  172/769   train_loss = 2.100
Epoch  32 Batch  182/769   train_loss = 1.975
Epoch  32 Batch  192/769   train_loss = 1.973
Epoch  32 Batch  202/769   train_loss = 1.988
Epoch  32 Batch  212/769   train_loss = 1.948
Epoch  32 Batch  222/769   train_loss = 2.028
Epoch  32 Batch  232/769   train_loss = 1.997
Epoch  32 Batch  242/769   train_loss = 1.980
Epoch  32 Batch  252/769   train_loss = 2.012
Epoch  32 Batch  262/769   train_loss = 1.975
Epoch  32 Batch  272/769   train_loss = 1.970
Epoch  32 Batch  282/769   train_loss = 2.018
Epoch  32 Batch  292/769   train_loss = 1.940
Epoch  32 Batch  302/769   train_loss = 2.005
Epoch  32 Batch  312/769   train_loss = 2.025
Epoch  32 Batch  322/769   train_loss = 1.979
Epoch  32 Batch  332/769   train_loss = 2.004
Epoch  32 Batch  342/769   train_loss = 1.934
Epoch  32 Batch  352/769   train_loss = 1.971
Epoch  32 Batch  362/769   train_loss = 2.027
Epoch  32 Batch  372/769   train_loss = 2.024
Epoch  32 Batch  382/769   train_loss = 1.924
Epoch  32 Batch  392/769   train_loss = 2.021
Epoch  32 Batch  402/769   train_loss = 2.097
Epoch  32 Batch  412/769   train_loss = 1.997
Epoch  32 Batch  422/769   train_loss = 1.935
Epoch  32 Batch  432/769   train_loss = 2.004
Epoch  32 Batch  442/769   train_loss = 1.916
Epoch  32 Batch  452/769   train_loss = 2.009
Epoch  32 Batch  462/769   train_loss = 2.048
Epoch  32 Batch  472/769   train_loss = 1.962
Epoch  32 Batch  482/769   train_loss = 2.022
Epoch  32 Batch  492/769   train_loss = 2.039
Epoch  32 Batch  502/769   train_loss = 1.998
Epoch  32 Batch  512/769   train_loss = 2.009
Epoch  32 Batch  522/769   train_loss = 1.991
Epoch  32 Batch  532/769   train_loss = 1.988
Epoch  32 Batch  542/769   train_loss = 1.987
Epoch  32 Batch  552/769   train_loss = 2.031
Epoch  32 Batch  562/769   train_loss = 1.962
Epoch  32 Batch  572/769   train_loss = 1.993
Epoch  32 Batch  582/769   train_loss = 1.987
Epoch  32 Batch  592/769   train_loss = 1.955
Epoch  32 Batch  602/769   train_loss = 1.994
Epoch  32 Batch  612/769   train_loss = 1.971
Epoch  32 Batch  622/769   train_loss = 1.955
Epoch  32 Batch  632/769   train_loss = 1.977
Epoch  32 Batch  642/769   train_loss = 2.031
Epoch  32 Batch  652/769   train_loss = 2.022
Epoch  32 Batch  662/769   train_loss = 1.991
Epoch  32 Batch  672/769   train_loss = 2.022
Epoch  32 Batch  682/769   train_loss = 2.044
Epoch  32 Batch  692/769   train_loss = 1.989
Epoch  32 Batch  702/769   train_loss = 2.100
Epoch  32 Batch  712/769   train_loss = 2.045
Epoch  32 Batch  722/769   train_loss = 1.990
Epoch  32 Batch  732/769   train_loss = 1.919
Epoch  32 Batch  742/769   train_loss = 1.998
Epoch  32 Batch  752/769   train_loss = 2.071
Epoch  32 Batch  762/769   train_loss = 2.011
Epoch  33 Batch    3/769   train_loss = 2.055
Epoch  33 Batch   13/769   train_loss = 2.050
Epoch  33 Batch   23/769   train_loss = 2.039
Epoch  33 Batch   33/769   train_loss = 2.010
Epoch  33 Batch   43/769   train_loss = 2.063
Epoch  33 Batch   53/769   train_loss = 2.004
Epoch  33 Batch   63/769   train_loss = 1.862
Epoch  33 Batch   73/769   train_loss = 2.071
Epoch  33 Batch   83/769   train_loss = 1.934
Epoch  33 Batch   93/769   train_loss = 1.976
Epoch  33 Batch  103/769   train_loss = 1.970
Epoch  33 Batch  113/769   train_loss = 1.966
Epoch  33 Batch  123/769   train_loss = 2.011
Epoch  33 Batch  133/769   train_loss = 1.928
Epoch  33 Batch  143/769   train_loss = 2.006
Epoch  33 Batch  153/769   train_loss = 2.018
Epoch  33 Batch  163/769   train_loss = 1.992
Epoch  33 Batch  173/769   train_loss = 2.007
Epoch  33 Batch  183/769   train_loss = 2.042
Epoch  33 Batch  193/769   train_loss = 1.947
Epoch  33 Batch  203/769   train_loss = 1.919
Epoch  33 Batch  213/769   train_loss = 1.948
Epoch  33 Batch  223/769   train_loss = 2.037
Epoch  33 Batch  233/769   train_loss = 1.933
Epoch  33 Batch  243/769   train_loss = 1.983
Epoch  33 Batch  253/769   train_loss = 1.965
Epoch  33 Batch  263/769   train_loss = 2.001
Epoch  33 Batch  273/769   train_loss = 2.015
Epoch  33 Batch  283/769   train_loss = 2.020
Epoch  33 Batch  293/769   train_loss = 1.968
Epoch  33 Batch  303/769   train_loss = 1.984
Epoch  33 Batch  313/769   train_loss = 2.017
Epoch  33 Batch  323/769   train_loss = 2.029
Epoch  33 Batch  333/769   train_loss = 1.983
Epoch  33 Batch  343/769   train_loss = 1.961
Epoch  33 Batch  353/769   train_loss = 2.029
Epoch  33 Batch  363/769   train_loss = 1.981
Epoch  33 Batch  373/769   train_loss = 2.079
Epoch  33 Batch  383/769   train_loss = 1.962
Epoch  33 Batch  393/769   train_loss = 2.042
Epoch  33 Batch  403/769   train_loss = 2.004
Epoch  33 Batch  413/769   train_loss = 2.016
Epoch  33 Batch  423/769   train_loss = 1.961
Epoch  33 Batch  433/769   train_loss = 1.958
Epoch  33 Batch  443/769   train_loss = 1.967
Epoch  33 Batch  453/769   train_loss = 2.005
Epoch  33 Batch  463/769   train_loss = 1.999
Epoch  33 Batch  473/769   train_loss = 1.993
Epoch  33 Batch  483/769   train_loss = 1.994
Epoch  33 Batch  493/769   train_loss = 1.996
Epoch  33 Batch  503/769   train_loss = 2.036
Epoch  33 Batch  513/769   train_loss = 2.031
Epoch  33 Batch  523/769   train_loss = 1.946
Epoch  33 Batch  533/769   train_loss = 1.975
Epoch  33 Batch  543/769   train_loss = 1.929
Epoch  33 Batch  553/769   train_loss = 1.961
Epoch  33 Batch  563/769   train_loss = 1.950
Epoch  33 Batch  573/769   train_loss = 1.953
Epoch  33 Batch  583/769   train_loss = 1.935
Epoch  33 Batch  593/769   train_loss = 2.022
Epoch  33 Batch  603/769   train_loss = 1.902
Epoch  33 Batch  613/769   train_loss = 1.977
Epoch  33 Batch  623/769   train_loss = 1.926
Epoch  33 Batch  633/769   train_loss = 1.892
Epoch  33 Batch  643/769   train_loss = 1.869
Epoch  33 Batch  653/769   train_loss = 1.952
Epoch  33 Batch  663/769   train_loss = 1.929
Epoch  33 Batch  673/769   train_loss = 2.013
Epoch  33 Batch  683/769   train_loss = 1.956
Epoch  33 Batch  693/769   train_loss = 2.000
Epoch  33 Batch  703/769   train_loss = 1.965
Epoch  33 Batch  713/769   train_loss = 1.999
Epoch  33 Batch  723/769   train_loss = 1.945
Epoch  33 Batch  733/769   train_loss = 1.974
Epoch  33 Batch  743/769   train_loss = 2.046
Epoch  33 Batch  753/769   train_loss = 1.968
Epoch  33 Batch  763/769   train_loss = 2.005
Epoch  34 Batch    4/769   train_loss = 1.992
Epoch  34 Batch   14/769   train_loss = 1.972
Epoch  34 Batch   24/769   train_loss = 1.987
Epoch  34 Batch   34/769   train_loss = 1.964
Epoch  34 Batch   44/769   train_loss = 2.003
Epoch  34 Batch   54/769   train_loss = 1.919
Epoch  34 Batch   64/769   train_loss = 1.929
Epoch  34 Batch   74/769   train_loss = 2.005
Epoch  34 Batch   84/769   train_loss = 1.931
Epoch  34 Batch   94/769   train_loss = 1.932
Epoch  34 Batch  104/769   train_loss = 2.045
Epoch  34 Batch  114/769   train_loss = 1.978
Epoch  34 Batch  124/769   train_loss = 1.926
Epoch  34 Batch  134/769   train_loss = 1.918
Epoch  34 Batch  144/769   train_loss = 1.992
Epoch  34 Batch  154/769   train_loss = 1.980
Epoch  34 Batch  164/769   train_loss = 1.957
Epoch  34 Batch  174/769   train_loss = 1.960
Epoch  34 Batch  184/769   train_loss = 1.952
Epoch  34 Batch  194/769   train_loss = 1.811
Epoch  34 Batch  204/769   train_loss = 1.889
Epoch  34 Batch  214/769   train_loss = 1.939
Epoch  34 Batch  224/769   train_loss = 1.963
Epoch  34 Batch  234/769   train_loss = 1.981
Epoch  34 Batch  244/769   train_loss = 1.964
Epoch  34 Batch  254/769   train_loss = 1.858
Epoch  34 Batch  264/769   train_loss = 1.910
Epoch  34 Batch  274/769   train_loss = 1.878
Epoch  34 Batch  284/769   train_loss = 1.890
Epoch  34 Batch  294/769   train_loss = 1.906
Epoch  34 Batch  304/769   train_loss = 1.957
Epoch  34 Batch  314/769   train_loss = 1.991
Epoch  34 Batch  324/769   train_loss = 1.933
Epoch  34 Batch  334/769   train_loss = 1.941
Epoch  34 Batch  344/769   train_loss = 1.954
Epoch  34 Batch  354/769   train_loss = 2.069
Epoch  34 Batch  364/769   train_loss = 1.941
Epoch  34 Batch  374/769   train_loss = 1.977
Epoch  34 Batch  384/769   train_loss = 1.856
Epoch  34 Batch  394/769   train_loss = 2.017
Epoch  34 Batch  404/769   train_loss = 2.032
Epoch  34 Batch  414/769   train_loss = 1.913
Epoch  34 Batch  424/769   train_loss = 1.858
Epoch  34 Batch  434/769   train_loss = 1.924
Epoch  34 Batch  444/769   train_loss = 2.000
Epoch  34 Batch  454/769   train_loss = 1.957
Epoch  34 Batch  464/769   train_loss = 2.051
Epoch  34 Batch  474/769   train_loss = 1.969
Epoch  34 Batch  484/769   train_loss = 1.950
Epoch  34 Batch  494/769   train_loss = 1.918
Epoch  34 Batch  504/769   train_loss = 2.021
Epoch  34 Batch  514/769   train_loss = 1.956
Epoch  34 Batch  524/769   train_loss = 2.037
Epoch  34 Batch  534/769   train_loss = 1.908
Epoch  34 Batch  544/769   train_loss = 1.940
Epoch  34 Batch  554/769   train_loss = 1.931
Epoch  34 Batch  564/769   train_loss = 1.954
Epoch  34 Batch  574/769   train_loss = 1.935
Epoch  34 Batch  584/769   train_loss = 1.828
Epoch  34 Batch  594/769   train_loss = 1.907
Epoch  34 Batch  604/769   train_loss = 1.869
Epoch  34 Batch  614/769   train_loss = 1.913
Epoch  34 Batch  624/769   train_loss = 2.014
Epoch  34 Batch  634/769   train_loss = 1.911
Epoch  34 Batch  644/769   train_loss = 1.863
Epoch  34 Batch  654/769   train_loss = 1.987
Epoch  34 Batch  664/769   train_loss = 1.955
Epoch  34 Batch  674/769   train_loss = 1.967
Epoch  34 Batch  684/769   train_loss = 1.966
Epoch  34 Batch  694/769   train_loss = 1.984
Epoch  34 Batch  704/769   train_loss = 1.977
Epoch  34 Batch  714/769   train_loss = 1.925
Epoch  34 Batch  724/769   train_loss = 1.969
Epoch  34 Batch  734/769   train_loss = 2.067
Epoch  34 Batch  744/769   train_loss = 1.942
Epoch  34 Batch  754/769   train_loss = 2.000
Epoch  34 Batch  764/769   train_loss = 1.945
Epoch  35 Batch    5/769   train_loss = 2.055
Epoch  35 Batch   15/769   train_loss = 1.940
Epoch  35 Batch   25/769   train_loss = 2.018
Epoch  35 Batch   35/769   train_loss = 2.008
Epoch  35 Batch   45/769   train_loss = 1.961
Epoch  35 Batch   55/769   train_loss = 1.940
Epoch  35 Batch   65/769   train_loss = 1.941
Epoch  35 Batch   75/769   train_loss = 1.999
Epoch  35 Batch   85/769   train_loss = 2.018
Epoch  35 Batch   95/769   train_loss = 1.900
Epoch  35 Batch  105/769   train_loss = 1.985
Epoch  35 Batch  115/769   train_loss = 1.950
Epoch  35 Batch  125/769   train_loss = 1.786
Epoch  35 Batch  135/769   train_loss = 1.929
Epoch  35 Batch  145/769   train_loss = 1.969
Epoch  35 Batch  155/769   train_loss = 1.989
Epoch  35 Batch  165/769   train_loss = 1.947
Epoch  35 Batch  175/769   train_loss = 1.843
Epoch  35 Batch  185/769   train_loss = 1.893
Epoch  35 Batch  195/769   train_loss = 1.880
Epoch  35 Batch  205/769   train_loss = 2.005
Epoch  35 Batch  215/769   train_loss = 1.939
Epoch  35 Batch  225/769   train_loss = 1.850
Epoch  35 Batch  235/769   train_loss = 1.897
Epoch  35 Batch  245/769   train_loss = 1.930
Epoch  35 Batch  255/769   train_loss = 1.931
Epoch  35 Batch  265/769   train_loss = 1.805
Epoch  35 Batch  275/769   train_loss = 1.886
Epoch  35 Batch  285/769   train_loss = 1.985
Epoch  35 Batch  295/769   train_loss = 1.946
Epoch  35 Batch  305/769   train_loss = 1.934
Epoch  35 Batch  315/769   train_loss = 1.844
Epoch  35 Batch  325/769   train_loss = 1.902
Epoch  35 Batch  335/769   train_loss = 1.946
Epoch  35 Batch  345/769   train_loss = 1.846
Epoch  35 Batch  355/769   train_loss = 1.859
Epoch  35 Batch  365/769   train_loss = 1.933
Epoch  35 Batch  375/769   train_loss = 2.003
Epoch  35 Batch  385/769   train_loss = 1.813
Epoch  35 Batch  395/769   train_loss = 1.987
Epoch  35 Batch  405/769   train_loss = 1.898
Epoch  35 Batch  415/769   train_loss = 1.784
Epoch  35 Batch  425/769   train_loss = 1.849
Epoch  35 Batch  435/769   train_loss = 1.849
Epoch  35 Batch  445/769   train_loss = 1.949
Epoch  35 Batch  455/769   train_loss = 1.969
Epoch  35 Batch  465/769   train_loss = 2.008
Epoch  35 Batch  475/769   train_loss = 1.824
Epoch  35 Batch  485/769   train_loss = 1.979
Epoch  35 Batch  495/769   train_loss = 1.936
Epoch  35 Batch  505/769   train_loss = 1.924
Epoch  35 Batch  515/769   train_loss = 1.973
Epoch  35 Batch  525/769   train_loss = 1.907
Epoch  35 Batch  535/769   train_loss = 1.881
Epoch  35 Batch  545/769   train_loss = 1.942
Epoch  35 Batch  555/769   train_loss = 1.938
Epoch  35 Batch  565/769   train_loss = 2.000
Epoch  35 Batch  575/769   train_loss = 1.933
Epoch  35 Batch  585/769   train_loss = 1.893
Epoch  35 Batch  595/769   train_loss = 1.886
Epoch  35 Batch  605/769   train_loss = 1.846
Epoch  35 Batch  615/769   train_loss = 1.956
Epoch  35 Batch  625/769   train_loss = 1.936
Epoch  35 Batch  635/769   train_loss = 1.849
Epoch  35 Batch  645/769   train_loss = 1.859
Epoch  35 Batch  655/769   train_loss = 1.883
Epoch  35 Batch  665/769   train_loss = 1.906
Epoch  35 Batch  675/769   train_loss = 1.929
Epoch  35 Batch  685/769   train_loss = 1.934
Epoch  35 Batch  695/769   train_loss = 2.016
Epoch  35 Batch  705/769   train_loss = 1.944
Epoch  35 Batch  715/769   train_loss = 1.957
Epoch  35 Batch  725/769   train_loss = 1.885
Epoch  35 Batch  735/769   train_loss = 1.933
Epoch  35 Batch  745/769   train_loss = 1.967
Epoch  35 Batch  755/769   train_loss = 1.829
Epoch  35 Batch  765/769   train_loss = 1.960
Epoch  36 Batch    6/769   train_loss = 1.879
Epoch  36 Batch   16/769   train_loss = 1.884
Epoch  36 Batch   26/769   train_loss = 1.911
Epoch  36 Batch   36/769   train_loss = 1.978
Epoch  36 Batch   46/769   train_loss = 1.984
Epoch  36 Batch   56/769   train_loss = 1.881
Epoch  36 Batch   66/769   train_loss = 1.942
Epoch  36 Batch   76/769   train_loss = 1.970
Epoch  36 Batch   86/769   train_loss = 1.969
Epoch  36 Batch   96/769   train_loss = 1.832
Epoch  36 Batch  106/769   train_loss = 1.979
Epoch  36 Batch  116/769   train_loss = 1.872
Epoch  36 Batch  126/769   train_loss = 1.858
Epoch  36 Batch  136/769   train_loss = 1.846
Epoch  36 Batch  146/769   train_loss = 1.903
Epoch  36 Batch  156/769   train_loss = 1.932
Epoch  36 Batch  166/769   train_loss = 1.891
Epoch  36 Batch  176/769   train_loss = 1.858
Epoch  36 Batch  186/769   train_loss = 1.821
Epoch  36 Batch  196/769   train_loss = 1.893
Epoch  36 Batch  206/769   train_loss = 1.925
Epoch  36 Batch  216/769   train_loss = 1.876
Epoch  36 Batch  226/769   train_loss = 1.886
Epoch  36 Batch  236/769   train_loss = 1.930
Epoch  36 Batch  246/769   train_loss = 1.901
Epoch  36 Batch  256/769   train_loss = 1.862
Epoch  36 Batch  266/769   train_loss = 1.899
Epoch  36 Batch  276/769   train_loss = 1.849
Epoch  36 Batch  286/769   train_loss = 1.866
Epoch  36 Batch  296/769   train_loss = 1.850
Epoch  36 Batch  306/769   train_loss = 1.858
Epoch  36 Batch  316/769   train_loss = 1.848
Epoch  36 Batch  326/769   train_loss = 1.937
Epoch  36 Batch  336/769   train_loss = 1.938
Epoch  36 Batch  346/769   train_loss = 1.894
Epoch  36 Batch  356/769   train_loss = 1.844
Epoch  36 Batch  366/769   train_loss = 1.901
Epoch  36 Batch  376/769   train_loss = 1.861
Epoch  36 Batch  386/769   train_loss = 1.933
Epoch  36 Batch  396/769   train_loss = 1.954
Epoch  36 Batch  406/769   train_loss = 1.838
Epoch  36 Batch  416/769   train_loss = 1.910
Epoch  36 Batch  426/769   train_loss = 1.951
Epoch  36 Batch  436/769   train_loss = 1.866
Epoch  36 Batch  446/769   train_loss = 1.906
Epoch  36 Batch  456/769   train_loss = 1.926
Epoch  36 Batch  466/769   train_loss = 1.916
Epoch  36 Batch  476/769   train_loss = 1.896
Epoch  36 Batch  486/769   train_loss = 1.935
Epoch  36 Batch  496/769   train_loss = 1.875
Epoch  36 Batch  506/769   train_loss = 1.945
Epoch  36 Batch  516/769   train_loss = 1.892
Epoch  36 Batch  526/769   train_loss = 1.832
Epoch  36 Batch  536/769   train_loss = 1.899
Epoch  36 Batch  546/769   train_loss = 1.913
Epoch  36 Batch  556/769   train_loss = 1.942
Epoch  36 Batch  566/769   train_loss = 1.996
Epoch  36 Batch  576/769   train_loss = 1.835
Epoch  36 Batch  586/769   train_loss = 1.825
Epoch  36 Batch  596/769   train_loss = 1.950
Epoch  36 Batch  606/769   train_loss = 1.906
Epoch  36 Batch  616/769   train_loss = 1.841
Epoch  36 Batch  626/769   train_loss = 1.914
Epoch  36 Batch  636/769   train_loss = 1.843
Epoch  36 Batch  646/769   train_loss = 1.986
Epoch  36 Batch  656/769   train_loss = 1.887
Epoch  36 Batch  666/769   train_loss = 1.891
Epoch  36 Batch  676/769   train_loss = 1.912
Epoch  36 Batch  686/769   train_loss = 1.913
Epoch  36 Batch  696/769   train_loss = 1.921
Epoch  36 Batch  706/769   train_loss = 1.972
Epoch  36 Batch  716/769   train_loss = 1.930
Epoch  36 Batch  726/769   train_loss = 1.910
Epoch  36 Batch  736/769   train_loss = 1.934
Epoch  36 Batch  746/769   train_loss = 1.850
Epoch  36 Batch  756/769   train_loss = 1.867
Epoch  36 Batch  766/769   train_loss = 1.913
Epoch  37 Batch    7/769   train_loss = 1.953
Epoch  37 Batch   17/769   train_loss = 1.931
Epoch  37 Batch   27/769   train_loss = 1.886
Epoch  37 Batch   37/769   train_loss = 1.854
Epoch  37 Batch   47/769   train_loss = 1.838
Epoch  37 Batch   57/769   train_loss = 1.929
Epoch  37 Batch   67/769   train_loss = 1.905
Epoch  37 Batch   77/769   train_loss = 1.959
Epoch  37 Batch   87/769   train_loss = 1.875
Epoch  37 Batch   97/769   train_loss = 1.820
Epoch  37 Batch  107/769   train_loss = 1.879
Epoch  37 Batch  117/769   train_loss = 1.838
Epoch  37 Batch  127/769   train_loss = 1.905
Epoch  37 Batch  137/769   train_loss = 1.822
Epoch  37 Batch  147/769   train_loss = 1.894
Epoch  37 Batch  157/769   train_loss = 1.986
Epoch  37 Batch  167/769   train_loss = 1.882
Epoch  37 Batch  177/769   train_loss = 1.804
Epoch  37 Batch  187/769   train_loss = 1.928
Epoch  37 Batch  197/769   train_loss = 1.823
Epoch  37 Batch  207/769   train_loss = 1.828
Epoch  37 Batch  217/769   train_loss = 1.882
Epoch  37 Batch  227/769   train_loss = 1.839
Epoch  37 Batch  237/769   train_loss = 1.849
Epoch  37 Batch  247/769   train_loss = 1.872
Epoch  37 Batch  257/769   train_loss = 1.826
Epoch  37 Batch  267/769   train_loss = 1.831
Epoch  37 Batch  277/769   train_loss = 1.885
Epoch  37 Batch  287/769   train_loss = 1.808
Epoch  37 Batch  297/769   train_loss = 1.824
Epoch  37 Batch  307/769   train_loss = 1.924
Epoch  37 Batch  317/769   train_loss = 1.767
Epoch  37 Batch  327/769   train_loss = 1.862
Epoch  37 Batch  337/769   train_loss = 1.889
Epoch  37 Batch  347/769   train_loss = 1.825
Epoch  37 Batch  357/769   train_loss = 1.880
Epoch  37 Batch  367/769   train_loss = 1.822
Epoch  37 Batch  377/769   train_loss = 1.867
Epoch  37 Batch  387/769   train_loss = 1.917
Epoch  37 Batch  397/769   train_loss = 1.843
Epoch  37 Batch  407/769   train_loss = 1.897
Epoch  37 Batch  417/769   train_loss = 1.906
Epoch  37 Batch  427/769   train_loss = 1.891
Epoch  37 Batch  437/769   train_loss = 1.911
Epoch  37 Batch  447/769   train_loss = 1.912
Epoch  37 Batch  457/769   train_loss = 1.824
Epoch  37 Batch  467/769   train_loss = 1.830
Epoch  37 Batch  477/769   train_loss = 1.916
Epoch  37 Batch  487/769   train_loss = 1.859
Epoch  37 Batch  497/769   train_loss = 1.921
Epoch  37 Batch  507/769   train_loss = 1.880
Epoch  37 Batch  517/769   train_loss = 1.879
Epoch  37 Batch  527/769   train_loss = 1.829
Epoch  37 Batch  537/769   train_loss = 1.948
Epoch  37 Batch  547/769   train_loss = 1.944
Epoch  37 Batch  557/769   train_loss = 1.875
Epoch  37 Batch  567/769   train_loss = 1.856
Epoch  37 Batch  577/769   train_loss = 1.937
Epoch  37 Batch  587/769   train_loss = 1.832
Epoch  37 Batch  597/769   train_loss = 1.775
Epoch  37 Batch  607/769   train_loss = 1.869
Epoch  37 Batch  617/769   train_loss = 1.860
Epoch  37 Batch  627/769   train_loss = 1.956
Epoch  37 Batch  637/769   train_loss = 1.834
Epoch  37 Batch  647/769   train_loss = 1.912
Epoch  37 Batch  657/769   train_loss = 1.771
Epoch  37 Batch  667/769   train_loss = 1.917
Epoch  37 Batch  677/769   train_loss = 1.841
Epoch  37 Batch  687/769   train_loss = 1.933
Epoch  37 Batch  697/769   train_loss = 1.962
Epoch  37 Batch  707/769   train_loss = 1.886
Epoch  37 Batch  717/769   train_loss = 1.873
Epoch  37 Batch  727/769   train_loss = 1.907
Epoch  37 Batch  737/769   train_loss = 1.976
Epoch  37 Batch  747/769   train_loss = 1.839
Epoch  37 Batch  757/769   train_loss = 1.905
Epoch  37 Batch  767/769   train_loss = 1.910
Epoch  38 Batch    8/769   train_loss = 1.878
Epoch  38 Batch   18/769   train_loss = 1.889
Epoch  38 Batch   28/769   train_loss = 1.920
Epoch  38 Batch   38/769   train_loss = 1.940
Epoch  38 Batch   48/769   train_loss = 1.887
Epoch  38 Batch   58/769   train_loss = 1.849
Epoch  38 Batch   68/769   train_loss = 1.927
Epoch  38 Batch   78/769   train_loss = 1.893
Epoch  38 Batch   88/769   train_loss = 1.856
Epoch  38 Batch   98/769   train_loss = 1.790
Epoch  38 Batch  108/769   train_loss = 1.881
Epoch  38 Batch  118/769   train_loss = 1.808
Epoch  38 Batch  128/769   train_loss = 1.868
Epoch  38 Batch  138/769   train_loss = 1.815
Epoch  38 Batch  148/769   train_loss = 1.897
Epoch  38 Batch  158/769   train_loss = 1.900
Epoch  38 Batch  168/769   train_loss = 1.824
Epoch  38 Batch  178/769   train_loss = 1.889
Epoch  38 Batch  188/769   train_loss = 1.847
Epoch  38 Batch  198/769   train_loss = 1.851
Epoch  38 Batch  208/769   train_loss = 1.878
Epoch  38 Batch  218/769   train_loss = 1.885
Epoch  38 Batch  228/769   train_loss = 1.828
Epoch  38 Batch  238/769   train_loss = 1.873
Epoch  38 Batch  248/769   train_loss = 1.853
Epoch  38 Batch  258/769   train_loss = 1.817
Epoch  38 Batch  268/769   train_loss = 1.840
Epoch  38 Batch  278/769   train_loss = 1.908
Epoch  38 Batch  288/769   train_loss = 1.871
Epoch  38 Batch  298/769   train_loss = 1.888
Epoch  38 Batch  308/769   train_loss = 1.868
Epoch  38 Batch  318/769   train_loss = 1.811
Epoch  38 Batch  328/769   train_loss = 1.820
Epoch  38 Batch  338/769   train_loss = 1.860
Epoch  38 Batch  348/769   train_loss = 1.828
Epoch  38 Batch  358/769   train_loss = 1.841
Epoch  38 Batch  368/769   train_loss = 1.872
Epoch  38 Batch  378/769   train_loss = 1.869
Epoch  38 Batch  388/769   train_loss = 1.775
Epoch  38 Batch  398/769   train_loss = 1.934
Epoch  38 Batch  408/769   train_loss = 1.858
Epoch  38 Batch  418/769   train_loss = 1.851
Epoch  38 Batch  428/769   train_loss = 1.851
Epoch  38 Batch  438/769   train_loss = 1.793
Epoch  38 Batch  448/769   train_loss = 1.871
Epoch  38 Batch  458/769   train_loss = 1.929
Epoch  38 Batch  468/769   train_loss = 1.852
Epoch  38 Batch  478/769   train_loss = 1.766
Epoch  38 Batch  488/769   train_loss = 1.851
Epoch  38 Batch  498/769   train_loss = 1.865
Epoch  38 Batch  508/769   train_loss = 1.868
Epoch  38 Batch  518/769   train_loss = 1.877
Epoch  38 Batch  528/769   train_loss = 1.800
Epoch  38 Batch  538/769   train_loss = 1.889
Epoch  38 Batch  548/769   train_loss = 1.832
Epoch  38 Batch  558/769   train_loss = 1.849
Epoch  38 Batch  568/769   train_loss = 1.820
Epoch  38 Batch  578/769   train_loss = 1.689
Epoch  38 Batch  588/769   train_loss = 1.780
Epoch  38 Batch  598/769   train_loss = 1.814
Epoch  38 Batch  608/769   train_loss = 1.806
Epoch  38 Batch  618/769   train_loss = 1.755
Epoch  38 Batch  628/769   train_loss = 1.826
Epoch  38 Batch  638/769   train_loss = 1.787
Epoch  38 Batch  648/769   train_loss = 1.930
Epoch  38 Batch  658/769   train_loss = 1.954
Epoch  38 Batch  668/769   train_loss = 1.841
Epoch  38 Batch  678/769   train_loss = 1.841
Epoch  38 Batch  688/769   train_loss = 1.834
Epoch  38 Batch  698/769   train_loss = 1.867
Epoch  38 Batch  708/769   train_loss = 1.856
Epoch  38 Batch  718/769   train_loss = 1.845
Epoch  38 Batch  728/769   train_loss = 1.895
Epoch  38 Batch  738/769   train_loss = 1.829
Epoch  38 Batch  748/769   train_loss = 1.870
Epoch  38 Batch  758/769   train_loss = 1.866
Epoch  38 Batch  768/769   train_loss = 1.870
Epoch  39 Batch    9/769   train_loss = 1.922
Epoch  39 Batch   19/769   train_loss = 1.927
Epoch  39 Batch   29/769   train_loss = 1.890
Epoch  39 Batch   39/769   train_loss = 1.875
Epoch  39 Batch   49/769   train_loss = 1.851
Epoch  39 Batch   59/769   train_loss = 1.803
Epoch  39 Batch   69/769   train_loss = 1.892
Epoch  39 Batch   79/769   train_loss = 1.835
Epoch  39 Batch   89/769   train_loss = 1.732
Epoch  39 Batch   99/769   train_loss = 1.870
Epoch  39 Batch  109/769   train_loss = 1.821
Epoch  39 Batch  119/769   train_loss = 1.796
Epoch  39 Batch  129/769   train_loss = 1.732
Epoch  39 Batch  139/769   train_loss = 1.802
Epoch  39 Batch  149/769   train_loss = 1.820
Epoch  39 Batch  159/769   train_loss = 1.758
Epoch  39 Batch  169/769   train_loss = 1.866
Epoch  39 Batch  179/769   train_loss = 1.814
Epoch  39 Batch  189/769   train_loss = 1.823
Epoch  39 Batch  199/769   train_loss = 1.793
Epoch  39 Batch  209/769   train_loss = 1.834
Epoch  39 Batch  219/769   train_loss = 1.823
Epoch  39 Batch  229/769   train_loss = 1.883
Epoch  39 Batch  239/769   train_loss = 1.784
Epoch  39 Batch  249/769   train_loss = 1.786
Epoch  39 Batch  259/769   train_loss = 1.773
Epoch  39 Batch  269/769   train_loss = 1.825
Epoch  39 Batch  279/769   train_loss = 1.883
Epoch  39 Batch  289/769   train_loss = 1.780
Epoch  39 Batch  299/769   train_loss = 1.912
Epoch  39 Batch  309/769   train_loss = 1.854
Epoch  39 Batch  319/769   train_loss = 1.846
Epoch  39 Batch  329/769   train_loss = 1.838
Epoch  39 Batch  339/769   train_loss = 1.793
Epoch  39 Batch  349/769   train_loss = 1.797
Epoch  39 Batch  359/769   train_loss = 1.867
Epoch  39 Batch  369/769   train_loss = 1.780
Epoch  39 Batch  379/769   train_loss = 1.731
Epoch  39 Batch  389/769   train_loss = 1.734
Epoch  39 Batch  399/769   train_loss = 1.877
Epoch  39 Batch  409/769   train_loss = 1.877
Epoch  39 Batch  419/769   train_loss = 1.926
Epoch  39 Batch  429/769   train_loss = 1.779
Epoch  39 Batch  439/769   train_loss = 1.886
Epoch  39 Batch  449/769   train_loss = 1.905
Epoch  39 Batch  459/769   train_loss = 1.809
Epoch  39 Batch  469/769   train_loss = 1.768
Epoch  39 Batch  479/769   train_loss = 1.845
Epoch  39 Batch  489/769   train_loss = 1.879
Epoch  39 Batch  499/769   train_loss = 1.876
Epoch  39 Batch  509/769   train_loss = 1.830
Epoch  39 Batch  519/769   train_loss = 1.908
Epoch  39 Batch  529/769   train_loss = 1.905
Epoch  39 Batch  539/769   train_loss = 1.869
Epoch  39 Batch  549/769   train_loss = 1.867
Epoch  39 Batch  559/769   train_loss = 1.873
Epoch  39 Batch  569/769   train_loss = 1.776
Epoch  39 Batch  579/769   train_loss = 1.704
Epoch  39 Batch  589/769   train_loss = 1.810
Epoch  39 Batch  599/769   train_loss = 1.780
Epoch  39 Batch  609/769   train_loss = 1.832
Epoch  39 Batch  619/769   train_loss = 1.862
Epoch  39 Batch  629/769   train_loss = 1.807
Epoch  39 Batch  639/769   train_loss = 1.797
Epoch  39 Batch  649/769   train_loss = 1.767
Epoch  39 Batch  659/769   train_loss = 1.800
Epoch  39 Batch  669/769   train_loss = 1.816
Epoch  39 Batch  679/769   train_loss = 1.785
Epoch  39 Batch  689/769   train_loss = 1.894
Epoch  39 Batch  699/769   train_loss = 1.812
Epoch  39 Batch  709/769   train_loss = 1.873
Epoch  39 Batch  719/769   train_loss = 1.790
Epoch  39 Batch  729/769   train_loss = 1.823
Epoch  39 Batch  739/769   train_loss = 1.785
Epoch  39 Batch  749/769   train_loss = 1.744
Epoch  39 Batch  759/769   train_loss = 1.822
Epoch  40 Batch    0/769   train_loss = 1.718
Epoch  40 Batch   10/769   train_loss = 1.930
Epoch  40 Batch   20/769   train_loss = 1.817
Epoch  40 Batch   30/769   train_loss = 1.804
Epoch  40 Batch   40/769   train_loss = 1.784
Epoch  40 Batch   50/769   train_loss = 1.806
Epoch  40 Batch   60/769   train_loss = 1.908
Epoch  40 Batch   70/769   train_loss = 1.839
Epoch  40 Batch   80/769   train_loss = 1.804
Epoch  40 Batch   90/769   train_loss = 1.860
Epoch  40 Batch  100/769   train_loss = 1.811
Epoch  40 Batch  110/769   train_loss = 1.803
Epoch  40 Batch  120/769   train_loss = 1.699
Epoch  40 Batch  130/769   train_loss = 1.737
Epoch  40 Batch  140/769   train_loss = 1.848
Epoch  40 Batch  150/769   train_loss = 1.797
Epoch  40 Batch  160/769   train_loss = 1.796
Epoch  40 Batch  170/769   train_loss = 1.780
Epoch  40 Batch  180/769   train_loss = 1.776
Epoch  40 Batch  190/769   train_loss = 1.729
Epoch  40 Batch  200/769   train_loss = 1.874
Epoch  40 Batch  210/769   train_loss = 1.749
Epoch  40 Batch  220/769   train_loss = 1.717
Epoch  40 Batch  230/769   train_loss = 1.776
Epoch  40 Batch  240/769   train_loss = 1.787
Epoch  40 Batch  250/769   train_loss = 1.742
Epoch  40 Batch  260/769   train_loss = 1.847
Epoch  40 Batch  270/769   train_loss = 1.760
Epoch  40 Batch  280/769   train_loss = 1.806
Epoch  40 Batch  290/769   train_loss = 1.795
Epoch  40 Batch  300/769   train_loss = 1.841
Epoch  40 Batch  310/769   train_loss = 1.748
Epoch  40 Batch  320/769   train_loss = 1.728
Epoch  40 Batch  330/769   train_loss = 1.821
Epoch  40 Batch  340/769   train_loss = 1.747
Epoch  40 Batch  350/769   train_loss = 1.694
Epoch  40 Batch  360/769   train_loss = 1.760
Epoch  40 Batch  370/769   train_loss = 1.769
Epoch  40 Batch  380/769   train_loss = 1.791
Epoch  40 Batch  390/769   train_loss = 1.719
Epoch  40 Batch  400/769   train_loss = 1.811
Epoch  40 Batch  410/769   train_loss = 1.873
Epoch  40 Batch  420/769   train_loss = 1.780
Epoch  40 Batch  430/769   train_loss = 1.802
Epoch  40 Batch  440/769   train_loss = 1.846
Epoch  40 Batch  450/769   train_loss = 1.808
Epoch  40 Batch  460/769   train_loss = 1.924
Epoch  40 Batch  470/769   train_loss = 1.751
Epoch  40 Batch  480/769   train_loss = 1.802
Epoch  40 Batch  490/769   train_loss = 1.783
Epoch  40 Batch  500/769   train_loss = 1.831
Epoch  40 Batch  510/769   train_loss = 1.858
Epoch  40 Batch  520/769   train_loss = 1.700
Epoch  40 Batch  530/769   train_loss = 1.816
Epoch  40 Batch  540/769   train_loss = 1.906
Epoch  40 Batch  550/769   train_loss = 1.737
Epoch  40 Batch  560/769   train_loss = 1.833
Epoch  40 Batch  570/769   train_loss = 1.790
Epoch  40 Batch  580/769   train_loss = 1.755
Epoch  40 Batch  590/769   train_loss = 1.757
Epoch  40 Batch  600/769   train_loss = 1.751
Epoch  40 Batch  610/769   train_loss = 1.774
Epoch  40 Batch  620/769   train_loss = 1.801
Epoch  40 Batch  630/769   train_loss = 1.855
Epoch  40 Batch  640/769   train_loss = 1.750
Epoch  40 Batch  650/769   train_loss = 1.783
Epoch  40 Batch  660/769   train_loss = 1.820
Epoch  40 Batch  670/769   train_loss = 1.764
Epoch  40 Batch  680/769   train_loss = 1.846
Epoch  40 Batch  690/769   train_loss = 1.691
Epoch  40 Batch  700/769   train_loss = 1.827
Epoch  40 Batch  710/769   train_loss = 1.826
Epoch  40 Batch  720/769   train_loss = 1.755
Epoch  40 Batch  730/769   train_loss = 1.799
Epoch  40 Batch  740/769   train_loss = 1.758
Epoch  40 Batch  750/769   train_loss = 1.725
Epoch  40 Batch  760/769   train_loss = 1.835
Epoch  41 Batch    1/769   train_loss = 1.803
Epoch  41 Batch   11/769   train_loss = 1.776
Epoch  41 Batch   21/769   train_loss = 1.793
Epoch  41 Batch   31/769   train_loss = 1.755
Epoch  41 Batch   41/769   train_loss = 1.869
Epoch  41 Batch   51/769   train_loss = 1.821
Epoch  41 Batch   61/769   train_loss = 1.849
Epoch  41 Batch   71/769   train_loss = 1.765
Epoch  41 Batch   81/769   train_loss = 1.944
Epoch  41 Batch   91/769   train_loss = 1.810
Epoch  41 Batch  101/769   train_loss = 1.754
Epoch  41 Batch  111/769   train_loss = 1.743
Epoch  41 Batch  121/769   train_loss = 1.822
Epoch  41 Batch  131/769   train_loss = 1.734
Epoch  41 Batch  141/769   train_loss = 1.784
Epoch  41 Batch  151/769   train_loss = 1.771
Epoch  41 Batch  161/769   train_loss = 1.833
Epoch  41 Batch  171/769   train_loss = 1.770
Epoch  41 Batch  181/769   train_loss = 1.816
Epoch  41 Batch  191/769   train_loss = 1.733
Epoch  41 Batch  201/769   train_loss = 1.789
Epoch  41 Batch  211/769   train_loss = 1.766
Epoch  41 Batch  221/769   train_loss = 1.708
Epoch  41 Batch  231/769   train_loss = 1.726
Epoch  41 Batch  241/769   train_loss = 1.829
Epoch  41 Batch  251/769   train_loss = 1.773
Epoch  41 Batch  261/769   train_loss = 1.689
Epoch  41 Batch  271/769   train_loss = 1.766
Epoch  41 Batch  281/769   train_loss = 1.810
Epoch  41 Batch  291/769   train_loss = 1.833
Epoch  41 Batch  301/769   train_loss = 1.817
Epoch  41 Batch  311/769   train_loss = 1.765
Epoch  41 Batch  321/769   train_loss = 1.791
Epoch  41 Batch  331/769   train_loss = 1.769
Epoch  41 Batch  341/769   train_loss = 1.673
Epoch  41 Batch  351/769   train_loss = 1.839
Epoch  41 Batch  361/769   train_loss = 1.673
Epoch  41 Batch  371/769   train_loss = 1.732
Epoch  41 Batch  381/769   train_loss = 1.742
Epoch  41 Batch  391/769   train_loss = 1.700
Epoch  41 Batch  401/769   train_loss = 1.768
Epoch  41 Batch  411/769   train_loss = 1.715
Epoch  41 Batch  421/769   train_loss = 1.780
Epoch  41 Batch  431/769   train_loss = 1.768
Epoch  41 Batch  441/769   train_loss = 1.763
Epoch  41 Batch  451/769   train_loss = 1.770
Epoch  41 Batch  461/769   train_loss = 1.757
Epoch  41 Batch  471/769   train_loss = 1.793
Epoch  41 Batch  481/769   train_loss = 1.791
Epoch  41 Batch  491/769   train_loss = 1.861
Epoch  41 Batch  501/769   train_loss = 1.868
Epoch  41 Batch  511/769   train_loss = 1.774
Epoch  41 Batch  521/769   train_loss = 1.780
Epoch  41 Batch  531/769   train_loss = 1.781
Epoch  41 Batch  541/769   train_loss = 1.748
Epoch  41 Batch  551/769   train_loss = 1.834
Epoch  41 Batch  561/769   train_loss = 1.816
Epoch  41 Batch  571/769   train_loss = 1.816
Epoch  41 Batch  581/769   train_loss = 1.681
Epoch  41 Batch  591/769   train_loss = 1.753
Epoch  41 Batch  601/769   train_loss = 1.764
Epoch  41 Batch  611/769   train_loss = 1.734
Epoch  41 Batch  621/769   train_loss = 1.771
Epoch  41 Batch  631/769   train_loss = 1.801
Epoch  41 Batch  641/769   train_loss = 1.816
Epoch  41 Batch  651/769   train_loss = 1.717
Epoch  41 Batch  661/769   train_loss = 1.738
Epoch  41 Batch  671/769   train_loss = 1.811
Epoch  41 Batch  681/769   train_loss = 1.737
Epoch  41 Batch  691/769   train_loss = 1.768
Epoch  41 Batch  701/769   train_loss = 1.734
Epoch  41 Batch  711/769   train_loss = 1.838
Epoch  41 Batch  721/769   train_loss = 1.856
Epoch  41 Batch  731/769   train_loss = 1.804
Epoch  41 Batch  741/769   train_loss = 1.706
Epoch  41 Batch  751/769   train_loss = 1.743
Epoch  41 Batch  761/769   train_loss = 1.687
Epoch  42 Batch    2/769   train_loss = 1.810
Epoch  42 Batch   12/769   train_loss = 1.744
Epoch  42 Batch   22/769   train_loss = 1.816
Epoch  42 Batch   32/769   train_loss = 1.876
Epoch  42 Batch   42/769   train_loss = 1.898
Epoch  42 Batch   52/769   train_loss = 1.772
Epoch  42 Batch   62/769   train_loss = 1.824
Epoch  42 Batch   72/769   train_loss = 1.878
Epoch  42 Batch   82/769   train_loss = 1.772
Epoch  42 Batch   92/769   train_loss = 1.828
Epoch  42 Batch  102/769   train_loss = 1.819
Epoch  42 Batch  112/769   train_loss = 1.716
Epoch  42 Batch  122/769   train_loss = 1.730
Epoch  42 Batch  132/769   train_loss = 1.702
Epoch  42 Batch  142/769   train_loss = 1.758
Epoch  42 Batch  152/769   train_loss = 1.732
Epoch  42 Batch  162/769   train_loss = 1.817
Epoch  42 Batch  172/769   train_loss = 1.834
Epoch  42 Batch  182/769   train_loss = 1.726
Epoch  42 Batch  192/769   train_loss = 1.723
Epoch  42 Batch  202/769   train_loss = 1.732
Epoch  42 Batch  212/769   train_loss = 1.688
Epoch  42 Batch  222/769   train_loss = 1.781
Epoch  42 Batch  232/769   train_loss = 1.738
Epoch  42 Batch  242/769   train_loss = 1.748
Epoch  42 Batch  252/769   train_loss = 1.746
Epoch  42 Batch  262/769   train_loss = 1.745
Epoch  42 Batch  272/769   train_loss = 1.741
Epoch  42 Batch  282/769   train_loss = 1.797
Epoch  42 Batch  292/769   train_loss = 1.706
Epoch  42 Batch  302/769   train_loss = 1.753
Epoch  42 Batch  312/769   train_loss = 1.741
Epoch  42 Batch  322/769   train_loss = 1.755
Epoch  42 Batch  332/769   train_loss = 1.754
Epoch  42 Batch  342/769   train_loss = 1.699
Epoch  42 Batch  352/769   train_loss = 1.705
Epoch  42 Batch  362/769   train_loss = 1.750
Epoch  42 Batch  372/769   train_loss = 1.740
Epoch  42 Batch  382/769   train_loss = 1.672
Epoch  42 Batch  392/769   train_loss = 1.736
Epoch  42 Batch  402/769   train_loss = 1.858
Epoch  42 Batch  412/769   train_loss = 1.756
Epoch  42 Batch  422/769   train_loss = 1.671
Epoch  42 Batch  432/769   train_loss = 1.735
Epoch  42 Batch  442/769   train_loss = 1.716
Epoch  42 Batch  452/769   train_loss = 1.755
Epoch  42 Batch  462/769   train_loss = 1.813
Epoch  42 Batch  472/769   train_loss = 1.728
Epoch  42 Batch  482/769   train_loss = 1.771
Epoch  42 Batch  492/769   train_loss = 1.799
Epoch  42 Batch  502/769   train_loss = 1.753
Epoch  42 Batch  512/769   train_loss = 1.789
Epoch  42 Batch  522/769   train_loss = 1.742
Epoch  42 Batch  532/769   train_loss = 1.759
Epoch  42 Batch  542/769   train_loss = 1.728
Epoch  42 Batch  552/769   train_loss = 1.774
Epoch  42 Batch  562/769   train_loss = 1.690
Epoch  42 Batch  572/769   train_loss = 1.742
Epoch  42 Batch  582/769   train_loss = 1.762
Epoch  42 Batch  592/769   train_loss = 1.726
Epoch  42 Batch  602/769   train_loss = 1.734
Epoch  42 Batch  612/769   train_loss = 1.757
Epoch  42 Batch  622/769   train_loss = 1.687
Epoch  42 Batch  632/769   train_loss = 1.699
Epoch  42 Batch  642/769   train_loss = 1.764
Epoch  42 Batch  652/769   train_loss = 1.790
Epoch  42 Batch  662/769   train_loss = 1.714
Epoch  42 Batch  672/769   train_loss = 1.734
Epoch  42 Batch  682/769   train_loss = 1.794
Epoch  42 Batch  692/769   train_loss = 1.728
Epoch  42 Batch  702/769   train_loss = 1.830
Epoch  42 Batch  712/769   train_loss = 1.786
Epoch  42 Batch  722/769   train_loss = 1.778
Epoch  42 Batch  732/769   train_loss = 1.700
Epoch  42 Batch  742/769   train_loss = 1.757
Epoch  42 Batch  752/769   train_loss = 1.794
Epoch  42 Batch  762/769   train_loss = 1.739
Epoch  43 Batch    3/769   train_loss = 1.816
Epoch  43 Batch   13/769   train_loss = 1.788
Epoch  43 Batch   23/769   train_loss = 1.784
Epoch  43 Batch   33/769   train_loss = 1.758
Epoch  43 Batch   43/769   train_loss = 1.804
Epoch  43 Batch   53/769   train_loss = 1.761
Epoch  43 Batch   63/769   train_loss = 1.640
Epoch  43 Batch   73/769   train_loss = 1.842
Epoch  43 Batch   83/769   train_loss = 1.699
Epoch  43 Batch   93/769   train_loss = 1.780
Epoch  43 Batch  103/769   train_loss = 1.713
Epoch  43 Batch  113/769   train_loss = 1.727
Epoch  43 Batch  123/769   train_loss = 1.781
Epoch  43 Batch  133/769   train_loss = 1.708
Epoch  43 Batch  143/769   train_loss = 1.739
Epoch  43 Batch  153/769   train_loss = 1.774
Epoch  43 Batch  163/769   train_loss = 1.775
Epoch  43 Batch  173/769   train_loss = 1.764
Epoch  43 Batch  183/769   train_loss = 1.779
Epoch  43 Batch  193/769   train_loss = 1.677
Epoch  43 Batch  203/769   train_loss = 1.671
Epoch  43 Batch  213/769   train_loss = 1.688
Epoch  43 Batch  223/769   train_loss = 1.773
Epoch  43 Batch  233/769   train_loss = 1.709
Epoch  43 Batch  243/769   train_loss = 1.740
Epoch  43 Batch  253/769   train_loss = 1.749
Epoch  43 Batch  263/769   train_loss = 1.756
Epoch  43 Batch  273/769   train_loss = 1.752
Epoch  43 Batch  283/769   train_loss = 1.787
Epoch  43 Batch  293/769   train_loss = 1.737
Epoch  43 Batch  303/769   train_loss = 1.757
Epoch  43 Batch  313/769   train_loss = 1.774
Epoch  43 Batch  323/769   train_loss = 1.795
Epoch  43 Batch  333/769   train_loss = 1.777
Epoch  43 Batch  343/769   train_loss = 1.741
Epoch  43 Batch  353/769   train_loss = 1.782
Epoch  43 Batch  363/769   train_loss = 1.699
Epoch  43 Batch  373/769   train_loss = 1.810
Epoch  43 Batch  383/769   train_loss = 1.712
Epoch  43 Batch  393/769   train_loss = 1.779
Epoch  43 Batch  403/769   train_loss = 1.800
Epoch  43 Batch  413/769   train_loss = 1.788
Epoch  43 Batch  423/769   train_loss = 1.702
Epoch  43 Batch  433/769   train_loss = 1.715
Epoch  43 Batch  443/769   train_loss = 1.736
Epoch  43 Batch  453/769   train_loss = 1.752
Epoch  43 Batch  463/769   train_loss = 1.741
Epoch  43 Batch  473/769   train_loss = 1.739
Epoch  43 Batch  483/769   train_loss = 1.749
Epoch  43 Batch  493/769   train_loss = 1.769
Epoch  43 Batch  503/769   train_loss = 1.812
Epoch  43 Batch  513/769   train_loss = 1.800
Epoch  43 Batch  523/769   train_loss = 1.686
Epoch  43 Batch  533/769   train_loss = 1.772
Epoch  43 Batch  543/769   train_loss = 1.691
Epoch  43 Batch  553/769   train_loss = 1.724
Epoch  43 Batch  563/769   train_loss = 1.713
Epoch  43 Batch  573/769   train_loss = 1.696
Epoch  43 Batch  583/769   train_loss = 1.716
Epoch  43 Batch  593/769   train_loss = 1.794
Epoch  43 Batch  603/769   train_loss = 1.660
Epoch  43 Batch  613/769   train_loss = 1.739
Epoch  43 Batch  623/769   train_loss = 1.675
Epoch  43 Batch  633/769   train_loss = 1.658
Epoch  43 Batch  643/769   train_loss = 1.604
Epoch  43 Batch  653/769   train_loss = 1.723
Epoch  43 Batch  663/769   train_loss = 1.692
Epoch  43 Batch  673/769   train_loss = 1.753
Epoch  43 Batch  683/769   train_loss = 1.736
Epoch  43 Batch  693/769   train_loss = 1.761
Epoch  43 Batch  703/769   train_loss = 1.684
Epoch  43 Batch  713/769   train_loss = 1.739
Epoch  43 Batch  723/769   train_loss = 1.687
Epoch  43 Batch  733/769   train_loss = 1.751
Epoch  43 Batch  743/769   train_loss = 1.759
Epoch  43 Batch  753/769   train_loss = 1.717
Epoch  43 Batch  763/769   train_loss = 1.754
Epoch  44 Batch    4/769   train_loss = 1.752
Epoch  44 Batch   14/769   train_loss = 1.760
Epoch  44 Batch   24/769   train_loss = 1.764
Epoch  44 Batch   34/769   train_loss = 1.731
Epoch  44 Batch   44/769   train_loss = 1.798
Epoch  44 Batch   54/769   train_loss = 1.690
Epoch  44 Batch   64/769   train_loss = 1.720
Epoch  44 Batch   74/769   train_loss = 1.786
Epoch  44 Batch   84/769   train_loss = 1.725
Epoch  44 Batch   94/769   train_loss = 1.694
Epoch  44 Batch  104/769   train_loss = 1.815
Epoch  44 Batch  114/769   train_loss = 1.748
Epoch  44 Batch  124/769   train_loss = 1.702
Epoch  44 Batch  134/769   train_loss = 1.691
Epoch  44 Batch  144/769   train_loss = 1.720
Epoch  44 Batch  154/769   train_loss = 1.731
Epoch  44 Batch  164/769   train_loss = 1.732
Epoch  44 Batch  174/769   train_loss = 1.721
Epoch  44 Batch  184/769   train_loss = 1.710
Epoch  44 Batch  194/769   train_loss = 1.600
Epoch  44 Batch  204/769   train_loss = 1.676
Epoch  44 Batch  214/769   train_loss = 1.702
Epoch  44 Batch  224/769   train_loss = 1.735
Epoch  44 Batch  234/769   train_loss = 1.752
Epoch  44 Batch  244/769   train_loss = 1.738
Epoch  44 Batch  254/769   train_loss = 1.633
Epoch  44 Batch  264/769   train_loss = 1.673
Epoch  44 Batch  274/769   train_loss = 1.646
Epoch  44 Batch  284/769   train_loss = 1.650
Epoch  44 Batch  294/769   train_loss = 1.673
Epoch  44 Batch  304/769   train_loss = 1.710
Epoch  44 Batch  314/769   train_loss = 1.729
Epoch  44 Batch  324/769   train_loss = 1.692
Epoch  44 Batch  334/769   train_loss = 1.718
Epoch  44 Batch  344/769   train_loss = 1.711
Epoch  44 Batch  354/769   train_loss = 1.834
Epoch  44 Batch  364/769   train_loss = 1.711
Epoch  44 Batch  374/769   train_loss = 1.726
Epoch  44 Batch  384/769   train_loss = 1.620
Epoch  44 Batch  394/769   train_loss = 1.780
Epoch  44 Batch  404/769   train_loss = 1.844
Epoch  44 Batch  414/769   train_loss = 1.725
Epoch  44 Batch  424/769   train_loss = 1.615
Epoch  44 Batch  434/769   train_loss = 1.686
Epoch  44 Batch  444/769   train_loss = 1.765
Epoch  44 Batch  454/769   train_loss = 1.723
Epoch  44 Batch  464/769   train_loss = 1.817
Epoch  44 Batch  474/769   train_loss = 1.737
Epoch  44 Batch  484/769   train_loss = 1.713
Epoch  44 Batch  494/769   train_loss = 1.687
Epoch  44 Batch  504/769   train_loss = 1.833
Epoch  44 Batch  514/769   train_loss = 1.757
Epoch  44 Batch  524/769   train_loss = 1.827
Epoch  44 Batch  534/769   train_loss = 1.705
Epoch  44 Batch  544/769   train_loss = 1.732
Epoch  44 Batch  554/769   train_loss = 1.694
Epoch  44 Batch  564/769   train_loss = 1.711
Epoch  44 Batch  574/769   train_loss = 1.713
Epoch  44 Batch  584/769   train_loss = 1.634
Epoch  44 Batch  594/769   train_loss = 1.668
Epoch  44 Batch  604/769   train_loss = 1.641
Epoch  44 Batch  614/769   train_loss = 1.675
Epoch  44 Batch  624/769   train_loss = 1.767
Epoch  44 Batch  634/769   train_loss = 1.682
Epoch  44 Batch  644/769   train_loss = 1.628
Epoch  44 Batch  654/769   train_loss = 1.740
Epoch  44 Batch  664/769   train_loss = 1.699
Epoch  44 Batch  674/769   train_loss = 1.664
Epoch  44 Batch  684/769   train_loss = 1.729
Epoch  44 Batch  694/769   train_loss = 1.734
Epoch  44 Batch  704/769   train_loss = 1.722
Epoch  44 Batch  714/769   train_loss = 1.680
Epoch  44 Batch  724/769   train_loss = 1.713
Epoch  44 Batch  734/769   train_loss = 1.852
Epoch  44 Batch  744/769   train_loss = 1.679
Epoch  44 Batch  754/769   train_loss = 1.741
Epoch  44 Batch  764/769   train_loss = 1.671
Epoch  45 Batch    5/769   train_loss = 1.788
Epoch  45 Batch   15/769   train_loss = 1.680
Epoch  45 Batch   25/769   train_loss = 1.763
Epoch  45 Batch   35/769   train_loss = 1.759
Epoch  45 Batch   45/769   train_loss = 1.740
Epoch  45 Batch   55/769   train_loss = 1.747
Epoch  45 Batch   65/769   train_loss = 1.721
Epoch  45 Batch   75/769   train_loss = 1.766
Epoch  45 Batch   85/769   train_loss = 1.794
Epoch  45 Batch   95/769   train_loss = 1.700
Epoch  45 Batch  105/769   train_loss = 1.785
Epoch  45 Batch  115/769   train_loss = 1.734
Epoch  45 Batch  125/769   train_loss = 1.569
Epoch  45 Batch  135/769   train_loss = 1.695
Epoch  45 Batch  145/769   train_loss = 1.710
Epoch  45 Batch  155/769   train_loss = 1.742
Epoch  45 Batch  165/769   train_loss = 1.699
Epoch  45 Batch  175/769   train_loss = 1.616
Epoch  45 Batch  185/769   train_loss = 1.678
Epoch  45 Batch  195/769   train_loss = 1.660
Epoch  45 Batch  205/769   train_loss = 1.745
Epoch  45 Batch  215/769   train_loss = 1.721
Epoch  45 Batch  225/769   train_loss = 1.630
Epoch  45 Batch  235/769   train_loss = 1.678
Epoch  45 Batch  245/769   train_loss = 1.701
Epoch  45 Batch  255/769   train_loss = 1.667
Epoch  45 Batch  265/769   train_loss = 1.600
Epoch  45 Batch  275/769   train_loss = 1.657
Epoch  45 Batch  285/769   train_loss = 1.767
Epoch  45 Batch  295/769   train_loss = 1.725
Epoch  45 Batch  305/769   train_loss = 1.715
Epoch  45 Batch  315/769   train_loss = 1.631
Epoch  45 Batch  325/769   train_loss = 1.671
Epoch  45 Batch  335/769   train_loss = 1.716
Epoch  45 Batch  345/769   train_loss = 1.635
Epoch  45 Batch  355/769   train_loss = 1.658
Epoch  45 Batch  365/769   train_loss = 1.692
Epoch  45 Batch  375/769   train_loss = 1.732
Epoch  45 Batch  385/769   train_loss = 1.570
Epoch  45 Batch  395/769   train_loss = 1.772
Epoch  45 Batch  405/769   train_loss = 1.703
Epoch  45 Batch  415/769   train_loss = 1.583
Epoch  45 Batch  425/769   train_loss = 1.606
Epoch  45 Batch  435/769   train_loss = 1.625
Epoch  45 Batch  445/769   train_loss = 1.723
Epoch  45 Batch  455/769   train_loss = 1.742
Epoch  45 Batch  465/769   train_loss = 1.804
Epoch  45 Batch  475/769   train_loss = 1.640
Epoch  45 Batch  485/769   train_loss = 1.725
Epoch  45 Batch  495/769   train_loss = 1.695
Epoch  45 Batch  505/769   train_loss = 1.730
Epoch  45 Batch  515/769   train_loss = 1.786
Epoch  45 Batch  525/769   train_loss = 1.698
Epoch  45 Batch  535/769   train_loss = 1.692
Epoch  45 Batch  545/769   train_loss = 1.739
Epoch  45 Batch  555/769   train_loss = 1.716
Epoch  45 Batch  565/769   train_loss = 1.800
Epoch  45 Batch  575/769   train_loss = 1.726
Epoch  45 Batch  585/769   train_loss = 1.674
Epoch  45 Batch  595/769   train_loss = 1.683
Epoch  45 Batch  605/769   train_loss = 1.636
Epoch  45 Batch  615/769   train_loss = 1.723
Epoch  45 Batch  625/769   train_loss = 1.686
Epoch  45 Batch  635/769   train_loss = 1.641
Epoch  45 Batch  645/769   train_loss = 1.614
Epoch  45 Batch  655/769   train_loss = 1.678
Epoch  45 Batch  665/769   train_loss = 1.678
Epoch  45 Batch  675/769   train_loss = 1.679
Epoch  45 Batch  685/769   train_loss = 1.707
Epoch  45 Batch  695/769   train_loss = 1.773
Epoch  45 Batch  705/769   train_loss = 1.689
Epoch  45 Batch  715/769   train_loss = 1.718
Epoch  45 Batch  725/769   train_loss = 1.657
Epoch  45 Batch  735/769   train_loss = 1.708
Epoch  45 Batch  745/769   train_loss = 1.736
Epoch  45 Batch  755/769   train_loss = 1.613
Epoch  45 Batch  765/769   train_loss = 1.695
Epoch  46 Batch    6/769   train_loss = 1.637
Epoch  46 Batch   16/769   train_loss = 1.673
Epoch  46 Batch   26/769   train_loss = 1.678
Epoch  46 Batch   36/769   train_loss = 1.758
Epoch  46 Batch   46/769   train_loss = 1.754
Epoch  46 Batch   56/769   train_loss = 1.680
Epoch  46 Batch   66/769   train_loss = 1.704
Epoch  46 Batch   76/769   train_loss = 1.749
Epoch  46 Batch   86/769   train_loss = 1.740
Epoch  46 Batch   96/769   train_loss = 1.639
Epoch  46 Batch  106/769   train_loss = 1.759
Epoch  46 Batch  116/769   train_loss = 1.661
Epoch  46 Batch  126/769   train_loss = 1.652
Epoch  46 Batch  136/769   train_loss = 1.618
Epoch  46 Batch  146/769   train_loss = 1.709
Epoch  46 Batch  156/769   train_loss = 1.683
Epoch  46 Batch  166/769   train_loss = 1.658
Epoch  46 Batch  176/769   train_loss = 1.623
Epoch  46 Batch  186/769   train_loss = 1.606
Epoch  46 Batch  196/769   train_loss = 1.637
Epoch  46 Batch  206/769   train_loss = 1.684
Epoch  46 Batch  216/769   train_loss = 1.702
Epoch  46 Batch  226/769   train_loss = 1.667
Epoch  46 Batch  236/769   train_loss = 1.710
Epoch  46 Batch  246/769   train_loss = 1.686
Epoch  46 Batch  256/769   train_loss = 1.630
Epoch  46 Batch  266/769   train_loss = 1.683
Epoch  46 Batch  276/769   train_loss = 1.658
Epoch  46 Batch  286/769   train_loss = 1.656
Epoch  46 Batch  296/769   train_loss = 1.657
Epoch  46 Batch  306/769   train_loss = 1.642
Epoch  46 Batch  316/769   train_loss = 1.636
Epoch  46 Batch  326/769   train_loss = 1.742
Epoch  46 Batch  336/769   train_loss = 1.698
Epoch  46 Batch  346/769   train_loss = 1.664
Epoch  46 Batch  356/769   train_loss = 1.658
Epoch  46 Batch  366/769   train_loss = 1.664
Epoch  46 Batch  376/769   train_loss = 1.634
Epoch  46 Batch  386/769   train_loss = 1.669
Epoch  46 Batch  396/769   train_loss = 1.732
Epoch  46 Batch  406/769   train_loss = 1.646
Epoch  46 Batch  416/769   train_loss = 1.671
Epoch  46 Batch  426/769   train_loss = 1.734
Epoch  46 Batch  436/769   train_loss = 1.652
Epoch  46 Batch  446/769   train_loss = 1.666
Epoch  46 Batch  456/769   train_loss = 1.749
Epoch  46 Batch  466/769   train_loss = 1.698
Epoch  46 Batch  476/769   train_loss = 1.692
Epoch  46 Batch  486/769   train_loss = 1.726
Epoch  46 Batch  496/769   train_loss = 1.658
Epoch  46 Batch  506/769   train_loss = 1.747
Epoch  46 Batch  516/769   train_loss = 1.651
Epoch  46 Batch  526/769   train_loss = 1.648
Epoch  46 Batch  536/769   train_loss = 1.702
Epoch  46 Batch  546/769   train_loss = 1.715
Epoch  46 Batch  556/769   train_loss = 1.749
Epoch  46 Batch  566/769   train_loss = 1.770
Epoch  46 Batch  576/769   train_loss = 1.631
Epoch  46 Batch  586/769   train_loss = 1.642
Epoch  46 Batch  596/769   train_loss = 1.747
Epoch  46 Batch  606/769   train_loss = 1.715
Epoch  46 Batch  616/769   train_loss = 1.652
Epoch  46 Batch  626/769   train_loss = 1.687
Epoch  46 Batch  636/769   train_loss = 1.642
Epoch  46 Batch  646/769   train_loss = 1.760
Epoch  46 Batch  656/769   train_loss = 1.664
Epoch  46 Batch  666/769   train_loss = 1.679
Epoch  46 Batch  676/769   train_loss = 1.688
Epoch  46 Batch  686/769   train_loss = 1.709
Epoch  46 Batch  696/769   train_loss = 1.685
Epoch  46 Batch  706/769   train_loss = 1.723
Epoch  46 Batch  716/769   train_loss = 1.673
Epoch  46 Batch  726/769   train_loss = 1.675
Epoch  46 Batch  736/769   train_loss = 1.727
Epoch  46 Batch  746/769   train_loss = 1.616
Epoch  46 Batch  756/769   train_loss = 1.624
Epoch  46 Batch  766/769   train_loss = 1.676
Epoch  47 Batch    7/769   train_loss = 1.718
Epoch  47 Batch   17/769   train_loss = 1.704
Epoch  47 Batch   27/769   train_loss = 1.667
Epoch  47 Batch   37/769   train_loss = 1.665
Epoch  47 Batch   47/769   train_loss = 1.660
Epoch  47 Batch   57/769   train_loss = 1.742
Epoch  47 Batch   67/769   train_loss = 1.681
Epoch  47 Batch   77/769   train_loss = 1.733
Epoch  47 Batch   87/769   train_loss = 1.698
Epoch  47 Batch   97/769   train_loss = 1.617
Epoch  47 Batch  107/769   train_loss = 1.703
Epoch  47 Batch  117/769   train_loss = 1.644
Epoch  47 Batch  127/769   train_loss = 1.716
Epoch  47 Batch  137/769   train_loss = 1.630
Epoch  47 Batch  147/769   train_loss = 1.658
Epoch  47 Batch  157/769   train_loss = 1.743
Epoch  47 Batch  167/769   train_loss = 1.658
Epoch  47 Batch  177/769   train_loss = 1.570
Epoch  47 Batch  187/769   train_loss = 1.709
Epoch  47 Batch  197/769   train_loss = 1.593
Epoch  47 Batch  207/769   train_loss = 1.576
Epoch  47 Batch  217/769   train_loss = 1.649
Epoch  47 Batch  227/769   train_loss = 1.609
Epoch  47 Batch  237/769   train_loss = 1.649
Epoch  47 Batch  247/769   train_loss = 1.656
Epoch  47 Batch  257/769   train_loss = 1.620
Epoch  47 Batch  267/769   train_loss = 1.639
Epoch  47 Batch  277/769   train_loss = 1.694
Epoch  47 Batch  287/769   train_loss = 1.637
Epoch  47 Batch  297/769   train_loss = 1.615
Epoch  47 Batch  307/769   train_loss = 1.706
Epoch  47 Batch  317/769   train_loss = 1.567
Epoch  47 Batch  327/769   train_loss = 1.671
Epoch  47 Batch  337/769   train_loss = 1.679
Epoch  47 Batch  347/769   train_loss = 1.631
Epoch  47 Batch  357/769   train_loss = 1.668
Epoch  47 Batch  367/769   train_loss = 1.628
Epoch  47 Batch  377/769   train_loss = 1.658
Epoch  47 Batch  387/769   train_loss = 1.706
Epoch  47 Batch  397/769   train_loss = 1.601
Epoch  47 Batch  407/769   train_loss = 1.684
Epoch  47 Batch  417/769   train_loss = 1.716
Epoch  47 Batch  427/769   train_loss = 1.687
Epoch  47 Batch  437/769   train_loss = 1.725
Epoch  47 Batch  447/769   train_loss = 1.730
Epoch  47 Batch  457/769   train_loss = 1.630
Epoch  47 Batch  467/769   train_loss = 1.637
Epoch  47 Batch  477/769   train_loss = 1.709
Epoch  47 Batch  487/769   train_loss = 1.670
Epoch  47 Batch  497/769   train_loss = 1.682
Epoch  47 Batch  507/769   train_loss = 1.676
Epoch  47 Batch  517/769   train_loss = 1.667
Epoch  47 Batch  527/769   train_loss = 1.651
Epoch  47 Batch  537/769   train_loss = 1.730
Epoch  47 Batch  547/769   train_loss = 1.772
Epoch  47 Batch  557/769   train_loss = 1.657
Epoch  47 Batch  567/769   train_loss = 1.662
Epoch  47 Batch  577/769   train_loss = 1.734
Epoch  47 Batch  587/769   train_loss = 1.641
Epoch  47 Batch  597/769   train_loss = 1.571
Epoch  47 Batch  607/769   train_loss = 1.660
Epoch  47 Batch  617/769   train_loss = 1.663
Epoch  47 Batch  627/769   train_loss = 1.727
Epoch  47 Batch  637/769   train_loss = 1.633
Epoch  47 Batch  647/769   train_loss = 1.724
Epoch  47 Batch  657/769   train_loss = 1.592
Epoch  47 Batch  667/769   train_loss = 1.705
Epoch  47 Batch  677/769   train_loss = 1.613
Epoch  47 Batch  687/769   train_loss = 1.730
Epoch  47 Batch  697/769   train_loss = 1.736
Epoch  47 Batch  707/769   train_loss = 1.640
Epoch  47 Batch  717/769   train_loss = 1.620
Epoch  47 Batch  727/769   train_loss = 1.693
Epoch  47 Batch  737/769   train_loss = 1.794
Epoch  47 Batch  747/769   train_loss = 1.636
Epoch  47 Batch  757/769   train_loss = 1.674
Epoch  47 Batch  767/769   train_loss = 1.675
Epoch  48 Batch    8/769   train_loss = 1.666
Epoch  48 Batch   18/769   train_loss = 1.692
Epoch  48 Batch   28/769   train_loss = 1.693
Epoch  48 Batch   38/769   train_loss = 1.712
Epoch  48 Batch   48/769   train_loss = 1.701
Epoch  48 Batch   58/769   train_loss = 1.656
Epoch  48 Batch   68/769   train_loss = 1.718
Epoch  48 Batch   78/769   train_loss = 1.687
Epoch  48 Batch   88/769   train_loss = 1.641
Epoch  48 Batch   98/769   train_loss = 1.597
Epoch  48 Batch  108/769   train_loss = 1.697
Epoch  48 Batch  118/769   train_loss = 1.605
Epoch  48 Batch  128/769   train_loss = 1.668
Epoch  48 Batch  138/769   train_loss = 1.598
Epoch  48 Batch  148/769   train_loss = 1.684
Epoch  48 Batch  158/769   train_loss = 1.679
Epoch  48 Batch  168/769   train_loss = 1.582
Epoch  48 Batch  178/769   train_loss = 1.683
Epoch  48 Batch  188/769   train_loss = 1.617
Epoch  48 Batch  198/769   train_loss = 1.633
Epoch  48 Batch  208/769   train_loss = 1.641
Epoch  48 Batch  218/769   train_loss = 1.679
Epoch  48 Batch  228/769   train_loss = 1.652
Epoch  48 Batch  238/769   train_loss = 1.691
Epoch  48 Batch  248/769   train_loss = 1.652
Epoch  48 Batch  258/769   train_loss = 1.613
Epoch  48 Batch  268/769   train_loss = 1.644
Epoch  48 Batch  278/769   train_loss = 1.697
Epoch  48 Batch  288/769   train_loss = 1.660
Epoch  48 Batch  298/769   train_loss = 1.692
Epoch  48 Batch  308/769   train_loss = 1.655
Epoch  48 Batch  318/769   train_loss = 1.595
Epoch  48 Batch  328/769   train_loss = 1.608
Epoch  48 Batch  338/769   train_loss = 1.656
Epoch  48 Batch  348/769   train_loss = 1.631
Epoch  48 Batch  358/769   train_loss = 1.670
Epoch  48 Batch  368/769   train_loss = 1.685
Epoch  48 Batch  378/769   train_loss = 1.651
Epoch  48 Batch  388/769   train_loss = 1.576
Epoch  48 Batch  398/769   train_loss = 1.712
Epoch  48 Batch  408/769   train_loss = 1.651
Epoch  48 Batch  418/769   train_loss = 1.652
Epoch  48 Batch  428/769   train_loss = 1.642
Epoch  48 Batch  438/769   train_loss = 1.559
Epoch  48 Batch  448/769   train_loss = 1.671
Epoch  48 Batch  458/769   train_loss = 1.739
Epoch  48 Batch  468/769   train_loss = 1.638
Epoch  48 Batch  478/769   train_loss = 1.560
Epoch  48 Batch  488/769   train_loss = 1.651
Epoch  48 Batch  498/769   train_loss = 1.636
Epoch  48 Batch  508/769   train_loss = 1.683
Epoch  48 Batch  518/769   train_loss = 1.675
Epoch  48 Batch  528/769   train_loss = 1.594
Epoch  48 Batch  538/769   train_loss = 1.697
Epoch  48 Batch  548/769   train_loss = 1.634
Epoch  48 Batch  558/769   train_loss = 1.648
Epoch  48 Batch  568/769   train_loss = 1.621
Epoch  48 Batch  578/769   train_loss = 1.503
Epoch  48 Batch  588/769   train_loss = 1.584
Epoch  48 Batch  598/769   train_loss = 1.620
Epoch  48 Batch  608/769   train_loss = 1.616
Epoch  48 Batch  618/769   train_loss = 1.580
Epoch  48 Batch  628/769   train_loss = 1.644
Epoch  48 Batch  638/769   train_loss = 1.588
Epoch  48 Batch  648/769   train_loss = 1.693
Epoch  48 Batch  658/769   train_loss = 1.743
Epoch  48 Batch  668/769   train_loss = 1.624
Epoch  48 Batch  678/769   train_loss = 1.638
Epoch  48 Batch  688/769   train_loss = 1.617
Epoch  48 Batch  698/769   train_loss = 1.637
Epoch  48 Batch  708/769   train_loss = 1.631
Epoch  48 Batch  718/769   train_loss = 1.604
Epoch  48 Batch  728/769   train_loss = 1.676
Epoch  48 Batch  738/769   train_loss = 1.682
Epoch  48 Batch  748/769   train_loss = 1.703
Epoch  48 Batch  758/769   train_loss = 1.654
Epoch  48 Batch  768/769   train_loss = 1.673
Epoch  49 Batch    9/769   train_loss = 1.698
Epoch  49 Batch   19/769   train_loss = 1.720
Epoch  49 Batch   29/769   train_loss = 1.639
Epoch  49 Batch   39/769   train_loss = 1.640
Epoch  49 Batch   49/769   train_loss = 1.651
Epoch  49 Batch   59/769   train_loss = 1.600
Epoch  49 Batch   69/769   train_loss = 1.688
Epoch  49 Batch   79/769   train_loss = 1.658
Epoch  49 Batch   89/769   train_loss = 1.538
Epoch  49 Batch   99/769   train_loss = 1.652
Epoch  49 Batch  109/769   train_loss = 1.629
Epoch  49 Batch  119/769   train_loss = 1.611
Epoch  49 Batch  129/769   train_loss = 1.554
Epoch  49 Batch  139/769   train_loss = 1.626
Epoch  49 Batch  149/769   train_loss = 1.608
Epoch  49 Batch  159/769   train_loss = 1.572
Epoch  49 Batch  169/769   train_loss = 1.664
Epoch  49 Batch  179/769   train_loss = 1.585
Epoch  49 Batch  189/769   train_loss = 1.608
Epoch  49 Batch  199/769   train_loss = 1.604
Epoch  49 Batch  209/769   train_loss = 1.626
Epoch  49 Batch  219/769   train_loss = 1.617
Epoch  49 Batch  229/769   train_loss = 1.666
Epoch  49 Batch  239/769   train_loss = 1.592
Epoch  49 Batch  249/769   train_loss = 1.583
Epoch  49 Batch  259/769   train_loss = 1.562
Epoch  49 Batch  269/769   train_loss = 1.614
Epoch  49 Batch  279/769   train_loss = 1.646
Epoch  49 Batch  289/769   train_loss = 1.568
Epoch  49 Batch  299/769   train_loss = 1.719
Epoch  49 Batch  309/769   train_loss = 1.660
Epoch  49 Batch  319/769   train_loss = 1.661
Epoch  49 Batch  329/769   train_loss = 1.672
Epoch  49 Batch  339/769   train_loss = 1.623
Epoch  49 Batch  349/769   train_loss = 1.583
Epoch  49 Batch  359/769   train_loss = 1.661
Epoch  49 Batch  369/769   train_loss = 1.581
Epoch  49 Batch  379/769   train_loss = 1.532
Epoch  49 Batch  389/769   train_loss = 1.502
Epoch  49 Batch  399/769   train_loss = 1.662
Epoch  49 Batch  409/769   train_loss = 1.706
Epoch  49 Batch  419/769   train_loss = 1.756
Epoch  49 Batch  429/769   train_loss = 1.610
Epoch  49 Batch  439/769   train_loss = 1.669
Epoch  49 Batch  449/769   train_loss = 1.669
Epoch  49 Batch  459/769   train_loss = 1.611
Epoch  49 Batch  469/769   train_loss = 1.573
Epoch  49 Batch  479/769   train_loss = 1.645
Epoch  49 Batch  489/769   train_loss = 1.711
Epoch  49 Batch  499/769   train_loss = 1.691
Epoch  49 Batch  509/769   train_loss = 1.605
Epoch  49 Batch  519/769   train_loss = 1.701
Epoch  49 Batch  529/769   train_loss = 1.694
Epoch  49 Batch  539/769   train_loss = 1.695
Epoch  49 Batch  549/769   train_loss = 1.678
Epoch  49 Batch  559/769   train_loss = 1.691
Epoch  49 Batch  569/769   train_loss = 1.576
Epoch  49 Batch  579/769   train_loss = 1.523
Epoch  49 Batch  589/769   train_loss = 1.650
Epoch  49 Batch  599/769   train_loss = 1.597
Epoch  49 Batch  609/769   train_loss = 1.639
Epoch  49 Batch  619/769   train_loss = 1.682
Epoch  49 Batch  629/769   train_loss = 1.614
Epoch  49 Batch  639/769   train_loss = 1.605
Epoch  49 Batch  649/769   train_loss = 1.582
Epoch  49 Batch  659/769   train_loss = 1.613
Epoch  49 Batch  669/769   train_loss = 1.609
Epoch  49 Batch  679/769   train_loss = 1.566
Epoch  49 Batch  689/769   train_loss = 1.728
Epoch  49 Batch  699/769   train_loss = 1.613
Epoch  49 Batch  709/769   train_loss = 1.659
Epoch  49 Batch  719/769   train_loss = 1.597
Epoch  49 Batch  729/769   train_loss = 1.606
Epoch  49 Batch  739/769   train_loss = 1.636
Epoch  49 Batch  749/769   train_loss = 1.570
Epoch  49 Batch  759/769   train_loss = 1.623
Epoch  50 Batch    0/769   train_loss = 1.531
Epoch  50 Batch   10/769   train_loss = 1.689
Epoch  50 Batch   20/769   train_loss = 1.611
Epoch  50 Batch   30/769   train_loss = 1.596
Epoch  50 Batch   40/769   train_loss = 1.581
Epoch  50 Batch   50/769   train_loss = 1.613
Epoch  50 Batch   60/769   train_loss = 1.725
Epoch  50 Batch   70/769   train_loss = 1.628
Epoch  50 Batch   80/769   train_loss = 1.607
Epoch  50 Batch   90/769   train_loss = 1.687
Epoch  50 Batch  100/769   train_loss = 1.618
Epoch  50 Batch  110/769   train_loss = 1.637
Epoch  50 Batch  120/769   train_loss = 1.552
Epoch  50 Batch  130/769   train_loss = 1.553
Epoch  50 Batch  140/769   train_loss = 1.671
Epoch  50 Batch  150/769   train_loss = 1.598
Epoch  50 Batch  160/769   train_loss = 1.600
Epoch  50 Batch  170/769   train_loss = 1.589
Epoch  50 Batch  180/769   train_loss = 1.595
Epoch  50 Batch  190/769   train_loss = 1.548
Epoch  50 Batch  200/769   train_loss = 1.669
Epoch  50 Batch  210/769   train_loss = 1.548
Epoch  50 Batch  220/769   train_loss = 1.518
Epoch  50 Batch  230/769   train_loss = 1.576
Epoch  50 Batch  240/769   train_loss = 1.576
Epoch  50 Batch  250/769   train_loss = 1.540
Epoch  50 Batch  260/769   train_loss = 1.638
Epoch  50 Batch  270/769   train_loss = 1.573
Epoch  50 Batch  280/769   train_loss = 1.630
Epoch  50 Batch  290/769   train_loss = 1.578
Epoch  50 Batch  300/769   train_loss = 1.682
Epoch  50 Batch  310/769   train_loss = 1.558
Epoch  50 Batch  320/769   train_loss = 1.523
Epoch  50 Batch  330/769   train_loss = 1.629
Epoch  50 Batch  340/769   train_loss = 1.567
Epoch  50 Batch  350/769   train_loss = 1.511
Epoch  50 Batch  360/769   train_loss = 1.562
Epoch  50 Batch  370/769   train_loss = 1.551
Epoch  50 Batch  380/769   train_loss = 1.609
Epoch  50 Batch  390/769   train_loss = 1.526
Epoch  50 Batch  400/769   train_loss = 1.640
Epoch  50 Batch  410/769   train_loss = 1.658
Epoch  50 Batch  420/769   train_loss = 1.623
Epoch  50 Batch  430/769   train_loss = 1.638
Epoch  50 Batch  440/769   train_loss = 1.636
Epoch  50 Batch  450/769   train_loss = 1.594
Epoch  50 Batch  460/769   train_loss = 1.736
Epoch  50 Batch  470/769   train_loss = 1.582
Epoch  50 Batch  480/769   train_loss = 1.614
Epoch  50 Batch  490/769   train_loss = 1.606
Epoch  50 Batch  500/769   train_loss = 1.650
Epoch  50 Batch  510/769   train_loss = 1.642
Epoch  50 Batch  520/769   train_loss = 1.539
Epoch  50 Batch  530/769   train_loss = 1.684
Epoch  50 Batch  540/769   train_loss = 1.709
Epoch  50 Batch  550/769   train_loss = 1.558
Epoch  50 Batch  560/769   train_loss = 1.662
Epoch  50 Batch  570/769   train_loss = 1.600
Epoch  50 Batch  580/769   train_loss = 1.569
Epoch  50 Batch  590/769   train_loss = 1.572
Epoch  50 Batch  600/769   train_loss = 1.584
Epoch  50 Batch  610/769   train_loss = 1.587
Epoch  50 Batch  620/769   train_loss = 1.647
Epoch  50 Batch  630/769   train_loss = 1.671
Epoch  50 Batch  640/769   train_loss = 1.545
Epoch  50 Batch  650/769   train_loss = 1.580
Epoch  50 Batch  660/769   train_loss = 1.632
Epoch  50 Batch  670/769   train_loss = 1.580
Epoch  50 Batch  680/769   train_loss = 1.641
Epoch  50 Batch  690/769   train_loss = 1.519
Epoch  50 Batch  700/769   train_loss = 1.643
Epoch  50 Batch  710/769   train_loss = 1.629
Epoch  50 Batch  720/769   train_loss = 1.534
Epoch  50 Batch  730/769   train_loss = 1.602
Epoch  50 Batch  740/769   train_loss = 1.625
Epoch  50 Batch  750/769   train_loss = 1.550
Epoch  50 Batch  760/769   train_loss = 1.646
Epoch  51 Batch    1/769   train_loss = 1.610
Epoch  51 Batch   11/769   train_loss = 1.568
Epoch  51 Batch   21/769   train_loss = 1.613
Epoch  51 Batch   31/769   train_loss = 1.566
Epoch  51 Batch   41/769   train_loss = 1.660
Epoch  51 Batch   51/769   train_loss = 1.629
Epoch  51 Batch   61/769   train_loss = 1.659
Epoch  51 Batch   71/769   train_loss = 1.589
Epoch  51 Batch   81/769   train_loss = 1.723
Epoch  51 Batch   91/769   train_loss = 1.609
Epoch  51 Batch  101/769   train_loss = 1.565
Epoch  51 Batch  111/769   train_loss = 1.568
Epoch  51 Batch  121/769   train_loss = 1.641
Epoch  51 Batch  131/769   train_loss = 1.539
Epoch  51 Batch  141/769   train_loss = 1.621
Epoch  51 Batch  151/769   train_loss = 1.603
Epoch  51 Batch  161/769   train_loss = 1.634
Epoch  51 Batch  171/769   train_loss = 1.603
Epoch  51 Batch  181/769   train_loss = 1.609
Epoch  51 Batch  191/769   train_loss = 1.537
Epoch  51 Batch  201/769   train_loss = 1.609
Epoch  51 Batch  211/769   train_loss = 1.584
Epoch  51 Batch  221/769   train_loss = 1.496
Epoch  51 Batch  231/769   train_loss = 1.552
Epoch  51 Batch  241/769   train_loss = 1.622
Epoch  51 Batch  251/769   train_loss = 1.595
Epoch  51 Batch  261/769   train_loss = 1.505
Epoch  51 Batch  271/769   train_loss = 1.571
Epoch  51 Batch  281/769   train_loss = 1.628
Epoch  51 Batch  291/769   train_loss = 1.650
Epoch  51 Batch  301/769   train_loss = 1.617
Epoch  51 Batch  311/769   train_loss = 1.606
Epoch  51 Batch  321/769   train_loss = 1.570
Epoch  51 Batch  331/769   train_loss = 1.580
Epoch  51 Batch  341/769   train_loss = 1.513
Epoch  51 Batch  351/769   train_loss = 1.670
Epoch  51 Batch  361/769   train_loss = 1.509
Epoch  51 Batch  371/769   train_loss = 1.585
Epoch  51 Batch  381/769   train_loss = 1.524
Epoch  51 Batch  391/769   train_loss = 1.532
Epoch  51 Batch  401/769   train_loss = 1.569
Epoch  51 Batch  411/769   train_loss = 1.553
Epoch  51 Batch  421/769   train_loss = 1.637
Epoch  51 Batch  431/769   train_loss = 1.602
Epoch  51 Batch  441/769   train_loss = 1.590
Epoch  51 Batch  451/769   train_loss = 1.557
Epoch  51 Batch  461/769   train_loss = 1.585
Epoch  51 Batch  471/769   train_loss = 1.592
Epoch  51 Batch  481/769   train_loss = 1.595
Epoch  51 Batch  491/769   train_loss = 1.680
Epoch  51 Batch  501/769   train_loss = 1.674
Epoch  51 Batch  511/769   train_loss = 1.597
Epoch  51 Batch  521/769   train_loss = 1.613
Epoch  51 Batch  531/769   train_loss = 1.589
Epoch  51 Batch  541/769   train_loss = 1.592
Epoch  51 Batch  551/769   train_loss = 1.659
Epoch  51 Batch  561/769   train_loss = 1.655
Epoch  51 Batch  571/769   train_loss = 1.627
Epoch  51 Batch  581/769   train_loss = 1.503
Epoch  51 Batch  591/769   train_loss = 1.577
Epoch  51 Batch  601/769   train_loss = 1.595
Epoch  51 Batch  611/769   train_loss = 1.579
Epoch  51 Batch  621/769   train_loss = 1.582
Epoch  51 Batch  631/769   train_loss = 1.635
Epoch  51 Batch  641/769   train_loss = 1.633
Epoch  51 Batch  651/769   train_loss = 1.545
Epoch  51 Batch  661/769   train_loss = 1.570
Epoch  51 Batch  671/769   train_loss = 1.617
Epoch  51 Batch  681/769   train_loss = 1.564
Epoch  51 Batch  691/769   train_loss = 1.610
Epoch  51 Batch  701/769   train_loss = 1.551
Epoch  51 Batch  711/769   train_loss = 1.639
Epoch  51 Batch  721/769   train_loss = 1.626
Epoch  51 Batch  731/769   train_loss = 1.600
Epoch  51 Batch  741/769   train_loss = 1.546
Epoch  51 Batch  751/769   train_loss = 1.566
Epoch  51 Batch  761/769   train_loss = 1.547
Epoch  52 Batch    2/769   train_loss = 1.649
Epoch  52 Batch   12/769   train_loss = 1.538
Epoch  52 Batch   22/769   train_loss = 1.646
Epoch  52 Batch   32/769   train_loss = 1.681
Epoch  52 Batch   42/769   train_loss = 1.710
Epoch  52 Batch   52/769   train_loss = 1.581
Epoch  52 Batch   62/769   train_loss = 1.675
Epoch  52 Batch   72/769   train_loss = 1.683
Epoch  52 Batch   82/769   train_loss = 1.597
Epoch  52 Batch   92/769   train_loss = 1.641
Epoch  52 Batch  102/769   train_loss = 1.632
Epoch  52 Batch  112/769   train_loss = 1.512
Epoch  52 Batch  122/769   train_loss = 1.550
Epoch  52 Batch  132/769   train_loss = 1.545
Epoch  52 Batch  142/769   train_loss = 1.579
Epoch  52 Batch  152/769   train_loss = 1.527
Epoch  52 Batch  162/769   train_loss = 1.653
Epoch  52 Batch  172/769   train_loss = 1.640
Epoch  52 Batch  182/769   train_loss = 1.546
Epoch  52 Batch  192/769   train_loss = 1.541
Epoch  52 Batch  202/769   train_loss = 1.546
Epoch  52 Batch  212/769   train_loss = 1.522
Epoch  52 Batch  222/769   train_loss = 1.584
Epoch  52 Batch  232/769   train_loss = 1.565
Epoch  52 Batch  242/769   train_loss = 1.571
Epoch  52 Batch  252/769   train_loss = 1.559
Epoch  52 Batch  262/769   train_loss = 1.533
Epoch  52 Batch  272/769   train_loss = 1.567
Epoch  52 Batch  282/769   train_loss = 1.606
Epoch  52 Batch  292/769   train_loss = 1.527
Epoch  52 Batch  302/769   train_loss = 1.586
Epoch  52 Batch  312/769   train_loss = 1.558
Epoch  52 Batch  322/769   train_loss = 1.564
Epoch  52 Batch  332/769   train_loss = 1.575
Epoch  52 Batch  342/769   train_loss = 1.548
Epoch  52 Batch  352/769   train_loss = 1.533
Epoch  52 Batch  362/769   train_loss = 1.576
Epoch  52 Batch  372/769   train_loss = 1.589
Epoch  52 Batch  382/769   train_loss = 1.489
Epoch  52 Batch  392/769   train_loss = 1.569
Epoch  52 Batch  402/769   train_loss = 1.670
Epoch  52 Batch  412/769   train_loss = 1.589
Epoch  52 Batch  422/769   train_loss = 1.536
Epoch  52 Batch  432/769   train_loss = 1.554
Epoch  52 Batch  442/769   train_loss = 1.506
Epoch  52 Batch  452/769   train_loss = 1.563
Epoch  52 Batch  462/769   train_loss = 1.607
Epoch  52 Batch  472/769   train_loss = 1.553
Epoch  52 Batch  482/769   train_loss = 1.573
Epoch  52 Batch  492/769   train_loss = 1.628
Epoch  52 Batch  502/769   train_loss = 1.595
Epoch  52 Batch  512/769   train_loss = 1.608
Epoch  52 Batch  522/769   train_loss = 1.578
Epoch  52 Batch  532/769   train_loss = 1.589
Epoch  52 Batch  542/769   train_loss = 1.568
Epoch  52 Batch  552/769   train_loss = 1.607
Epoch  52 Batch  562/769   train_loss = 1.513
Epoch  52 Batch  572/769   train_loss = 1.568
Epoch  52 Batch  582/769   train_loss = 1.608
Epoch  52 Batch  592/769   train_loss = 1.524
Epoch  52 Batch  602/769   train_loss = 1.574
Epoch  52 Batch  612/769   train_loss = 1.577
Epoch  52 Batch  622/769   train_loss = 1.505
Epoch  52 Batch  632/769   train_loss = 1.561
Epoch  52 Batch  642/769   train_loss = 1.619
Epoch  52 Batch  652/769   train_loss = 1.626
Epoch  52 Batch  662/769   train_loss = 1.573
Epoch  52 Batch  672/769   train_loss = 1.542
Epoch  52 Batch  682/769   train_loss = 1.612
Epoch  52 Batch  692/769   train_loss = 1.568
Epoch  52 Batch  702/769   train_loss = 1.649
Epoch  52 Batch  712/769   train_loss = 1.611
Epoch  52 Batch  722/769   train_loss = 1.588
Epoch  52 Batch  732/769   train_loss = 1.522
Epoch  52 Batch  742/769   train_loss = 1.608
Epoch  52 Batch  752/769   train_loss = 1.618
Epoch  52 Batch  762/769   train_loss = 1.564
Epoch  53 Batch    3/769   train_loss = 1.653
Epoch  53 Batch   13/769   train_loss = 1.639
Epoch  53 Batch   23/769   train_loss = 1.619
Epoch  53 Batch   33/769   train_loss = 1.566
Epoch  53 Batch   43/769   train_loss = 1.620
Epoch  53 Batch   53/769   train_loss = 1.602
Epoch  53 Batch   63/769   train_loss = 1.480
Epoch  53 Batch   73/769   train_loss = 1.665
Epoch  53 Batch   83/769   train_loss = 1.530
Epoch  53 Batch   93/769   train_loss = 1.588
Epoch  53 Batch  103/769   train_loss = 1.538
Epoch  53 Batch  113/769   train_loss = 1.534
Epoch  53 Batch  123/769   train_loss = 1.617
Epoch  53 Batch  133/769   train_loss = 1.553
Epoch  53 Batch  143/769   train_loss = 1.565
Epoch  53 Batch  153/769   train_loss = 1.601
Epoch  53 Batch  163/769   train_loss = 1.596
Epoch  53 Batch  173/769   train_loss = 1.574
Epoch  53 Batch  183/769   train_loss = 1.623
Epoch  53 Batch  193/769   train_loss = 1.519
Epoch  53 Batch  203/769   train_loss = 1.492
Epoch  53 Batch  213/769   train_loss = 1.510
Epoch  53 Batch  223/769   train_loss = 1.554
Epoch  53 Batch  233/769   train_loss = 1.525
Epoch  53 Batch  243/769   train_loss = 1.573
Epoch  53 Batch  253/769   train_loss = 1.561
Epoch  53 Batch  263/769   train_loss = 1.609
Epoch  53 Batch  273/769   train_loss = 1.566
Epoch  53 Batch  283/769   train_loss = 1.615
Epoch  53 Batch  293/769   train_loss = 1.540
Epoch  53 Batch  303/769   train_loss = 1.588
Epoch  53 Batch  313/769   train_loss = 1.601
Epoch  53 Batch  323/769   train_loss = 1.597
Epoch  53 Batch  333/769   train_loss = 1.611
Epoch  53 Batch  343/769   train_loss = 1.589
Epoch  53 Batch  353/769   train_loss = 1.606
Epoch  53 Batch  363/769   train_loss = 1.553
Epoch  53 Batch  373/769   train_loss = 1.651
Epoch  53 Batch  383/769   train_loss = 1.550
Epoch  53 Batch  393/769   train_loss = 1.595
Epoch  53 Batch  403/769   train_loss = 1.644
Epoch  53 Batch  413/769   train_loss = 1.626
Epoch  53 Batch  423/769   train_loss = 1.544
Epoch  53 Batch  433/769   train_loss = 1.554
Epoch  53 Batch  443/769   train_loss = 1.553
Epoch  53 Batch  453/769   train_loss = 1.562
Epoch  53 Batch  463/769   train_loss = 1.548
Epoch  53 Batch  473/769   train_loss = 1.543
Epoch  53 Batch  483/769   train_loss = 1.563
Epoch  53 Batch  493/769   train_loss = 1.564
Epoch  53 Batch  503/769   train_loss = 1.650
Epoch  53 Batch  513/769   train_loss = 1.644
Epoch  53 Batch  523/769   train_loss = 1.510
Epoch  53 Batch  533/769   train_loss = 1.563
Epoch  53 Batch  543/769   train_loss = 1.526
Epoch  53 Batch  553/769   train_loss = 1.546
Epoch  53 Batch  563/769   train_loss = 1.550
Epoch  53 Batch  573/769   train_loss = 1.539
Epoch  53 Batch  583/769   train_loss = 1.536
Epoch  53 Batch  593/769   train_loss = 1.614
Epoch  53 Batch  603/769   train_loss = 1.508
Epoch  53 Batch  613/769   train_loss = 1.546
Epoch  53 Batch  623/769   train_loss = 1.546
Epoch  53 Batch  633/769   train_loss = 1.483
Epoch  53 Batch  643/769   train_loss = 1.458
Epoch  53 Batch  653/769   train_loss = 1.546
Epoch  53 Batch  663/769   train_loss = 1.508
Epoch  53 Batch  673/769   train_loss = 1.586
Epoch  53 Batch  683/769   train_loss = 1.551
Epoch  53 Batch  693/769   train_loss = 1.601
Epoch  53 Batch  703/769   train_loss = 1.528
Epoch  53 Batch  713/769   train_loss = 1.571
Epoch  53 Batch  723/769   train_loss = 1.494
Epoch  53 Batch  733/769   train_loss = 1.558
Epoch  53 Batch  743/769   train_loss = 1.599
Epoch  53 Batch  753/769   train_loss = 1.563
Epoch  53 Batch  763/769   train_loss = 1.588
Epoch  54 Batch    4/769   train_loss = 1.587
Epoch  54 Batch   14/769   train_loss = 1.601
Epoch  54 Batch   24/769   train_loss = 1.627
Epoch  54 Batch   34/769   train_loss = 1.547
Epoch  54 Batch   44/769   train_loss = 1.588
Epoch  54 Batch   54/769   train_loss = 1.520
Epoch  54 Batch   64/769   train_loss = 1.555
Epoch  54 Batch   74/769   train_loss = 1.616
Epoch  54 Batch   84/769   train_loss = 1.544
Epoch  54 Batch   94/769   train_loss = 1.544
Epoch  54 Batch  104/769   train_loss = 1.614
Epoch  54 Batch  114/769   train_loss = 1.537
Epoch  54 Batch  124/769   train_loss = 1.546
Epoch  54 Batch  134/769   train_loss = 1.520
Epoch  54 Batch  144/769   train_loss = 1.554
Epoch  54 Batch  154/769   train_loss = 1.559
Epoch  54 Batch  164/769   train_loss = 1.563
Epoch  54 Batch  174/769   train_loss = 1.551
Epoch  54 Batch  184/769   train_loss = 1.571
Epoch  54 Batch  194/769   train_loss = 1.441
Epoch  54 Batch  204/769   train_loss = 1.524
Epoch  54 Batch  214/769   train_loss = 1.538
Epoch  54 Batch  224/769   train_loss = 1.548
Epoch  54 Batch  234/769   train_loss = 1.558
Epoch  54 Batch  244/769   train_loss = 1.612
Epoch  54 Batch  254/769   train_loss = 1.485
Epoch  54 Batch  264/769   train_loss = 1.496
Epoch  54 Batch  274/769   train_loss = 1.483
Epoch  54 Batch  284/769   train_loss = 1.478
Epoch  54 Batch  294/769   train_loss = 1.497
Epoch  54 Batch  304/769   train_loss = 1.523
Epoch  54 Batch  314/769   train_loss = 1.512
Epoch  54 Batch  324/769   train_loss = 1.516
Epoch  54 Batch  334/769   train_loss = 1.551
Epoch  54 Batch  344/769   train_loss = 1.553
Epoch  54 Batch  354/769   train_loss = 1.613
Epoch  54 Batch  364/769   train_loss = 1.535
Epoch  54 Batch  374/769   train_loss = 1.565
Epoch  54 Batch  384/769   train_loss = 1.436
Epoch  54 Batch  394/769   train_loss = 1.602
Epoch  54 Batch  404/769   train_loss = 1.634
Epoch  54 Batch  414/769   train_loss = 1.551
Epoch  54 Batch  424/769   train_loss = 1.470
Epoch  54 Batch  434/769   train_loss = 1.556
Epoch  54 Batch  444/769   train_loss = 1.586
Epoch  54 Batch  454/769   train_loss = 1.540
Epoch  54 Batch  464/769   train_loss = 1.652
Epoch  54 Batch  474/769   train_loss = 1.562
Epoch  54 Batch  484/769   train_loss = 1.520
Epoch  54 Batch  494/769   train_loss = 1.530
Epoch  54 Batch  504/769   train_loss = 1.671
Epoch  54 Batch  514/769   train_loss = 1.578
Epoch  54 Batch  524/769   train_loss = 1.666
Epoch  54 Batch  534/769   train_loss = 1.533
Epoch  54 Batch  544/769   train_loss = 1.532
Epoch  54 Batch  554/769   train_loss = 1.539
Epoch  54 Batch  564/769   train_loss = 1.537
Epoch  54 Batch  574/769   train_loss = 1.538
Epoch  54 Batch  584/769   train_loss = 1.441
Epoch  54 Batch  594/769   train_loss = 1.523
Epoch  54 Batch  604/769   train_loss = 1.508
Epoch  54 Batch  614/769   train_loss = 1.532
Epoch  54 Batch  624/769   train_loss = 1.606
Epoch  54 Batch  634/769   train_loss = 1.521
Epoch  54 Batch  644/769   train_loss = 1.448
Epoch  54 Batch  654/769   train_loss = 1.581
Epoch  54 Batch  664/769   train_loss = 1.551
Epoch  54 Batch  674/769   train_loss = 1.527
Epoch  54 Batch  684/769   train_loss = 1.544
Epoch  54 Batch  694/769   train_loss = 1.596
Epoch  54 Batch  704/769   train_loss = 1.572
Epoch  54 Batch  714/769   train_loss = 1.518
Epoch  54 Batch  724/769   train_loss = 1.512
Epoch  54 Batch  734/769   train_loss = 1.647
Epoch  54 Batch  744/769   train_loss = 1.534
Epoch  54 Batch  754/769   train_loss = 1.569
Epoch  54 Batch  764/769   train_loss = 1.533
Epoch  55 Batch    5/769   train_loss = 1.640
Epoch  55 Batch   15/769   train_loss = 1.526
Epoch  55 Batch   25/769   train_loss = 1.621
Epoch  55 Batch   35/769   train_loss = 1.614
Epoch  55 Batch   45/769   train_loss = 1.544
Epoch  55 Batch   55/769   train_loss = 1.603
Epoch  55 Batch   65/769   train_loss = 1.548
Epoch  55 Batch   75/769   train_loss = 1.597
Epoch  55 Batch   85/769   train_loss = 1.605
Epoch  55 Batch   95/769   train_loss = 1.539
Epoch  55 Batch  105/769   train_loss = 1.593
Epoch  55 Batch  115/769   train_loss = 1.562
Epoch  55 Batch  125/769   train_loss = 1.402
Epoch  55 Batch  135/769   train_loss = 1.571
Epoch  55 Batch  145/769   train_loss = 1.547
Epoch  55 Batch  155/769   train_loss = 1.561
Epoch  55 Batch  165/769   train_loss = 1.519
Epoch  55 Batch  175/769   train_loss = 1.447
Epoch  55 Batch  185/769   train_loss = 1.531
Epoch  55 Batch  195/769   train_loss = 1.519
Epoch  55 Batch  205/769   train_loss = 1.589
Epoch  55 Batch  215/769   train_loss = 1.531
Epoch  55 Batch  225/769   train_loss = 1.450
Epoch  55 Batch  235/769   train_loss = 1.525
Epoch  55 Batch  245/769   train_loss = 1.547
Epoch  55 Batch  255/769   train_loss = 1.517
Epoch  55 Batch  265/769   train_loss = 1.421
Epoch  55 Batch  275/769   train_loss = 1.513
Epoch  55 Batch  285/769   train_loss = 1.613
Epoch  55 Batch  295/769   train_loss = 1.547
Epoch  55 Batch  305/769   train_loss = 1.546
Epoch  55 Batch  315/769   train_loss = 1.473
Epoch  55 Batch  325/769   train_loss = 1.493
Epoch  55 Batch  335/769   train_loss = 1.573
Epoch  55 Batch  345/769   train_loss = 1.487
Epoch  55 Batch  355/769   train_loss = 1.490
Epoch  55 Batch  365/769   train_loss = 1.528
Epoch  55 Batch  375/769   train_loss = 1.612
Epoch  55 Batch  385/769   train_loss = 1.425
Epoch  55 Batch  395/769   train_loss = 1.594
Epoch  55 Batch  405/769   train_loss = 1.540
Epoch  55 Batch  415/769   train_loss = 1.431
Epoch  55 Batch  425/769   train_loss = 1.477
Epoch  55 Batch  435/769   train_loss = 1.460
Epoch  55 Batch  445/769   train_loss = 1.529
Epoch  55 Batch  455/769   train_loss = 1.573
Epoch  55 Batch  465/769   train_loss = 1.598
Epoch  55 Batch  475/769   train_loss = 1.452
Epoch  55 Batch  485/769   train_loss = 1.544
Epoch  55 Batch  495/769   train_loss = 1.550
Epoch  55 Batch  505/769   train_loss = 1.564
Epoch  55 Batch  515/769   train_loss = 1.611
Epoch  55 Batch  525/769   train_loss = 1.525
Epoch  55 Batch  535/769   train_loss = 1.531
Epoch  55 Batch  545/769   train_loss = 1.569
Epoch  55 Batch  555/769   train_loss = 1.578
Epoch  55 Batch  565/769   train_loss = 1.597
Epoch  55 Batch  575/769   train_loss = 1.538
Epoch  55 Batch  585/769   train_loss = 1.504
Epoch  55 Batch  595/769   train_loss = 1.472
Epoch  55 Batch  605/769   train_loss = 1.502
Epoch  55 Batch  615/769   train_loss = 1.555
Epoch  55 Batch  625/769   train_loss = 1.535
Epoch  55 Batch  635/769   train_loss = 1.479
Epoch  55 Batch  645/769   train_loss = 1.493
Epoch  55 Batch  655/769   train_loss = 1.547
Epoch  55 Batch  665/769   train_loss = 1.519
Epoch  55 Batch  675/769   train_loss = 1.504
Epoch  55 Batch  685/769   train_loss = 1.547
Epoch  55 Batch  695/769   train_loss = 1.602
Epoch  55 Batch  705/769   train_loss = 1.573
Epoch  55 Batch  715/769   train_loss = 1.558
Epoch  55 Batch  725/769   train_loss = 1.480
Epoch  55 Batch  735/769   train_loss = 1.504
Epoch  55 Batch  745/769   train_loss = 1.561
Epoch  55 Batch  755/769   train_loss = 1.484
Epoch  55 Batch  765/769   train_loss = 1.544
Epoch  56 Batch    6/769   train_loss = 1.504
Epoch  56 Batch   16/769   train_loss = 1.512
Epoch  56 Batch   26/769   train_loss = 1.521
Epoch  56 Batch   36/769   train_loss = 1.622
Epoch  56 Batch   46/769   train_loss = 1.596
Epoch  56 Batch   56/769   train_loss = 1.549
Epoch  56 Batch   66/769   train_loss = 1.575
Epoch  56 Batch   76/769   train_loss = 1.605
Epoch  56 Batch   86/769   train_loss = 1.559
Epoch  56 Batch   96/769   train_loss = 1.474
Epoch  56 Batch  106/769   train_loss = 1.590
Epoch  56 Batch  116/769   train_loss = 1.513
Epoch  56 Batch  126/769   train_loss = 1.479
Epoch  56 Batch  136/769   train_loss = 1.472
Epoch  56 Batch  146/769   train_loss = 1.534
Epoch  56 Batch  156/769   train_loss = 1.546
Epoch  56 Batch  166/769   train_loss = 1.485
Epoch  56 Batch  176/769   train_loss = 1.467
Epoch  56 Batch  186/769   train_loss = 1.478
Epoch  56 Batch  196/769   train_loss = 1.485
Epoch  56 Batch  206/769   train_loss = 1.493
Epoch  56 Batch  216/769   train_loss = 1.517
Epoch  56 Batch  226/769   train_loss = 1.503
Epoch  56 Batch  236/769   train_loss = 1.537
Epoch  56 Batch  246/769   train_loss = 1.526
Epoch  56 Batch  256/769   train_loss = 1.519
Epoch  56 Batch  266/769   train_loss = 1.524
Epoch  56 Batch  276/769   train_loss = 1.496
Epoch  56 Batch  286/769   train_loss = 1.504
Epoch  56 Batch  296/769   train_loss = 1.487
Epoch  56 Batch  306/769   train_loss = 1.493
Epoch  56 Batch  316/769   train_loss = 1.473
Epoch  56 Batch  326/769   train_loss = 1.575
Epoch  56 Batch  336/769   train_loss = 1.546
Epoch  56 Batch  346/769   train_loss = 1.526
Epoch  56 Batch  356/769   train_loss = 1.517
Epoch  56 Batch  366/769   train_loss = 1.510
Epoch  56 Batch  376/769   train_loss = 1.471
Epoch  56 Batch  386/769   train_loss = 1.517
Epoch  56 Batch  396/769   train_loss = 1.583
Epoch  56 Batch  406/769   train_loss = 1.478
Epoch  56 Batch  416/769   train_loss = 1.527
Epoch  56 Batch  426/769   train_loss = 1.597
Epoch  56 Batch  436/769   train_loss = 1.520
Epoch  56 Batch  446/769   train_loss = 1.540
Epoch  56 Batch  456/769   train_loss = 1.561
Epoch  56 Batch  466/769   train_loss = 1.540
Epoch  56 Batch  476/769   train_loss = 1.520
Epoch  56 Batch  486/769   train_loss = 1.539
Epoch  56 Batch  496/769   train_loss = 1.489
Epoch  56 Batch  506/769   train_loss = 1.598
Epoch  56 Batch  516/769   train_loss = 1.488
Epoch  56 Batch  526/769   train_loss = 1.471
Epoch  56 Batch  536/769   train_loss = 1.497
Epoch  56 Batch  546/769   train_loss = 1.527
Epoch  56 Batch  556/769   train_loss = 1.576
Epoch  56 Batch  566/769   train_loss = 1.616
Epoch  56 Batch  576/769   train_loss = 1.464
Epoch  56 Batch  586/769   train_loss = 1.474
Epoch  56 Batch  596/769   train_loss = 1.561
Epoch  56 Batch  606/769   train_loss = 1.542
Epoch  56 Batch  616/769   train_loss = 1.494
Epoch  56 Batch  626/769   train_loss = 1.537
Epoch  56 Batch  636/769   train_loss = 1.483
Epoch  56 Batch  646/769   train_loss = 1.596
Epoch  56 Batch  656/769   train_loss = 1.531
Epoch  56 Batch  666/769   train_loss = 1.551
Epoch  56 Batch  676/769   train_loss = 1.537
Epoch  56 Batch  686/769   train_loss = 1.536
Epoch  56 Batch  696/769   train_loss = 1.512
Epoch  56 Batch  706/769   train_loss = 1.554
Epoch  56 Batch  716/769   train_loss = 1.518
Epoch  56 Batch  726/769   train_loss = 1.502
Epoch  56 Batch  736/769   train_loss = 1.557
Epoch  56 Batch  746/769   train_loss = 1.465
Epoch  56 Batch  756/769   train_loss = 1.490
Epoch  56 Batch  766/769   train_loss = 1.515
Epoch  57 Batch    7/769   train_loss = 1.557
Epoch  57 Batch   17/769   train_loss = 1.571
Epoch  57 Batch   27/769   train_loss = 1.520
Epoch  57 Batch   37/769   train_loss = 1.497
Epoch  57 Batch   47/769   train_loss = 1.487
Epoch  57 Batch   57/769   train_loss = 1.587
Epoch  57 Batch   67/769   train_loss = 1.560
Epoch  57 Batch   77/769   train_loss = 1.562
Epoch  57 Batch   87/769   train_loss = 1.544
Epoch  57 Batch   97/769   train_loss = 1.467
Epoch  57 Batch  107/769   train_loss = 1.539
Epoch  57 Batch  117/769   train_loss = 1.465
Epoch  57 Batch  127/769   train_loss = 1.573
Epoch  57 Batch  137/769   train_loss = 1.501
Epoch  57 Batch  147/769   train_loss = 1.502
Epoch  57 Batch  157/769   train_loss = 1.594
Epoch  57 Batch  167/769   train_loss = 1.494
Epoch  57 Batch  177/769   train_loss = 1.454
Epoch  57 Batch  187/769   train_loss = 1.561
Epoch  57 Batch  197/769   train_loss = 1.457
Epoch  57 Batch  207/769   train_loss = 1.412
Epoch  57 Batch  217/769   train_loss = 1.489
Epoch  57 Batch  227/769   train_loss = 1.455
Epoch  57 Batch  237/769   train_loss = 1.504
Epoch  57 Batch  247/769   train_loss = 1.494
Epoch  57 Batch  257/769   train_loss = 1.471
Epoch  57 Batch  267/769   train_loss = 1.476
Epoch  57 Batch  277/769   train_loss = 1.514
Epoch  57 Batch  287/769   train_loss = 1.492
Epoch  57 Batch  297/769   train_loss = 1.490
Epoch  57 Batch  307/769   train_loss = 1.551
Epoch  57 Batch  317/769   train_loss = 1.408
Epoch  57 Batch  327/769   train_loss = 1.490
Epoch  57 Batch  337/769   train_loss = 1.506
Epoch  57 Batch  347/769   train_loss = 1.481
Epoch  57 Batch  357/769   train_loss = 1.487
Epoch  57 Batch  367/769   train_loss = 1.448
Epoch  57 Batch  377/769   train_loss = 1.518
Epoch  57 Batch  387/769   train_loss = 1.558
Epoch  57 Batch  397/769   train_loss = 1.429
Epoch  57 Batch  407/769   train_loss = 1.528
Epoch  57 Batch  417/769   train_loss = 1.543
Epoch  57 Batch  427/769   train_loss = 1.562
Epoch  57 Batch  437/769   train_loss = 1.568
Epoch  57 Batch  447/769   train_loss = 1.592
Epoch  57 Batch  457/769   train_loss = 1.489
Epoch  57 Batch  467/769   train_loss = 1.505
Epoch  57 Batch  477/769   train_loss = 1.534
Epoch  57 Batch  487/769   train_loss = 1.489
Epoch  57 Batch  497/769   train_loss = 1.534
Epoch  57 Batch  507/769   train_loss = 1.500
Epoch  57 Batch  517/769   train_loss = 1.533
Epoch  57 Batch  527/769   train_loss = 1.476
Epoch  57 Batch  537/769   train_loss = 1.552
Epoch  57 Batch  547/769   train_loss = 1.600
Epoch  57 Batch  557/769   train_loss = 1.502
Epoch  57 Batch  567/769   train_loss = 1.508
Epoch  57 Batch  577/769   train_loss = 1.545
Epoch  57 Batch  587/769   train_loss = 1.476
Epoch  57 Batch  597/769   train_loss = 1.393
Epoch  57 Batch  607/769   train_loss = 1.491
Epoch  57 Batch  617/769   train_loss = 1.497
Epoch  57 Batch  627/769   train_loss = 1.573
Epoch  57 Batch  637/769   train_loss = 1.459
Epoch  57 Batch  647/769   train_loss = 1.563
Epoch  57 Batch  657/769   train_loss = 1.432
Epoch  57 Batch  667/769   train_loss = 1.535
Epoch  57 Batch  677/769   train_loss = 1.460
Epoch  57 Batch  687/769   train_loss = 1.578
Epoch  57 Batch  697/769   train_loss = 1.586
Epoch  57 Batch  707/769   train_loss = 1.517
Epoch  57 Batch  717/769   train_loss = 1.475
Epoch  57 Batch  727/769   train_loss = 1.537
Epoch  57 Batch  737/769   train_loss = 1.608
Epoch  57 Batch  747/769   train_loss = 1.507
Epoch  57 Batch  757/769   train_loss = 1.517
Epoch  57 Batch  767/769   train_loss = 1.516
Epoch  58 Batch    8/769   train_loss = 1.519
Epoch  58 Batch   18/769   train_loss = 1.529
Epoch  58 Batch   28/769   train_loss = 1.513
Epoch  58 Batch   38/769   train_loss = 1.558
Epoch  58 Batch   48/769   train_loss = 1.549
Epoch  58 Batch   58/769   train_loss = 1.515
Epoch  58 Batch   68/769   train_loss = 1.602
Epoch  58 Batch   78/769   train_loss = 1.529
Epoch  58 Batch   88/769   train_loss = 1.500
Epoch  58 Batch   98/769   train_loss = 1.469
Epoch  58 Batch  108/769   train_loss = 1.567
Epoch  58 Batch  118/769   train_loss = 1.459
Epoch  58 Batch  128/769   train_loss = 1.555
Epoch  58 Batch  138/769   train_loss = 1.463
Epoch  58 Batch  148/769   train_loss = 1.525
Epoch  58 Batch  158/769   train_loss = 1.546
Epoch  58 Batch  168/769   train_loss = 1.426
Epoch  58 Batch  178/769   train_loss = 1.565
Epoch  58 Batch  188/769   train_loss = 1.498
Epoch  58 Batch  198/769   train_loss = 1.512
Epoch  58 Batch  208/769   train_loss = 1.494
Epoch  58 Batch  218/769   train_loss = 1.520
Epoch  58 Batch  228/769   train_loss = 1.483
Epoch  58 Batch  238/769   train_loss = 1.518
Epoch  58 Batch  248/769   train_loss = 1.526
Epoch  58 Batch  258/769   train_loss = 1.502
Epoch  58 Batch  268/769   train_loss = 1.494
Epoch  58 Batch  278/769   train_loss = 1.528
Epoch  58 Batch  288/769   train_loss = 1.508
Epoch  58 Batch  298/769   train_loss = 1.587
Epoch  58 Batch  308/769   train_loss = 1.521
Epoch  58 Batch  318/769   train_loss = 1.427
Epoch  58 Batch  328/769   train_loss = 1.478
Epoch  58 Batch  338/769   train_loss = 1.494
Epoch  58 Batch  348/769   train_loss = 1.457
Epoch  58 Batch  358/769   train_loss = 1.514
Epoch  58 Batch  368/769   train_loss = 1.515
Epoch  58 Batch  378/769   train_loss = 1.502
Epoch  58 Batch  388/769   train_loss = 1.437
Epoch  58 Batch  398/769   train_loss = 1.550
Epoch  58 Batch  408/769   train_loss = 1.507
Epoch  58 Batch  418/769   train_loss = 1.513
Epoch  58 Batch  428/769   train_loss = 1.504
Epoch  58 Batch  438/769   train_loss = 1.431
Epoch  58 Batch  448/769   train_loss = 1.513
Epoch  58 Batch  458/769   train_loss = 1.592
Epoch  58 Batch  468/769   train_loss = 1.485
Epoch  58 Batch  478/769   train_loss = 1.411
Epoch  58 Batch  488/769   train_loss = 1.483
Epoch  58 Batch  498/769   train_loss = 1.482
Epoch  58 Batch  508/769   train_loss = 1.487
Epoch  58 Batch  518/769   train_loss = 1.533
Epoch  58 Batch  528/769   train_loss = 1.451
Epoch  58 Batch  538/769   train_loss = 1.525
Epoch  58 Batch  548/769   train_loss = 1.462
Epoch  58 Batch  558/769   train_loss = 1.525
Epoch  58 Batch  568/769   train_loss = 1.500
Epoch  58 Batch  578/769   train_loss = 1.362
Epoch  58 Batch  588/769   train_loss = 1.415
Epoch  58 Batch  598/769   train_loss = 1.466
Epoch  58 Batch  608/769   train_loss = 1.420
Epoch  58 Batch  618/769   train_loss = 1.433
Epoch  58 Batch  628/769   train_loss = 1.495
Epoch  58 Batch  638/769   train_loss = 1.430
Epoch  58 Batch  648/769   train_loss = 1.537
Epoch  58 Batch  658/769   train_loss = 1.594
Epoch  58 Batch  668/769   train_loss = 1.472
Epoch  58 Batch  678/769   train_loss = 1.475
Epoch  58 Batch  688/769   train_loss = 1.480
Epoch  58 Batch  698/769   train_loss = 1.498
Epoch  58 Batch  708/769   train_loss = 1.500
Epoch  58 Batch  718/769   train_loss = 1.474
Epoch  58 Batch  728/769   train_loss = 1.523
Epoch  58 Batch  738/769   train_loss = 1.508
Epoch  58 Batch  748/769   train_loss = 1.543
Epoch  58 Batch  758/769   train_loss = 1.500
Epoch  58 Batch  768/769   train_loss = 1.495
Epoch  59 Batch    9/769   train_loss = 1.513
Epoch  59 Batch   19/769   train_loss = 1.551
Epoch  59 Batch   29/769   train_loss = 1.523
Epoch  59 Batch   39/769   train_loss = 1.489
Epoch  59 Batch   49/769   train_loss = 1.510
Epoch  59 Batch   59/769   train_loss = 1.484
Epoch  59 Batch   69/769   train_loss = 1.543
Epoch  59 Batch   79/769   train_loss = 1.509
Epoch  59 Batch   89/769   train_loss = 1.432
Epoch  59 Batch   99/769   train_loss = 1.536
Epoch  59 Batch  109/769   train_loss = 1.503
Epoch  59 Batch  119/769   train_loss = 1.443
Epoch  59 Batch  129/769   train_loss = 1.427
Epoch  59 Batch  139/769   train_loss = 1.499
Epoch  59 Batch  149/769   train_loss = 1.482
Epoch  59 Batch  159/769   train_loss = 1.444
Epoch  59 Batch  169/769   train_loss = 1.524
Epoch  59 Batch  179/769   train_loss = 1.462
Epoch  59 Batch  189/769   train_loss = 1.496
Epoch  59 Batch  199/769   train_loss = 1.489
Epoch  59 Batch  209/769   train_loss = 1.470
Epoch  59 Batch  219/769   train_loss = 1.462
Epoch  59 Batch  229/769   train_loss = 1.516
Epoch  59 Batch  239/769   train_loss = 1.458
Epoch  59 Batch  249/769   train_loss = 1.419
Epoch  59 Batch  259/769   train_loss = 1.451
Epoch  59 Batch  269/769   train_loss = 1.441
Epoch  59 Batch  279/769   train_loss = 1.488
Epoch  59 Batch  289/769   train_loss = 1.419
Epoch  59 Batch  299/769   train_loss = 1.569
Epoch  59 Batch  309/769   train_loss = 1.508
Epoch  59 Batch  319/769   train_loss = 1.528
Epoch  59 Batch  329/769   train_loss = 1.502
Epoch  59 Batch  339/769   train_loss = 1.448
Epoch  59 Batch  349/769   train_loss = 1.451
Epoch  59 Batch  359/769   train_loss = 1.526
Epoch  59 Batch  369/769   train_loss = 1.425
Epoch  59 Batch  379/769   train_loss = 1.400
Epoch  59 Batch  389/769   train_loss = 1.379
Epoch  59 Batch  399/769   train_loss = 1.519
Epoch  59 Batch  409/769   train_loss = 1.553
Epoch  59 Batch  419/769   train_loss = 1.617
Epoch  59 Batch  429/769   train_loss = 1.464
Epoch  59 Batch  439/769   train_loss = 1.517
Epoch  59 Batch  449/769   train_loss = 1.541
Epoch  59 Batch  459/769   train_loss = 1.471
Epoch  59 Batch  469/769   train_loss = 1.416
Epoch  59 Batch  479/769   train_loss = 1.492
Epoch  59 Batch  489/769   train_loss = 1.563
Epoch  59 Batch  499/769   train_loss = 1.537
Epoch  59 Batch  509/769   train_loss = 1.435
Epoch  59 Batch  519/769   train_loss = 1.561
Epoch  59 Batch  529/769   train_loss = 1.548
Epoch  59 Batch  539/769   train_loss = 1.524
Epoch  59 Batch  549/769   train_loss = 1.537
Epoch  59 Batch  559/769   train_loss = 1.530
Epoch  59 Batch  569/769   train_loss = 1.440
Epoch  59 Batch  579/769   train_loss = 1.389
Epoch  59 Batch  589/769   train_loss = 1.501
Epoch  59 Batch  599/769   train_loss = 1.449
Epoch  59 Batch  609/769   train_loss = 1.497
Epoch  59 Batch  619/769   train_loss = 1.519
Epoch  59 Batch  629/769   train_loss = 1.481
Epoch  59 Batch  639/769   train_loss = 1.432
Epoch  59 Batch  649/769   train_loss = 1.451
Epoch  59 Batch  659/769   train_loss = 1.480
Epoch  59 Batch  669/769   train_loss = 1.465
Epoch  59 Batch  679/769   train_loss = 1.434
Epoch  59 Batch  689/769   train_loss = 1.546
Epoch  59 Batch  699/769   train_loss = 1.498
Epoch  59 Batch  709/769   train_loss = 1.535
Epoch  59 Batch  719/769   train_loss = 1.481
Epoch  59 Batch  729/769   train_loss = 1.481
Epoch  59 Batch  739/769   train_loss = 1.447
Epoch  59 Batch  749/769   train_loss = 1.452
Epoch  59 Batch  759/769   train_loss = 1.498
Epoch  60 Batch    0/769   train_loss = 1.413
Epoch  60 Batch   10/769   train_loss = 1.537
Epoch  60 Batch   20/769   train_loss = 1.478
Epoch  60 Batch   30/769   train_loss = 1.490
Epoch  60 Batch   40/769   train_loss = 1.458
Epoch  60 Batch   50/769   train_loss = 1.473
Epoch  60 Batch   60/769   train_loss = 1.578
Epoch  60 Batch   70/769   train_loss = 1.504
Epoch  60 Batch   80/769   train_loss = 1.457
Epoch  60 Batch   90/769   train_loss = 1.524
Epoch  60 Batch  100/769   train_loss = 1.505
Epoch  60 Batch  110/769   train_loss = 1.465
Epoch  60 Batch  120/769   train_loss = 1.399
Epoch  60 Batch  130/769   train_loss = 1.426
Epoch  60 Batch  140/769   train_loss = 1.536
Epoch  60 Batch  150/769   train_loss = 1.458
Epoch  60 Batch  160/769   train_loss = 1.467
Epoch  60 Batch  170/769   train_loss = 1.450
Epoch  60 Batch  180/769   train_loss = 1.462
Epoch  60 Batch  190/769   train_loss = 1.424
Epoch  60 Batch  200/769   train_loss = 1.538
Epoch  60 Batch  210/769   train_loss = 1.433
Epoch  60 Batch  220/769   train_loss = 1.386
Epoch  60 Batch  230/769   train_loss = 1.458
Epoch  60 Batch  240/769   train_loss = 1.418
Epoch  60 Batch  250/769   train_loss = 1.424
Epoch  60 Batch  260/769   train_loss = 1.502
Epoch  60 Batch  270/769   train_loss = 1.443
Epoch  60 Batch  280/769   train_loss = 1.487
Epoch  60 Batch  290/769   train_loss = 1.447
Epoch  60 Batch  300/769   train_loss = 1.527
Epoch  60 Batch  310/769   train_loss = 1.433
Epoch  60 Batch  320/769   train_loss = 1.406
Epoch  60 Batch  330/769   train_loss = 1.482
Epoch  60 Batch  340/769   train_loss = 1.423
Epoch  60 Batch  350/769   train_loss = 1.386
Epoch  60 Batch  360/769   train_loss = 1.383
Epoch  60 Batch  370/769   train_loss = 1.409
Epoch  60 Batch  380/769   train_loss = 1.461
Epoch  60 Batch  390/769   train_loss = 1.408
Epoch  60 Batch  400/769   train_loss = 1.451
Epoch  60 Batch  410/769   train_loss = 1.522
Epoch  60 Batch  420/769   train_loss = 1.479
Epoch  60 Batch  430/769   train_loss = 1.482
Epoch  60 Batch  440/769   train_loss = 1.513
Epoch  60 Batch  450/769   train_loss = 1.461
Epoch  60 Batch  460/769   train_loss = 1.579
Epoch  60 Batch  470/769   train_loss = 1.466
Epoch  60 Batch  480/769   train_loss = 1.457
Epoch  60 Batch  490/769   train_loss = 1.425
Epoch  60 Batch  500/769   train_loss = 1.508
Epoch  60 Batch  510/769   train_loss = 1.518
Epoch  60 Batch  520/769   train_loss = 1.382
Epoch  60 Batch  530/769   train_loss = 1.506
Epoch  60 Batch  540/769   train_loss = 1.562
Epoch  60 Batch  550/769   train_loss = 1.394
Epoch  60 Batch  560/769   train_loss = 1.512
Epoch  60 Batch  570/769   train_loss = 1.469
Epoch  60 Batch  580/769   train_loss = 1.424
Epoch  60 Batch  590/769   train_loss = 1.426
Epoch  60 Batch  600/769   train_loss = 1.444
Epoch  60 Batch  610/769   train_loss = 1.444
Epoch  60 Batch  620/769   train_loss = 1.493
Epoch  60 Batch  630/769   train_loss = 1.512
Epoch  60 Batch  640/769   train_loss = 1.397
Epoch  60 Batch  650/769   train_loss = 1.443
Epoch  60 Batch  660/769   train_loss = 1.485
Epoch  60 Batch  670/769   train_loss = 1.438
Epoch  60 Batch  680/769   train_loss = 1.496
Epoch  60 Batch  690/769   train_loss = 1.384
Epoch  60 Batch  700/769   train_loss = 1.500
Epoch  60 Batch  710/769   train_loss = 1.472
Epoch  60 Batch  720/769   train_loss = 1.410
Epoch  60 Batch  730/769   train_loss = 1.462
Epoch  60 Batch  740/769   train_loss = 1.481
Epoch  60 Batch  750/769   train_loss = 1.438
Epoch  60 Batch  760/769   train_loss = 1.530
Epoch  61 Batch    1/769   train_loss = 1.469
Epoch  61 Batch   11/769   train_loss = 1.448
Epoch  61 Batch   21/769   train_loss = 1.473
Epoch  61 Batch   31/769   train_loss = 1.450
Epoch  61 Batch   41/769   train_loss = 1.504
Epoch  61 Batch   51/769   train_loss = 1.500
Epoch  61 Batch   61/769   train_loss = 1.522
Epoch  61 Batch   71/769   train_loss = 1.475
Epoch  61 Batch   81/769   train_loss = 1.570
Epoch  61 Batch   91/769   train_loss = 1.482
Epoch  61 Batch  101/769   train_loss = 1.451
Epoch  61 Batch  111/769   train_loss = 1.442
Epoch  61 Batch  121/769   train_loss = 1.512
Epoch  61 Batch  131/769   train_loss = 1.416
Epoch  61 Batch  141/769   train_loss = 1.510
Epoch  61 Batch  151/769   train_loss = 1.448
Epoch  61 Batch  161/769   train_loss = 1.505
Epoch  61 Batch  171/769   train_loss = 1.494
Epoch  61 Batch  181/769   train_loss = 1.487
Epoch  61 Batch  191/769   train_loss = 1.419
Epoch  61 Batch  201/769   train_loss = 1.471
Epoch  61 Batch  211/769   train_loss = 1.428
Epoch  61 Batch  221/769   train_loss = 1.371
Epoch  61 Batch  231/769   train_loss = 1.444
Epoch  61 Batch  241/769   train_loss = 1.482
Epoch  61 Batch  251/769   train_loss = 1.496
Epoch  61 Batch  261/769   train_loss = 1.372
Epoch  61 Batch  271/769   train_loss = 1.423
Epoch  61 Batch  281/769   train_loss = 1.476
Epoch  61 Batch  291/769   train_loss = 1.495
Epoch  61 Batch  301/769   train_loss = 1.468
Epoch  61 Batch  311/769   train_loss = 1.470
Epoch  61 Batch  321/769   train_loss = 1.415
Epoch  61 Batch  331/769   train_loss = 1.420
Epoch  61 Batch  341/769   train_loss = 1.394
Epoch  61 Batch  351/769   train_loss = 1.539
Epoch  61 Batch  361/769   train_loss = 1.369
Epoch  61 Batch  371/769   train_loss = 1.412
Epoch  61 Batch  381/769   train_loss = 1.392
Epoch  61 Batch  391/769   train_loss = 1.410
Epoch  61 Batch  401/769   train_loss = 1.417
Epoch  61 Batch  411/769   train_loss = 1.440
Epoch  61 Batch  421/769   train_loss = 1.509
Epoch  61 Batch  431/769   train_loss = 1.476
Epoch  61 Batch  441/769   train_loss = 1.452
Epoch  61 Batch  451/769   train_loss = 1.428
Epoch  61 Batch  461/769   train_loss = 1.447
Epoch  61 Batch  471/769   train_loss = 1.465
Epoch  61 Batch  481/769   train_loss = 1.472
Epoch  61 Batch  491/769   train_loss = 1.516
Epoch  61 Batch  501/769   train_loss = 1.523
Epoch  61 Batch  511/769   train_loss = 1.457
Epoch  61 Batch  521/769   train_loss = 1.468
Epoch  61 Batch  531/769   train_loss = 1.434
Epoch  61 Batch  541/769   train_loss = 1.459
Epoch  61 Batch  551/769   train_loss = 1.512
Epoch  61 Batch  561/769   train_loss = 1.502
Epoch  61 Batch  571/769   train_loss = 1.488
Epoch  61 Batch  581/769   train_loss = 1.377
Epoch  61 Batch  591/769   train_loss = 1.438
Epoch  61 Batch  601/769   train_loss = 1.472
Epoch  61 Batch  611/769   train_loss = 1.403
Epoch  61 Batch  621/769   train_loss = 1.456
Epoch  61 Batch  631/769   train_loss = 1.495
Epoch  61 Batch  641/769   train_loss = 1.474
Epoch  61 Batch  651/769   train_loss = 1.405
Epoch  61 Batch  661/769   train_loss = 1.436
Epoch  61 Batch  671/769   train_loss = 1.482
Epoch  61 Batch  681/769   train_loss = 1.414
Epoch  61 Batch  691/769   train_loss = 1.456
Epoch  61 Batch  701/769   train_loss = 1.421
Epoch  61 Batch  711/769   train_loss = 1.519
Epoch  61 Batch  721/769   train_loss = 1.511
Epoch  61 Batch  731/769   train_loss = 1.455
Epoch  61 Batch  741/769   train_loss = 1.423
Epoch  61 Batch  751/769   train_loss = 1.451
Epoch  61 Batch  761/769   train_loss = 1.416
Epoch  62 Batch    2/769   train_loss = 1.500
Epoch  62 Batch   12/769   train_loss = 1.400
Epoch  62 Batch   22/769   train_loss = 1.515
Epoch  62 Batch   32/769   train_loss = 1.563
Epoch  62 Batch   42/769   train_loss = 1.563
Epoch  62 Batch   52/769   train_loss = 1.431
Epoch  62 Batch   62/769   train_loss = 1.533
Epoch  62 Batch   72/769   train_loss = 1.520
Epoch  62 Batch   82/769   train_loss = 1.481
Epoch  62 Batch   92/769   train_loss = 1.524
Epoch  62 Batch  102/769   train_loss = 1.511
Epoch  62 Batch  112/769   train_loss = 1.419
Epoch  62 Batch  122/769   train_loss = 1.403
Epoch  62 Batch  132/769   train_loss = 1.392
Epoch  62 Batch  142/769   train_loss = 1.455
Epoch  62 Batch  152/769   train_loss = 1.392
Epoch  62 Batch  162/769   train_loss = 1.504
Epoch  62 Batch  172/769   train_loss = 1.505
Epoch  62 Batch  182/769   train_loss = 1.417
Epoch  62 Batch  192/769   train_loss = 1.425
Epoch  62 Batch  202/769   train_loss = 1.444
Epoch  62 Batch  212/769   train_loss = 1.389
Epoch  62 Batch  222/769   train_loss = 1.478
Epoch  62 Batch  232/769   train_loss = 1.449
Epoch  62 Batch  242/769   train_loss = 1.435
Epoch  62 Batch  252/769   train_loss = 1.437
Epoch  62 Batch  262/769   train_loss = 1.449
Epoch  62 Batch  272/769   train_loss = 1.450
Epoch  62 Batch  282/769   train_loss = 1.457
Epoch  62 Batch  292/769   train_loss = 1.426
Epoch  62 Batch  302/769   train_loss = 1.488
Epoch  62 Batch  312/769   train_loss = 1.462
Epoch  62 Batch  322/769   train_loss = 1.442
Epoch  62 Batch  332/769   train_loss = 1.426
Epoch  62 Batch  342/769   train_loss = 1.410
Epoch  62 Batch  352/769   train_loss = 1.423
Epoch  62 Batch  362/769   train_loss = 1.417
Epoch  62 Batch  372/769   train_loss = 1.455
Epoch  62 Batch  382/769   train_loss = 1.361
Epoch  62 Batch  392/769   train_loss = 1.449
Epoch  62 Batch  402/769   train_loss = 1.512
Epoch  62 Batch  412/769   train_loss = 1.455
Epoch  62 Batch  422/769   train_loss = 1.373
Epoch  62 Batch  432/769   train_loss = 1.412
Epoch  62 Batch  442/769   train_loss = 1.385
Epoch  62 Batch  452/769   train_loss = 1.437
Epoch  62 Batch  462/769   train_loss = 1.489
Epoch  62 Batch  472/769   train_loss = 1.425
Epoch  62 Batch  482/769   train_loss = 1.428
Epoch  62 Batch  492/769   train_loss = 1.489
Epoch  62 Batch  502/769   train_loss = 1.453
Epoch  62 Batch  512/769   train_loss = 1.458
Epoch  62 Batch  522/769   train_loss = 1.448
Epoch  62 Batch  532/769   train_loss = 1.458
Epoch  62 Batch  542/769   train_loss = 1.405
Epoch  62 Batch  552/769   train_loss = 1.434
Epoch  62 Batch  562/769   train_loss = 1.368
Epoch  62 Batch  572/769   train_loss = 1.433
Epoch  62 Batch  582/769   train_loss = 1.476
Epoch  62 Batch  592/769   train_loss = 1.373
Epoch  62 Batch  602/769   train_loss = 1.413
Epoch  62 Batch  612/769   train_loss = 1.425
Epoch  62 Batch  622/769   train_loss = 1.379
Epoch  62 Batch  632/769   train_loss = 1.429
Epoch  62 Batch  642/769   train_loss = 1.463
Epoch  62 Batch  652/769   train_loss = 1.457
Epoch  62 Batch  662/769   train_loss = 1.472
Epoch  62 Batch  672/769   train_loss = 1.412
Epoch  62 Batch  682/769   train_loss = 1.479
Epoch  62 Batch  692/769   train_loss = 1.429
Epoch  62 Batch  702/769   train_loss = 1.515
Epoch  62 Batch  712/769   train_loss = 1.477
Epoch  62 Batch  722/769   train_loss = 1.444
Epoch  62 Batch  732/769   train_loss = 1.408
Epoch  62 Batch  742/769   train_loss = 1.467
Epoch  62 Batch  752/769   train_loss = 1.514
Epoch  62 Batch  762/769   train_loss = 1.440
Epoch  63 Batch    3/769   train_loss = 1.511
Epoch  63 Batch   13/769   train_loss = 1.475
Epoch  63 Batch   23/769   train_loss = 1.468
Epoch  63 Batch   33/769   train_loss = 1.444
Epoch  63 Batch   43/769   train_loss = 1.500
Epoch  63 Batch   53/769   train_loss = 1.456
Epoch  63 Batch   63/769   train_loss = 1.347
Epoch  63 Batch   73/769   train_loss = 1.544
Epoch  63 Batch   83/769   train_loss = 1.406
Epoch  63 Batch   93/769   train_loss = 1.461
Epoch  63 Batch  103/769   train_loss = 1.454
Epoch  63 Batch  113/769   train_loss = 1.419
Epoch  63 Batch  123/769   train_loss = 1.480
Epoch  63 Batch  133/769   train_loss = 1.416
Epoch  63 Batch  143/769   train_loss = 1.447
Epoch  63 Batch  153/769   train_loss = 1.469
Epoch  63 Batch  163/769   train_loss = 1.445
Epoch  63 Batch  173/769   train_loss = 1.464
Epoch  63 Batch  183/769   train_loss = 1.524
Epoch  63 Batch  193/769   train_loss = 1.404
Epoch  63 Batch  203/769   train_loss = 1.393
Epoch  63 Batch  213/769   train_loss = 1.398
Epoch  63 Batch  223/769   train_loss = 1.450
Epoch  63 Batch  233/769   train_loss = 1.382
Epoch  63 Batch  243/769   train_loss = 1.452
Epoch  63 Batch  253/769   train_loss = 1.433
Epoch  63 Batch  263/769   train_loss = 1.501
Epoch  63 Batch  273/769   train_loss = 1.465
Epoch  63 Batch  283/769   train_loss = 1.493
Epoch  63 Batch  293/769   train_loss = 1.422
Epoch  63 Batch  303/769   train_loss = 1.475
Epoch  63 Batch  313/769   train_loss = 1.530
Epoch  63 Batch  323/769   train_loss = 1.461
Epoch  63 Batch  333/769   train_loss = 1.443
Epoch  63 Batch  343/769   train_loss = 1.442
Epoch  63 Batch  353/769   train_loss = 1.469
Epoch  63 Batch  363/769   train_loss = 1.434
Epoch  63 Batch  373/769   train_loss = 1.513
Epoch  63 Batch  383/769   train_loss = 1.434
Epoch  63 Batch  393/769   train_loss = 1.513
Epoch  63 Batch  403/769   train_loss = 1.499
Epoch  63 Batch  413/769   train_loss = 1.491
Epoch  63 Batch  423/769   train_loss = 1.425
Epoch  63 Batch  433/769   train_loss = 1.428
Epoch  63 Batch  443/769   train_loss = 1.435
Epoch  63 Batch  453/769   train_loss = 1.446
Epoch  63 Batch  463/769   train_loss = 1.394
Epoch  63 Batch  473/769   train_loss = 1.416
Epoch  63 Batch  483/769   train_loss = 1.436
Epoch  63 Batch  493/769   train_loss = 1.422
Epoch  63 Batch  503/769   train_loss = 1.508
Epoch  63 Batch  513/769   train_loss = 1.480
Epoch  63 Batch  523/769   train_loss = 1.380
Epoch  63 Batch  533/769   train_loss = 1.438
Epoch  63 Batch  543/769   train_loss = 1.415
Epoch  63 Batch  553/769   train_loss = 1.420
Epoch  63 Batch  563/769   train_loss = 1.437
Epoch  63 Batch  573/769   train_loss = 1.403
Epoch  63 Batch  583/769   train_loss = 1.426
Epoch  63 Batch  593/769   train_loss = 1.468
Epoch  63 Batch  603/769   train_loss = 1.377
Epoch  63 Batch  613/769   train_loss = 1.381
Epoch  63 Batch  623/769   train_loss = 1.402
Epoch  63 Batch  633/769   train_loss = 1.373
Epoch  63 Batch  643/769   train_loss = 1.322
Epoch  63 Batch  653/769   train_loss = 1.421
Epoch  63 Batch  663/769   train_loss = 1.397
Epoch  63 Batch  673/769   train_loss = 1.463
Epoch  63 Batch  683/769   train_loss = 1.399
Epoch  63 Batch  693/769   train_loss = 1.448
Epoch  63 Batch  703/769   train_loss = 1.417
Epoch  63 Batch  713/769   train_loss = 1.462
Epoch  63 Batch  723/769   train_loss = 1.363
Epoch  63 Batch  733/769   train_loss = 1.456
Epoch  63 Batch  743/769   train_loss = 1.486
Epoch  63 Batch  753/769   train_loss = 1.449
Epoch  63 Batch  763/769   train_loss = 1.481
Epoch  64 Batch    4/769   train_loss = 1.480
Epoch  64 Batch   14/769   train_loss = 1.453
Epoch  64 Batch   24/769   train_loss = 1.459
Epoch  64 Batch   34/769   train_loss = 1.432
Epoch  64 Batch   44/769   train_loss = 1.459
Epoch  64 Batch   54/769   train_loss = 1.413
Epoch  64 Batch   64/769   train_loss = 1.460
Epoch  64 Batch   74/769   train_loss = 1.507
Epoch  64 Batch   84/769   train_loss = 1.442
Epoch  64 Batch   94/769   train_loss = 1.425
Epoch  64 Batch  104/769   train_loss = 1.512
Epoch  64 Batch  114/769   train_loss = 1.427
Epoch  64 Batch  124/769   train_loss = 1.399
Epoch  64 Batch  134/769   train_loss = 1.386
Epoch  64 Batch  144/769   train_loss = 1.439
Epoch  64 Batch  154/769   train_loss = 1.425
Epoch  64 Batch  164/769   train_loss = 1.462
Epoch  64 Batch  174/769   train_loss = 1.422
Epoch  64 Batch  184/769   train_loss = 1.446
Epoch  64 Batch  194/769   train_loss = 1.344
Epoch  64 Batch  204/769   train_loss = 1.418
Epoch  64 Batch  214/769   train_loss = 1.429
Epoch  64 Batch  224/769   train_loss = 1.414
Epoch  64 Batch  234/769   train_loss = 1.451
Epoch  64 Batch  244/769   train_loss = 1.460
Epoch  64 Batch  254/769   train_loss = 1.365
Epoch  64 Batch  264/769   train_loss = 1.416
Epoch  64 Batch  274/769   train_loss = 1.379
Epoch  64 Batch  284/769   train_loss = 1.358
Epoch  64 Batch  294/769   train_loss = 1.356
Epoch  64 Batch  304/769   train_loss = 1.441
Epoch  64 Batch  314/769   train_loss = 1.411
Epoch  64 Batch  324/769   train_loss = 1.397
Epoch  64 Batch  334/769   train_loss = 1.414
Epoch  64 Batch  344/769   train_loss = 1.432
Epoch  64 Batch  354/769   train_loss = 1.508
Epoch  64 Batch  364/769   train_loss = 1.374
Epoch  64 Batch  374/769   train_loss = 1.390
Epoch  64 Batch  384/769   train_loss = 1.323
Epoch  64 Batch  394/769   train_loss = 1.453
Epoch  64 Batch  404/769   train_loss = 1.510
Epoch  64 Batch  414/769   train_loss = 1.412
Epoch  64 Batch  424/769   train_loss = 1.338
Epoch  64 Batch  434/769   train_loss = 1.426
Epoch  64 Batch  444/769   train_loss = 1.470
Epoch  64 Batch  454/769   train_loss = 1.445
Epoch  64 Batch  464/769   train_loss = 1.516
Epoch  64 Batch  474/769   train_loss = 1.424
Epoch  64 Batch  484/769   train_loss = 1.403
Epoch  64 Batch  494/769   train_loss = 1.401
Epoch  64 Batch  504/769   train_loss = 1.510
Epoch  64 Batch  514/769   train_loss = 1.459
Epoch  64 Batch  524/769   train_loss = 1.578
Epoch  64 Batch  534/769   train_loss = 1.399
Epoch  64 Batch  544/769   train_loss = 1.414
Epoch  64 Batch  554/769   train_loss = 1.385
Epoch  64 Batch  564/769   train_loss = 1.386
Epoch  64 Batch  574/769   train_loss = 1.391
Epoch  64 Batch  584/769   train_loss = 1.325
Epoch  64 Batch  594/769   train_loss = 1.398
Epoch  64 Batch  604/769   train_loss = 1.382
Epoch  64 Batch  614/769   train_loss = 1.362
Epoch  64 Batch  624/769   train_loss = 1.474
Epoch  64 Batch  634/769   train_loss = 1.382
Epoch  64 Batch  644/769   train_loss = 1.353
Epoch  64 Batch  654/769   train_loss = 1.481
Epoch  64 Batch  664/769   train_loss = 1.460
Epoch  64 Batch  674/769   train_loss = 1.405
Epoch  64 Batch  684/769   train_loss = 1.417
Epoch  64 Batch  694/769   train_loss = 1.443
Epoch  64 Batch  704/769   train_loss = 1.450
Epoch  64 Batch  714/769   train_loss = 1.395
Epoch  64 Batch  724/769   train_loss = 1.390
Epoch  64 Batch  734/769   train_loss = 1.515
Epoch  64 Batch  744/769   train_loss = 1.408
Epoch  64 Batch  754/769   train_loss = 1.455
Epoch  64 Batch  764/769   train_loss = 1.429
Epoch  65 Batch    5/769   train_loss = 1.478
Epoch  65 Batch   15/769   train_loss = 1.432
Epoch  65 Batch   25/769   train_loss = 1.480
Epoch  65 Batch   35/769   train_loss = 1.476
Epoch  65 Batch   45/769   train_loss = 1.438
Epoch  65 Batch   55/769   train_loss = 1.461
Epoch  65 Batch   65/769   train_loss = 1.403
Epoch  65 Batch   75/769   train_loss = 1.488
Epoch  65 Batch   85/769   train_loss = 1.477
Epoch  65 Batch   95/769   train_loss = 1.437
Epoch  65 Batch  105/769   train_loss = 1.475
Epoch  65 Batch  115/769   train_loss = 1.456
Epoch  65 Batch  125/769   train_loss = 1.311
Epoch  65 Batch  135/769   train_loss = 1.445
Epoch  65 Batch  145/769   train_loss = 1.439
Epoch  65 Batch  155/769   train_loss = 1.460
Epoch  65 Batch  165/769   train_loss = 1.377
Epoch  65 Batch  175/769   train_loss = 1.332
Epoch  65 Batch  185/769   train_loss = 1.412
Epoch  65 Batch  195/769   train_loss = 1.417
Epoch  65 Batch  205/769   train_loss = 1.461
Epoch  65 Batch  215/769   train_loss = 1.426
Epoch  65 Batch  225/769   train_loss = 1.342
Epoch  65 Batch  235/769   train_loss = 1.403
Epoch  65 Batch  245/769   train_loss = 1.389
Epoch  65 Batch  255/769   train_loss = 1.420
Epoch  65 Batch  265/769   train_loss = 1.324
Epoch  65 Batch  275/769   train_loss = 1.381
Epoch  65 Batch  285/769   train_loss = 1.503
Epoch  65 Batch  295/769   train_loss = 1.443
Epoch  65 Batch  305/769   train_loss = 1.434
Epoch  65 Batch  315/769   train_loss = 1.328
Epoch  65 Batch  325/769   train_loss = 1.376
Epoch  65 Batch  335/769   train_loss = 1.440
Epoch  65 Batch  345/769   train_loss = 1.352
Epoch  65 Batch  355/769   train_loss = 1.382
Epoch  65 Batch  365/769   train_loss = 1.399
Epoch  65 Batch  375/769   train_loss = 1.467
Epoch  65 Batch  385/769   train_loss = 1.290
Epoch  65 Batch  395/769   train_loss = 1.474
Epoch  65 Batch  405/769   train_loss = 1.417
Epoch  65 Batch  415/769   train_loss = 1.318
Epoch  65 Batch  425/769   train_loss = 1.337
Epoch  65 Batch  435/769   train_loss = 1.384
Epoch  65 Batch  445/769   train_loss = 1.423
Epoch  65 Batch  455/769   train_loss = 1.463
Epoch  65 Batch  465/769   train_loss = 1.487
Epoch  65 Batch  475/769   train_loss = 1.328
Epoch  65 Batch  485/769   train_loss = 1.404
Epoch  65 Batch  495/769   train_loss = 1.429
Epoch  65 Batch  505/769   train_loss = 1.435
Epoch  65 Batch  515/769   train_loss = 1.479
Epoch  65 Batch  525/769   train_loss = 1.390
Epoch  65 Batch  535/769   train_loss = 1.386
Epoch  65 Batch  545/769   train_loss = 1.442
Epoch  65 Batch  555/769   train_loss = 1.451
Epoch  65 Batch  565/769   train_loss = 1.475
Epoch  65 Batch  575/769   train_loss = 1.438
Epoch  65 Batch  585/769   train_loss = 1.375
Epoch  65 Batch  595/769   train_loss = 1.353
Epoch  65 Batch  605/769   train_loss = 1.376
Epoch  65 Batch  615/769   train_loss = 1.433
Epoch  65 Batch  625/769   train_loss = 1.409
Epoch  65 Batch  635/769   train_loss = 1.367
Epoch  65 Batch  645/769   train_loss = 1.355
Epoch  65 Batch  655/769   train_loss = 1.418
Epoch  65 Batch  665/769   train_loss = 1.424
Epoch  65 Batch  675/769   train_loss = 1.422
Epoch  65 Batch  685/769   train_loss = 1.419
Epoch  65 Batch  695/769   train_loss = 1.475
Epoch  65 Batch  705/769   train_loss = 1.442
Epoch  65 Batch  715/769   train_loss = 1.431
Epoch  65 Batch  725/769   train_loss = 1.376
Epoch  65 Batch  735/769   train_loss = 1.375
Epoch  65 Batch  745/769   train_loss = 1.431
Epoch  65 Batch  755/769   train_loss = 1.351
Epoch  65 Batch  765/769   train_loss = 1.424
Epoch  66 Batch    6/769   train_loss = 1.393
Epoch  66 Batch   16/769   train_loss = 1.404
Epoch  66 Batch   26/769   train_loss = 1.402
Epoch  66 Batch   36/769   train_loss = 1.468
Epoch  66 Batch   46/769   train_loss = 1.477
Epoch  66 Batch   56/769   train_loss = 1.406
Epoch  66 Batch   66/769   train_loss = 1.438
Epoch  66 Batch   76/769   train_loss = 1.495
Epoch  66 Batch   86/769   train_loss = 1.440
Epoch  66 Batch   96/769   train_loss = 1.364
Epoch  66 Batch  106/769   train_loss = 1.448
Epoch  66 Batch  116/769   train_loss = 1.386
Epoch  66 Batch  126/769   train_loss = 1.385
Epoch  66 Batch  136/769   train_loss = 1.371
Epoch  66 Batch  146/769   train_loss = 1.431
Epoch  66 Batch  156/769   train_loss = 1.441
Epoch  66 Batch  166/769   train_loss = 1.365
Epoch  66 Batch  176/769   train_loss = 1.355
Epoch  66 Batch  186/769   train_loss = 1.362
Epoch  66 Batch  196/769   train_loss = 1.380
Epoch  66 Batch  206/769   train_loss = 1.401
Epoch  66 Batch  216/769   train_loss = 1.390
Epoch  66 Batch  226/769   train_loss = 1.409
Epoch  66 Batch  236/769   train_loss = 1.409
Epoch  66 Batch  246/769   train_loss = 1.395
Epoch  66 Batch  256/769   train_loss = 1.398
Epoch  66 Batch  266/769   train_loss = 1.408
Epoch  66 Batch  276/769   train_loss = 1.373
Epoch  66 Batch  286/769   train_loss = 1.390
Epoch  66 Batch  296/769   train_loss = 1.369
Epoch  66 Batch  306/769   train_loss = 1.376
Epoch  66 Batch  316/769   train_loss = 1.364
Epoch  66 Batch  326/769   train_loss = 1.437
Epoch  66 Batch  336/769   train_loss = 1.409
Epoch  66 Batch  346/769   train_loss = 1.389
Epoch  66 Batch  356/769   train_loss = 1.422
Epoch  66 Batch  366/769   train_loss = 1.399
Epoch  66 Batch  376/769   train_loss = 1.331
Epoch  66 Batch  386/769   train_loss = 1.395
Epoch  66 Batch  396/769   train_loss = 1.442
Epoch  66 Batch  406/769   train_loss = 1.357
Epoch  66 Batch  416/769   train_loss = 1.391
Epoch  66 Batch  426/769   train_loss = 1.448
Epoch  66 Batch  436/769   train_loss = 1.413
Epoch  66 Batch  446/769   train_loss = 1.416
Epoch  66 Batch  456/769   train_loss = 1.461
Epoch  66 Batch  466/769   train_loss = 1.428
Epoch  66 Batch  476/769   train_loss = 1.429
Epoch  66 Batch  486/769   train_loss = 1.443
Epoch  66 Batch  496/769   train_loss = 1.365
Epoch  66 Batch  506/769   train_loss = 1.456
Epoch  66 Batch  516/769   train_loss = 1.356
Epoch  66 Batch  526/769   train_loss = 1.332
Epoch  66 Batch  536/769   train_loss = 1.364
Epoch  66 Batch  546/769   train_loss = 1.406
Epoch  66 Batch  556/769   train_loss = 1.455
Epoch  66 Batch  566/769   train_loss = 1.497
Epoch  66 Batch  576/769   train_loss = 1.335
Epoch  66 Batch  586/769   train_loss = 1.357
Epoch  66 Batch  596/769   train_loss = 1.450
Epoch  66 Batch  606/769   train_loss = 1.438
Epoch  66 Batch  616/769   train_loss = 1.380
Epoch  66 Batch  626/769   train_loss = 1.402
Epoch  66 Batch  636/769   train_loss = 1.362
Epoch  66 Batch  646/769   train_loss = 1.449
Epoch  66 Batch  656/769   train_loss = 1.384
Epoch  66 Batch  666/769   train_loss = 1.441
Epoch  66 Batch  676/769   train_loss = 1.420
Epoch  66 Batch  686/769   train_loss = 1.388
Epoch  66 Batch  696/769   train_loss = 1.379
Epoch  66 Batch  706/769   train_loss = 1.455
Epoch  66 Batch  716/769   train_loss = 1.392
Epoch  66 Batch  726/769   train_loss = 1.390
Epoch  66 Batch  736/769   train_loss = 1.409
Epoch  66 Batch  746/769   train_loss = 1.360
Epoch  66 Batch  756/769   train_loss = 1.403
Epoch  66 Batch  766/769   train_loss = 1.424
Epoch  67 Batch    7/769   train_loss = 1.421
Epoch  67 Batch   17/769   train_loss = 1.417
Epoch  67 Batch   27/769   train_loss = 1.418
Epoch  67 Batch   37/769   train_loss = 1.398
Epoch  67 Batch   47/769   train_loss = 1.372
Epoch  67 Batch   57/769   train_loss = 1.474
Epoch  67 Batch   67/769   train_loss = 1.435
Epoch  67 Batch   77/769   train_loss = 1.448
Epoch  67 Batch   87/769   train_loss = 1.456
Epoch  67 Batch   97/769   train_loss = 1.392
Epoch  67 Batch  107/769   train_loss = 1.420
Epoch  67 Batch  117/769   train_loss = 1.363
Epoch  67 Batch  127/769   train_loss = 1.459
Epoch  67 Batch  137/769   train_loss = 1.357
Epoch  67 Batch  147/769   train_loss = 1.390
Epoch  67 Batch  157/769   train_loss = 1.508
Epoch  67 Batch  167/769   train_loss = 1.396
Epoch  67 Batch  177/769   train_loss = 1.344
Epoch  67 Batch  187/769   train_loss = 1.473
Epoch  67 Batch  197/769   train_loss = 1.348
Epoch  67 Batch  207/769   train_loss = 1.315
Epoch  67 Batch  217/769   train_loss = 1.392
Epoch  67 Batch  227/769   train_loss = 1.367
Epoch  67 Batch  237/769   train_loss = 1.357
Epoch  67 Batch  247/769   train_loss = 1.382
Epoch  67 Batch  257/769   train_loss = 1.359
Epoch  67 Batch  267/769   train_loss = 1.401
Epoch  67 Batch  277/769   train_loss = 1.407
Epoch  67 Batch  287/769   train_loss = 1.381
Epoch  67 Batch  297/769   train_loss = 1.394
Epoch  67 Batch  307/769   train_loss = 1.440
Epoch  67 Batch  317/769   train_loss = 1.302
Epoch  67 Batch  327/769   train_loss = 1.380
Epoch  67 Batch  337/769   train_loss = 1.402
Epoch  67 Batch  347/769   train_loss = 1.377
Epoch  67 Batch  357/769   train_loss = 1.384
Epoch  67 Batch  367/769   train_loss = 1.371
Epoch  67 Batch  377/769   train_loss = 1.390
Epoch  67 Batch  387/769   train_loss = 1.453
Epoch  67 Batch  397/769   train_loss = 1.338
Epoch  67 Batch  407/769   train_loss = 1.435
Epoch  67 Batch  417/769   train_loss = 1.448
Epoch  67 Batch  427/769   train_loss = 1.432
Epoch  67 Batch  437/769   train_loss = 1.457
Epoch  67 Batch  447/769   train_loss = 1.445
Epoch  67 Batch  457/769   train_loss = 1.384
Epoch  67 Batch  467/769   train_loss = 1.396
Epoch  67 Batch  477/769   train_loss = 1.402
Epoch  67 Batch  487/769   train_loss = 1.391
Epoch  67 Batch  497/769   train_loss = 1.391
Epoch  67 Batch  507/769   train_loss = 1.376
Epoch  67 Batch  517/769   train_loss = 1.408
Epoch  67 Batch  527/769   train_loss = 1.377
Epoch  67 Batch  537/769   train_loss = 1.417
Epoch  67 Batch  547/769   train_loss = 1.477
Epoch  67 Batch  557/769   train_loss = 1.382
Epoch  67 Batch  567/769   train_loss = 1.393
Epoch  67 Batch  577/769   train_loss = 1.468
Epoch  67 Batch  587/769   train_loss = 1.394
Epoch  67 Batch  597/769   train_loss = 1.289
Epoch  67 Batch  607/769   train_loss = 1.396
Epoch  67 Batch  617/769   train_loss = 1.404
Epoch  67 Batch  627/769   train_loss = 1.445
Epoch  67 Batch  637/769   train_loss = 1.340
Epoch  67 Batch  647/769   train_loss = 1.470
Epoch  67 Batch  657/769   train_loss = 1.352
Epoch  67 Batch  667/769   train_loss = 1.435
Epoch  67 Batch  677/769   train_loss = 1.363
Epoch  67 Batch  687/769   train_loss = 1.451
Epoch  67 Batch  697/769   train_loss = 1.467
Epoch  67 Batch  707/769   train_loss = 1.413
Epoch  67 Batch  717/769   train_loss = 1.392
Epoch  67 Batch  727/769   train_loss = 1.411
Epoch  67 Batch  737/769   train_loss = 1.480
Epoch  67 Batch  747/769   train_loss = 1.393
Epoch  67 Batch  757/769   train_loss = 1.424
Epoch  67 Batch  767/769   train_loss = 1.415
Epoch  68 Batch    8/769   train_loss = 1.406
Epoch  68 Batch   18/769   train_loss = 1.438
Epoch  68 Batch   28/769   train_loss = 1.414
Epoch  68 Batch   38/769   train_loss = 1.463
Epoch  68 Batch   48/769   train_loss = 1.438
Epoch  68 Batch   58/769   train_loss = 1.385
Epoch  68 Batch   68/769   train_loss = 1.494
Epoch  68 Batch   78/769   train_loss = 1.439
Epoch  68 Batch   88/769   train_loss = 1.392
Epoch  68 Batch   98/769   train_loss = 1.370
Epoch  68 Batch  108/769   train_loss = 1.449
Epoch  68 Batch  118/769   train_loss = 1.376
Epoch  68 Batch  128/769   train_loss = 1.435
Epoch  68 Batch  138/769   train_loss = 1.343
Epoch  68 Batch  148/769   train_loss = 1.409
Epoch  68 Batch  158/769   train_loss = 1.439
Epoch  68 Batch  168/769   train_loss = 1.325
Epoch  68 Batch  178/769   train_loss = 1.450
Epoch  68 Batch  188/769   train_loss = 1.378
Epoch  68 Batch  198/769   train_loss = 1.401
Epoch  68 Batch  208/769   train_loss = 1.403
Epoch  68 Batch  218/769   train_loss = 1.398
Epoch  68 Batch  228/769   train_loss = 1.390
Epoch  68 Batch  238/769   train_loss = 1.411
Epoch  68 Batch  248/769   train_loss = 1.403
Epoch  68 Batch  258/769   train_loss = 1.385
Epoch  68 Batch  268/769   train_loss = 1.395
Epoch  68 Batch  278/769   train_loss = 1.412
Epoch  68 Batch  288/769   train_loss = 1.381
Epoch  68 Batch  298/769   train_loss = 1.440
Epoch  68 Batch  308/769   train_loss = 1.407
Epoch  68 Batch  318/769   train_loss = 1.338
Epoch  68 Batch  328/769   train_loss = 1.364
Epoch  68 Batch  338/769   train_loss = 1.362
Epoch  68 Batch  348/769   train_loss = 1.337
Epoch  68 Batch  358/769   train_loss = 1.382
Epoch  68 Batch  368/769   train_loss = 1.407
Epoch  68 Batch  378/769   train_loss = 1.371
Epoch  68 Batch  388/769   train_loss = 1.350
Epoch  68 Batch  398/769   train_loss = 1.426
Epoch  68 Batch  408/769   train_loss = 1.396
Epoch  68 Batch  418/769   train_loss = 1.394
Epoch  68 Batch  428/769   train_loss = 1.356
Epoch  68 Batch  438/769   train_loss = 1.342
Epoch  68 Batch  448/769   train_loss = 1.395
Epoch  68 Batch  458/769   train_loss = 1.478
Epoch  68 Batch  468/769   train_loss = 1.395
Epoch  68 Batch  478/769   train_loss = 1.299
Epoch  68 Batch  488/769   train_loss = 1.377
Epoch  68 Batch  498/769   train_loss = 1.369
Epoch  68 Batch  508/769   train_loss = 1.369
Epoch  68 Batch  518/769   train_loss = 1.391
Epoch  68 Batch  528/769   train_loss = 1.337
Epoch  68 Batch  538/769   train_loss = 1.400
Epoch  68 Batch  548/769   train_loss = 1.334
Epoch  68 Batch  558/769   train_loss = 1.419
Epoch  68 Batch  568/769   train_loss = 1.388
Epoch  68 Batch  578/769   train_loss = 1.271
Epoch  68 Batch  588/769   train_loss = 1.327
Epoch  68 Batch  598/769   train_loss = 1.332
Epoch  68 Batch  608/769   train_loss = 1.299
Epoch  68 Batch  618/769   train_loss = 1.324
Epoch  68 Batch  628/769   train_loss = 1.400
Epoch  68 Batch  638/769   train_loss = 1.360
Epoch  68 Batch  648/769   train_loss = 1.421
Epoch  68 Batch  658/769   train_loss = 1.475
Epoch  68 Batch  668/769   train_loss = 1.383
Epoch  68 Batch  678/769   train_loss = 1.395
Epoch  68 Batch  688/769   train_loss = 1.397
Epoch  68 Batch  698/769   train_loss = 1.383
Epoch  68 Batch  708/769   train_loss = 1.413
Epoch  68 Batch  718/769   train_loss = 1.360
Epoch  68 Batch  728/769   train_loss = 1.401
Epoch  68 Batch  738/769   train_loss = 1.421
Epoch  68 Batch  748/769   train_loss = 1.430
Epoch  68 Batch  758/769   train_loss = 1.379
Epoch  68 Batch  768/769   train_loss = 1.385
Epoch  69 Batch    9/769   train_loss = 1.398
Epoch  69 Batch   19/769   train_loss = 1.458
Epoch  69 Batch   29/769   train_loss = 1.392
Epoch  69 Batch   39/769   train_loss = 1.376
Epoch  69 Batch   49/769   train_loss = 1.378
Epoch  69 Batch   59/769   train_loss = 1.365
Epoch  69 Batch   69/769   train_loss = 1.398
Epoch  69 Batch   79/769   train_loss = 1.418
Epoch  69 Batch   89/769   train_loss = 1.333
Epoch  69 Batch   99/769   train_loss = 1.442
Epoch  69 Batch  109/769   train_loss = 1.423
Epoch  69 Batch  119/769   train_loss = 1.375
Epoch  69 Batch  129/769   train_loss = 1.338
Epoch  69 Batch  139/769   train_loss = 1.388
Epoch  69 Batch  149/769   train_loss = 1.390
Epoch  69 Batch  159/769   train_loss = 1.337
Epoch  69 Batch  169/769   train_loss = 1.415
Epoch  69 Batch  179/769   train_loss = 1.361
Epoch  69 Batch  189/769   train_loss = 1.387
Epoch  69 Batch  199/769   train_loss = 1.400
Epoch  69 Batch  209/769   train_loss = 1.376
Epoch  69 Batch  219/769   train_loss = 1.392
Epoch  69 Batch  229/769   train_loss = 1.389
Epoch  69 Batch  239/769   train_loss = 1.344
Epoch  69 Batch  249/769   train_loss = 1.321
Epoch  69 Batch  259/769   train_loss = 1.347
Epoch  69 Batch  269/769   train_loss = 1.355
Epoch  69 Batch  279/769   train_loss = 1.385
Epoch  69 Batch  289/769   train_loss = 1.333
Epoch  69 Batch  299/769   train_loss = 1.469
Epoch  69 Batch  309/769   train_loss = 1.404
Epoch  69 Batch  319/769   train_loss = 1.437
Epoch  69 Batch  329/769   train_loss = 1.381
Epoch  69 Batch  339/769   train_loss = 1.372
Epoch  69 Batch  349/769   train_loss = 1.345
Epoch  69 Batch  359/769   train_loss = 1.414
Epoch  69 Batch  369/769   train_loss = 1.346
Epoch  69 Batch  379/769   train_loss = 1.273
Epoch  69 Batch  389/769   train_loss = 1.283
Epoch  69 Batch  399/769   train_loss = 1.416
Epoch  69 Batch  409/769   train_loss = 1.427
Epoch  69 Batch  419/769   train_loss = 1.486
Epoch  69 Batch  429/769   train_loss = 1.359
Epoch  69 Batch  439/769   train_loss = 1.415
Epoch  69 Batch  449/769   train_loss = 1.429
Epoch  69 Batch  459/769   train_loss = 1.394
Epoch  69 Batch  469/769   train_loss = 1.313
Epoch  69 Batch  479/769   train_loss = 1.390
Epoch  69 Batch  489/769   train_loss = 1.471
Epoch  69 Batch  499/769   train_loss = 1.402
Epoch  69 Batch  509/769   train_loss = 1.337
Epoch  69 Batch  519/769   train_loss = 1.437
Epoch  69 Batch  529/769   train_loss = 1.441
Epoch  69 Batch  539/769   train_loss = 1.431
Epoch  69 Batch  549/769   train_loss = 1.442
Epoch  69 Batch  559/769   train_loss = 1.432
Epoch  69 Batch  569/769   train_loss = 1.345
Epoch  69 Batch  579/769   train_loss = 1.275
Epoch  69 Batch  589/769   train_loss = 1.391
Epoch  69 Batch  599/769   train_loss = 1.363
Epoch  69 Batch  609/769   train_loss = 1.393
Epoch  69 Batch  619/769   train_loss = 1.405
Epoch  69 Batch  629/769   train_loss = 1.361
Epoch  69 Batch  639/769   train_loss = 1.348
Epoch  69 Batch  649/769   train_loss = 1.332
Epoch  69 Batch  659/769   train_loss = 1.377
Epoch  69 Batch  669/769   train_loss = 1.383
Epoch  69 Batch  679/769   train_loss = 1.327
Epoch  69 Batch  689/769   train_loss = 1.426
Epoch  69 Batch  699/769   train_loss = 1.399
Epoch  69 Batch  709/769   train_loss = 1.443
Epoch  69 Batch  719/769   train_loss = 1.373
Epoch  69 Batch  729/769   train_loss = 1.355
Epoch  69 Batch  739/769   train_loss = 1.315
Epoch  69 Batch  749/769   train_loss = 1.341
Epoch  69 Batch  759/769   train_loss = 1.374
Epoch  70 Batch    0/769   train_loss = 1.318
Epoch  70 Batch   10/769   train_loss = 1.432
Epoch  70 Batch   20/769   train_loss = 1.370
Epoch  70 Batch   30/769   train_loss = 1.362
Epoch  70 Batch   40/769   train_loss = 1.394
Epoch  70 Batch   50/769   train_loss = 1.379
Epoch  70 Batch   60/769   train_loss = 1.478
Epoch  70 Batch   70/769   train_loss = 1.409
Epoch  70 Batch   80/769   train_loss = 1.356
Epoch  70 Batch   90/769   train_loss = 1.415
Epoch  70 Batch  100/769   train_loss = 1.418
Epoch  70 Batch  110/769   train_loss = 1.382
Epoch  70 Batch  120/769   train_loss = 1.306
Epoch  70 Batch  130/769   train_loss = 1.331
Epoch  70 Batch  140/769   train_loss = 1.421
Epoch  70 Batch  150/769   train_loss = 1.369
Epoch  70 Batch  160/769   train_loss = 1.373
Epoch  70 Batch  170/769   train_loss = 1.338
Epoch  70 Batch  180/769   train_loss = 1.345
Epoch  70 Batch  190/769   train_loss = 1.323
Epoch  70 Batch  200/769   train_loss = 1.433
Epoch  70 Batch  210/769   train_loss = 1.326
Epoch  70 Batch  220/769   train_loss = 1.311
Epoch  70 Batch  230/769   train_loss = 1.317
Epoch  70 Batch  240/769   train_loss = 1.324
Epoch  70 Batch  250/769   train_loss = 1.331
Epoch  70 Batch  260/769   train_loss = 1.387
Epoch  70 Batch  270/769   train_loss = 1.333
Epoch  70 Batch  280/769   train_loss = 1.389
Epoch  70 Batch  290/769   train_loss = 1.331
Epoch  70 Batch  300/769   train_loss = 1.424
Epoch  70 Batch  310/769   train_loss = 1.323
Epoch  70 Batch  320/769   train_loss = 1.326
Epoch  70 Batch  330/769   train_loss = 1.390
Epoch  70 Batch  340/769   train_loss = 1.331
Epoch  70 Batch  350/769   train_loss = 1.292
Epoch  70 Batch  360/769   train_loss = 1.285
Epoch  70 Batch  370/769   train_loss = 1.316
Epoch  70 Batch  380/769   train_loss = 1.341
Epoch  70 Batch  390/769   train_loss = 1.276
Epoch  70 Batch  400/769   train_loss = 1.358
Epoch  70 Batch  410/769   train_loss = 1.408
Epoch  70 Batch  420/769   train_loss = 1.397
Epoch  70 Batch  430/769   train_loss = 1.365
Epoch  70 Batch  440/769   train_loss = 1.425
Epoch  70 Batch  450/769   train_loss = 1.346
Epoch  70 Batch  460/769   train_loss = 1.502
Epoch  70 Batch  470/769   train_loss = 1.379
Epoch  70 Batch  480/769   train_loss = 1.338
Epoch  70 Batch  490/769   train_loss = 1.354
Epoch  70 Batch  500/769   train_loss = 1.369
Epoch  70 Batch  510/769   train_loss = 1.421
Epoch  70 Batch  520/769   train_loss = 1.307
Epoch  70 Batch  530/769   train_loss = 1.392
Epoch  70 Batch  540/769   train_loss = 1.441
Epoch  70 Batch  550/769   train_loss = 1.308
Epoch  70 Batch  560/769   train_loss = 1.386
Epoch  70 Batch  570/769   train_loss = 1.349
Epoch  70 Batch  580/769   train_loss = 1.299
Epoch  70 Batch  590/769   train_loss = 1.320
Epoch  70 Batch  600/769   train_loss = 1.367
Epoch  70 Batch  610/769   train_loss = 1.326
Epoch  70 Batch  620/769   train_loss = 1.397
Epoch  70 Batch  630/769   train_loss = 1.393
Epoch  70 Batch  640/769   train_loss = 1.284
Epoch  70 Batch  650/769   train_loss = 1.344
Epoch  70 Batch  660/769   train_loss = 1.403
Epoch  70 Batch  670/769   train_loss = 1.357
Epoch  70 Batch  680/769   train_loss = 1.405
Epoch  70 Batch  690/769   train_loss = 1.270
Epoch  70 Batch  700/769   train_loss = 1.389
Epoch  70 Batch  710/769   train_loss = 1.365
Epoch  70 Batch  720/769   train_loss = 1.303
Epoch  70 Batch  730/769   train_loss = 1.355
Epoch  70 Batch  740/769   train_loss = 1.360
Epoch  70 Batch  750/769   train_loss = 1.335
Epoch  70 Batch  760/769   train_loss = 1.411
Epoch  71 Batch    1/769   train_loss = 1.360
Epoch  71 Batch   11/769   train_loss = 1.354
Epoch  71 Batch   21/769   train_loss = 1.379
Epoch  71 Batch   31/769   train_loss = 1.342
Epoch  71 Batch   41/769   train_loss = 1.429
Epoch  71 Batch   51/769   train_loss = 1.402
Epoch  71 Batch   61/769   train_loss = 1.441
Epoch  71 Batch   71/769   train_loss = 1.357
Epoch  71 Batch   81/769   train_loss = 1.490
Epoch  71 Batch   91/769   train_loss = 1.375
Epoch  71 Batch  101/769   train_loss = 1.354
Epoch  71 Batch  111/769   train_loss = 1.368
Epoch  71 Batch  121/769   train_loss = 1.405
Epoch  71 Batch  131/769   train_loss = 1.317
Epoch  71 Batch  141/769   train_loss = 1.370
Epoch  71 Batch  151/769   train_loss = 1.373
Epoch  71 Batch  161/769   train_loss = 1.418
Epoch  71 Batch  171/769   train_loss = 1.385
Epoch  71 Batch  181/769   train_loss = 1.388
Epoch  71 Batch  191/769   train_loss = 1.328
Epoch  71 Batch  201/769   train_loss = 1.387
Epoch  71 Batch  211/769   train_loss = 1.329
Epoch  71 Batch  221/769   train_loss = 1.246
Epoch  71 Batch  231/769   train_loss = 1.351
Epoch  71 Batch  241/769   train_loss = 1.387
Epoch  71 Batch  251/769   train_loss = 1.390
Epoch  71 Batch  261/769   train_loss = 1.305
Epoch  71 Batch  271/769   train_loss = 1.354
Epoch  71 Batch  281/769   train_loss = 1.372
Epoch  71 Batch  291/769   train_loss = 1.394
Epoch  71 Batch  301/769   train_loss = 1.371
Epoch  71 Batch  311/769   train_loss = 1.395
Epoch  71 Batch  321/769   train_loss = 1.340
Epoch  71 Batch  331/769   train_loss = 1.356
Epoch  71 Batch  341/769   train_loss = 1.294
Epoch  71 Batch  351/769   train_loss = 1.436
Epoch  71 Batch  361/769   train_loss = 1.273
Epoch  71 Batch  371/769   train_loss = 1.316
Epoch  71 Batch  381/769   train_loss = 1.281
Epoch  71 Batch  391/769   train_loss = 1.322
Epoch  71 Batch  401/769   train_loss = 1.310
Epoch  71 Batch  411/769   train_loss = 1.339
Epoch  71 Batch  421/769   train_loss = 1.410
Epoch  71 Batch  431/769   train_loss = 1.362
Epoch  71 Batch  441/769   train_loss = 1.346
Epoch  71 Batch  451/769   train_loss = 1.334
Epoch  71 Batch  461/769   train_loss = 1.354
Epoch  71 Batch  471/769   train_loss = 1.361
Epoch  71 Batch  481/769   train_loss = 1.383
Epoch  71 Batch  491/769   train_loss = 1.393
Epoch  71 Batch  501/769   train_loss = 1.400
Epoch  71 Batch  511/769   train_loss = 1.373
Epoch  71 Batch  521/769   train_loss = 1.354
Epoch  71 Batch  531/769   train_loss = 1.335
Epoch  71 Batch  541/769   train_loss = 1.360
Epoch  71 Batch  551/769   train_loss = 1.384
Epoch  71 Batch  561/769   train_loss = 1.391
Epoch  71 Batch  571/769   train_loss = 1.374
Epoch  71 Batch  581/769   train_loss = 1.266
Epoch  71 Batch  591/769   train_loss = 1.322
Epoch  71 Batch  601/769   train_loss = 1.362
Epoch  71 Batch  611/769   train_loss = 1.344
Epoch  71 Batch  621/769   train_loss = 1.375
Epoch  71 Batch  631/769   train_loss = 1.383
Epoch  71 Batch  641/769   train_loss = 1.380
Epoch  71 Batch  651/769   train_loss = 1.301
Epoch  71 Batch  661/769   train_loss = 1.363
Epoch  71 Batch  671/769   train_loss = 1.386
Epoch  71 Batch  681/769   train_loss = 1.333
Epoch  71 Batch  691/769   train_loss = 1.354
Epoch  71 Batch  701/769   train_loss = 1.296
Epoch  71 Batch  711/769   train_loss = 1.405
Epoch  71 Batch  721/769   train_loss = 1.401
Epoch  71 Batch  731/769   train_loss = 1.357
Epoch  71 Batch  741/769   train_loss = 1.293
Epoch  71 Batch  751/769   train_loss = 1.334
Epoch  71 Batch  761/769   train_loss = 1.330
Epoch  72 Batch    2/769   train_loss = 1.408
Epoch  72 Batch   12/769   train_loss = 1.324
Epoch  72 Batch   22/769   train_loss = 1.402
Epoch  72 Batch   32/769   train_loss = 1.438
Epoch  72 Batch   42/769   train_loss = 1.492
Epoch  72 Batch   52/769   train_loss = 1.356
Epoch  72 Batch   62/769   train_loss = 1.430
Epoch  72 Batch   72/769   train_loss = 1.429
Epoch  72 Batch   82/769   train_loss = 1.402
Epoch  72 Batch   92/769   train_loss = 1.403
Epoch  72 Batch  102/769   train_loss = 1.392
Epoch  72 Batch  112/769   train_loss = 1.333
Epoch  72 Batch  122/769   train_loss = 1.313
Epoch  72 Batch  132/769   train_loss = 1.306
Epoch  72 Batch  142/769   train_loss = 1.342
Epoch  72 Batch  152/769   train_loss = 1.282
Epoch  72 Batch  162/769   train_loss = 1.417
Epoch  72 Batch  172/769   train_loss = 1.405
Epoch  72 Batch  182/769   train_loss = 1.317
Epoch  72 Batch  192/769   train_loss = 1.325
Epoch  72 Batch  202/769   train_loss = 1.335
Epoch  72 Batch  212/769   train_loss = 1.316
Epoch  72 Batch  222/769   train_loss = 1.372
Epoch  72 Batch  232/769   train_loss = 1.336
Epoch  72 Batch  242/769   train_loss = 1.353
Epoch  72 Batch  252/769   train_loss = 1.333
Epoch  72 Batch  262/769   train_loss = 1.335
Epoch  72 Batch  272/769   train_loss = 1.365
Epoch  72 Batch  282/769   train_loss = 1.388
Epoch  72 Batch  292/769   train_loss = 1.296
Epoch  72 Batch  302/769   train_loss = 1.353
Epoch  72 Batch  312/769   train_loss = 1.374
Epoch  72 Batch  322/769   train_loss = 1.342
Epoch  72 Batch  332/769   train_loss = 1.335
Epoch  72 Batch  342/769   train_loss = 1.288
Epoch  72 Batch  352/769   train_loss = 1.330
Epoch  72 Batch  362/769   train_loss = 1.330
Epoch  72 Batch  372/769   train_loss = 1.354
Epoch  72 Batch  382/769   train_loss = 1.270
Epoch  72 Batch  392/769   train_loss = 1.359
Epoch  72 Batch  402/769   train_loss = 1.428
Epoch  72 Batch  412/769   train_loss = 1.347
Epoch  72 Batch  422/769   train_loss = 1.277
Epoch  72 Batch  432/769   train_loss = 1.322
Epoch  72 Batch  442/769   train_loss = 1.265
Epoch  72 Batch  452/769   train_loss = 1.324
Epoch  72 Batch  462/769   train_loss = 1.408
Epoch  72 Batch  472/769   train_loss = 1.332
Epoch  72 Batch  482/769   train_loss = 1.350
Epoch  72 Batch  492/769   train_loss = 1.405
Epoch  72 Batch  502/769   train_loss = 1.344
Epoch  72 Batch  512/769   train_loss = 1.341
Epoch  72 Batch  522/769   train_loss = 1.337
Epoch  72 Batch  532/769   train_loss = 1.326
Epoch  72 Batch  542/769   train_loss = 1.347
Epoch  72 Batch  552/769   train_loss = 1.316
Epoch  72 Batch  562/769   train_loss = 1.258
Epoch  72 Batch  572/769   train_loss = 1.333
Epoch  72 Batch  582/769   train_loss = 1.360
Epoch  72 Batch  592/769   train_loss = 1.272
Epoch  72 Batch  602/769   train_loss = 1.324
Epoch  72 Batch  612/769   train_loss = 1.329
Epoch  72 Batch  622/769   train_loss = 1.287
Epoch  72 Batch  632/769   train_loss = 1.347
Epoch  72 Batch  642/769   train_loss = 1.382
Epoch  72 Batch  652/769   train_loss = 1.352
Epoch  72 Batch  662/769   train_loss = 1.349
Epoch  72 Batch  672/769   train_loss = 1.333
Epoch  72 Batch  682/769   train_loss = 1.385
Epoch  72 Batch  692/769   train_loss = 1.359
Epoch  72 Batch  702/769   train_loss = 1.400
Epoch  72 Batch  712/769   train_loss = 1.369
Epoch  72 Batch  722/769   train_loss = 1.356
Epoch  72 Batch  732/769   train_loss = 1.296
Epoch  72 Batch  742/769   train_loss = 1.383
Epoch  72 Batch  752/769   train_loss = 1.414
Epoch  72 Batch  762/769   train_loss = 1.327
Epoch  73 Batch    3/769   train_loss = 1.418
Epoch  73 Batch   13/769   train_loss = 1.415
Epoch  73 Batch   23/769   train_loss = 1.367
Epoch  73 Batch   33/769   train_loss = 1.351
Epoch  73 Batch   43/769   train_loss = 1.413
Epoch  73 Batch   53/769   train_loss = 1.344
Epoch  73 Batch   63/769   train_loss = 1.266
Epoch  73 Batch   73/769   train_loss = 1.458
Epoch  73 Batch   83/769   train_loss = 1.307
Epoch  73 Batch   93/769   train_loss = 1.374
Epoch  73 Batch  103/769   train_loss = 1.333
Epoch  73 Batch  113/769   train_loss = 1.318
Epoch  73 Batch  123/769   train_loss = 1.393
Epoch  73 Batch  133/769   train_loss = 1.320
Epoch  73 Batch  143/769   train_loss = 1.334
Epoch  73 Batch  153/769   train_loss = 1.351
Epoch  73 Batch  163/769   train_loss = 1.384
Epoch  73 Batch  173/769   train_loss = 1.359
Epoch  73 Batch  183/769   train_loss = 1.434
Epoch  73 Batch  193/769   train_loss = 1.295
Epoch  73 Batch  203/769   train_loss = 1.290
Epoch  73 Batch  213/769   train_loss = 1.298
Epoch  73 Batch  223/769   train_loss = 1.348
Epoch  73 Batch  233/769   train_loss = 1.288
Epoch  73 Batch  243/769   train_loss = 1.374
Epoch  73 Batch  253/769   train_loss = 1.344
Epoch  73 Batch  263/769   train_loss = 1.380
Epoch  73 Batch  273/769   train_loss = 1.362
Epoch  73 Batch  283/769   train_loss = 1.398
Epoch  73 Batch  293/769   train_loss = 1.321
Epoch  73 Batch  303/769   train_loss = 1.379
Epoch  73 Batch  313/769   train_loss = 1.401
Epoch  73 Batch  323/769   train_loss = 1.361
Epoch  73 Batch  333/769   train_loss = 1.366
Epoch  73 Batch  343/769   train_loss = 1.343
Epoch  73 Batch  353/769   train_loss = 1.400
Epoch  73 Batch  363/769   train_loss = 1.355
Epoch  73 Batch  373/769   train_loss = 1.412
Epoch  73 Batch  383/769   train_loss = 1.313
Epoch  73 Batch  393/769   train_loss = 1.393
Epoch  73 Batch  403/769   train_loss = 1.407
Epoch  73 Batch  413/769   train_loss = 1.406
Epoch  73 Batch  423/769   train_loss = 1.317
Epoch  73 Batch  433/769   train_loss = 1.329
Epoch  73 Batch  443/769   train_loss = 1.334
Epoch  73 Batch  453/769   train_loss = 1.367
Epoch  73 Batch  463/769   train_loss = 1.320
Epoch  73 Batch  473/769   train_loss = 1.313
Epoch  73 Batch  483/769   train_loss = 1.316
Epoch  73 Batch  493/769   train_loss = 1.337
Epoch  73 Batch  503/769   train_loss = 1.385
Epoch  73 Batch  513/769   train_loss = 1.386
Epoch  73 Batch  523/769   train_loss = 1.293
Epoch  73 Batch  533/769   train_loss = 1.353
Epoch  73 Batch  543/769   train_loss = 1.322
Epoch  73 Batch  553/769   train_loss = 1.307
Epoch  73 Batch  563/769   train_loss = 1.340
Epoch  73 Batch  573/769   train_loss = 1.304
Epoch  73 Batch  583/769   train_loss = 1.328
Epoch  73 Batch  593/769   train_loss = 1.385
Epoch  73 Batch  603/769   train_loss = 1.289
Epoch  73 Batch  613/769   train_loss = 1.302
Epoch  73 Batch  623/769   train_loss = 1.304
Epoch  73 Batch  633/769   train_loss = 1.264
Epoch  73 Batch  643/769   train_loss = 1.243
Epoch  73 Batch  653/769   train_loss = 1.337
Epoch  73 Batch  663/769   train_loss = 1.307
Epoch  73 Batch  673/769   train_loss = 1.371
Epoch  73 Batch  683/769   train_loss = 1.336
Epoch  73 Batch  693/769   train_loss = 1.349
Epoch  73 Batch  703/769   train_loss = 1.325
Epoch  73 Batch  713/769   train_loss = 1.350
Epoch  73 Batch  723/769   train_loss = 1.285
Epoch  73 Batch  733/769   train_loss = 1.375
Epoch  73 Batch  743/769   train_loss = 1.367
Epoch  73 Batch  753/769   train_loss = 1.348
Epoch  73 Batch  763/769   train_loss = 1.405
Epoch  74 Batch    4/769   train_loss = 1.385
Epoch  74 Batch   14/769   train_loss = 1.357
Epoch  74 Batch   24/769   train_loss = 1.369
Epoch  74 Batch   34/769   train_loss = 1.326
Epoch  74 Batch   44/769   train_loss = 1.371
Epoch  74 Batch   54/769   train_loss = 1.318
Epoch  74 Batch   64/769   train_loss = 1.350
Epoch  74 Batch   74/769   train_loss = 1.407
Epoch  74 Batch   84/769   train_loss = 1.361
Epoch  74 Batch   94/769   train_loss = 1.311
Epoch  74 Batch  104/769   train_loss = 1.421
Epoch  74 Batch  114/769   train_loss = 1.344
Epoch  74 Batch  124/769   train_loss = 1.318
Epoch  74 Batch  134/769   train_loss = 1.299
Epoch  74 Batch  144/769   train_loss = 1.315
Epoch  74 Batch  154/769   train_loss = 1.326
Epoch  74 Batch  164/769   train_loss = 1.371
Epoch  74 Batch  174/769   train_loss = 1.320
Epoch  74 Batch  184/769   train_loss = 1.341
Epoch  74 Batch  194/769   train_loss = 1.224
Epoch  74 Batch  204/769   train_loss = 1.341
Epoch  74 Batch  214/769   train_loss = 1.332
Epoch  74 Batch  224/769   train_loss = 1.326
Epoch  74 Batch  234/769   train_loss = 1.386
Epoch  74 Batch  244/769   train_loss = 1.393
Epoch  74 Batch  254/769   train_loss = 1.275
Epoch  74 Batch  264/769   train_loss = 1.326
Epoch  74 Batch  274/769   train_loss = 1.311
Epoch  74 Batch  284/769   train_loss = 1.294
Epoch  74 Batch  294/769   train_loss = 1.289
Epoch  74 Batch  304/769   train_loss = 1.338
Epoch  74 Batch  314/769   train_loss = 1.345
Epoch  74 Batch  324/769   train_loss = 1.308
Epoch  74 Batch  334/769   train_loss = 1.324
Epoch  74 Batch  344/769   train_loss = 1.344
Epoch  74 Batch  354/769   train_loss = 1.402
Epoch  74 Batch  364/769   train_loss = 1.276
Epoch  74 Batch  374/769   train_loss = 1.313
Epoch  74 Batch  384/769   train_loss = 1.250
Epoch  74 Batch  394/769   train_loss = 1.374
Epoch  74 Batch  404/769   train_loss = 1.416
Epoch  74 Batch  414/769   train_loss = 1.328
Epoch  74 Batch  424/769   train_loss = 1.243
Epoch  74 Batch  434/769   train_loss = 1.346
Epoch  74 Batch  444/769   train_loss = 1.385
Epoch  74 Batch  454/769   train_loss = 1.356
Epoch  74 Batch  464/769   train_loss = 1.449
Epoch  74 Batch  474/769   train_loss = 1.349
Epoch  74 Batch  484/769   train_loss = 1.328
Epoch  74 Batch  494/769   train_loss = 1.313
Epoch  74 Batch  504/769   train_loss = 1.437
Epoch  74 Batch  514/769   train_loss = 1.370
Epoch  74 Batch  524/769   train_loss = 1.474
Epoch  74 Batch  534/769   train_loss = 1.320
Epoch  74 Batch  544/769   train_loss = 1.310
Epoch  74 Batch  554/769   train_loss = 1.300
Epoch  74 Batch  564/769   train_loss = 1.312
Epoch  74 Batch  574/769   train_loss = 1.315
Epoch  74 Batch  584/769   train_loss = 1.252
Epoch  74 Batch  594/769   train_loss = 1.285
Epoch  74 Batch  604/769   train_loss = 1.302
Epoch  74 Batch  614/769   train_loss = 1.296
Epoch  74 Batch  624/769   train_loss = 1.401
Epoch  74 Batch  634/769   train_loss = 1.275
Epoch  74 Batch  644/769   train_loss = 1.254
Epoch  74 Batch  654/769   train_loss = 1.379
Epoch  74 Batch  664/769   train_loss = 1.358
Epoch  74 Batch  674/769   train_loss = 1.334
Epoch  74 Batch  684/769   train_loss = 1.354
Epoch  74 Batch  694/769   train_loss = 1.365
Epoch  74 Batch  704/769   train_loss = 1.336
Epoch  74 Batch  714/769   train_loss = 1.331
Epoch  74 Batch  724/769   train_loss = 1.311
Epoch  74 Batch  734/769   train_loss = 1.415
Epoch  74 Batch  744/769   train_loss = 1.299
Epoch  74 Batch  754/769   train_loss = 1.331
Epoch  74 Batch  764/769   train_loss = 1.348
Epoch  75 Batch    5/769   train_loss = 1.414
Epoch  75 Batch   15/769   train_loss = 1.339
Epoch  75 Batch   25/769   train_loss = 1.413
Epoch  75 Batch   35/769   train_loss = 1.368
Epoch  75 Batch   45/769   train_loss = 1.363
Epoch  75 Batch   55/769   train_loss = 1.367
Epoch  75 Batch   65/769   train_loss = 1.365
Epoch  75 Batch   75/769   train_loss = 1.409
Epoch  75 Batch   85/769   train_loss = 1.397
Epoch  75 Batch   95/769   train_loss = 1.337
Epoch  75 Batch  105/769   train_loss = 1.387
Epoch  75 Batch  115/769   train_loss = 1.353
Epoch  75 Batch  125/769   train_loss = 1.235
Epoch  75 Batch  135/769   train_loss = 1.355
Epoch  75 Batch  145/769   train_loss = 1.330
Epoch  75 Batch  155/769   train_loss = 1.383
Epoch  75 Batch  165/769   train_loss = 1.279
Epoch  75 Batch  175/769   train_loss = 1.267
Epoch  75 Batch  185/769   train_loss = 1.319
Epoch  75 Batch  195/769   train_loss = 1.330
Epoch  75 Batch  205/769   train_loss = 1.388
Epoch  75 Batch  215/769   train_loss = 1.352
Epoch  75 Batch  225/769   train_loss = 1.275
Epoch  75 Batch  235/769   train_loss = 1.337
Epoch  75 Batch  245/769   train_loss = 1.324
Epoch  75 Batch  255/769   train_loss = 1.353
Epoch  75 Batch  265/769   train_loss = 1.245
Epoch  75 Batch  275/769   train_loss = 1.294
Epoch  75 Batch  285/769   train_loss = 1.416
Epoch  75 Batch  295/769   train_loss = 1.362
Epoch  75 Batch  305/769   train_loss = 1.343
Epoch  75 Batch  315/769   train_loss = 1.283
Epoch  75 Batch  325/769   train_loss = 1.293
Epoch  75 Batch  335/769   train_loss = 1.362
Epoch  75 Batch  345/769   train_loss = 1.279
Epoch  75 Batch  355/769   train_loss = 1.285
Epoch  75 Batch  365/769   train_loss = 1.298
Epoch  75 Batch  375/769   train_loss = 1.383
Epoch  75 Batch  385/769   train_loss = 1.224
Epoch  75 Batch  395/769   train_loss = 1.372
Epoch  75 Batch  405/769   train_loss = 1.361
Epoch  75 Batch  415/769   train_loss = 1.214
Epoch  75 Batch  425/769   train_loss = 1.264
Epoch  75 Batch  435/769   train_loss = 1.290
Epoch  75 Batch  445/769   train_loss = 1.348
Epoch  75 Batch  455/769   train_loss = 1.357
Epoch  75 Batch  465/769   train_loss = 1.432
Epoch  75 Batch  475/769   train_loss = 1.270
Epoch  75 Batch  485/769   train_loss = 1.338
Epoch  75 Batch  495/769   train_loss = 1.315
Epoch  75 Batch  505/769   train_loss = 1.327
Epoch  75 Batch  515/769   train_loss = 1.362
Epoch  75 Batch  525/769   train_loss = 1.299
Epoch  75 Batch  535/769   train_loss = 1.311
Epoch  75 Batch  545/769   train_loss = 1.381
Epoch  75 Batch  555/769   train_loss = 1.362
Epoch  75 Batch  565/769   train_loss = 1.384
Epoch  75 Batch  575/769   train_loss = 1.372
Epoch  75 Batch  585/769   train_loss = 1.289
Epoch  75 Batch  595/769   train_loss = 1.269
Epoch  75 Batch  605/769   train_loss = 1.299
Epoch  75 Batch  615/769   train_loss = 1.323
Epoch  75 Batch  625/769   train_loss = 1.320
Epoch  75 Batch  635/769   train_loss = 1.259
Epoch  75 Batch  645/769   train_loss = 1.247
Epoch  75 Batch  655/769   train_loss = 1.331
Epoch  75 Batch  665/769   train_loss = 1.323
Epoch  75 Batch  675/769   train_loss = 1.338
Epoch  75 Batch  685/769   train_loss = 1.347
Epoch  75 Batch  695/769   train_loss = 1.361
Epoch  75 Batch  705/769   train_loss = 1.320
Epoch  75 Batch  715/769   train_loss = 1.342
Epoch  75 Batch  725/769   train_loss = 1.284
Epoch  75 Batch  735/769   train_loss = 1.312
Epoch  75 Batch  745/769   train_loss = 1.340
Epoch  75 Batch  755/769   train_loss = 1.278
Epoch  75 Batch  765/769   train_loss = 1.316
Epoch  76 Batch    6/769   train_loss = 1.304
Epoch  76 Batch   16/769   train_loss = 1.340
Epoch  76 Batch   26/769   train_loss = 1.320
Epoch  76 Batch   36/769   train_loss = 1.347
Epoch  76 Batch   46/769   train_loss = 1.395
Epoch  76 Batch   56/769   train_loss = 1.341
Epoch  76 Batch   66/769   train_loss = 1.358
Epoch  76 Batch   76/769   train_loss = 1.409
Epoch  76 Batch   86/769   train_loss = 1.347
Epoch  76 Batch   96/769   train_loss = 1.292
Epoch  76 Batch  106/769   train_loss = 1.374
Epoch  76 Batch  116/769   train_loss = 1.335
Epoch  76 Batch  126/769   train_loss = 1.315
Epoch  76 Batch  136/769   train_loss = 1.277
Epoch  76 Batch  146/769   train_loss = 1.344
Epoch  76 Batch  156/769   train_loss = 1.358
Epoch  76 Batch  166/769   train_loss = 1.282
Epoch  76 Batch  176/769   train_loss = 1.278
Epoch  76 Batch  186/769   train_loss = 1.290
Epoch  76 Batch  196/769   train_loss = 1.317
Epoch  76 Batch  206/769   train_loss = 1.332
Epoch  76 Batch  216/769   train_loss = 1.312
Epoch  76 Batch  226/769   train_loss = 1.334
Epoch  76 Batch  236/769   train_loss = 1.342
Epoch  76 Batch  246/769   train_loss = 1.332
Epoch  76 Batch  256/769   train_loss = 1.316
Epoch  76 Batch  266/769   train_loss = 1.343
Epoch  76 Batch  276/769   train_loss = 1.296
Epoch  76 Batch  286/769   train_loss = 1.314
Epoch  76 Batch  296/769   train_loss = 1.276
Epoch  76 Batch  306/769   train_loss = 1.311
Epoch  76 Batch  316/769   train_loss = 1.270
Epoch  76 Batch  326/769   train_loss = 1.375
Epoch  76 Batch  336/769   train_loss = 1.346
Epoch  76 Batch  346/769   train_loss = 1.322
Epoch  76 Batch  356/769   train_loss = 1.322
Epoch  76 Batch  366/769   train_loss = 1.334
Epoch  76 Batch  376/769   train_loss = 1.267
Epoch  76 Batch  386/769   train_loss = 1.312
Epoch  76 Batch  396/769   train_loss = 1.372
Epoch  76 Batch  406/769   train_loss = 1.296
Epoch  76 Batch  416/769   train_loss = 1.298
Epoch  76 Batch  426/769   train_loss = 1.401
Epoch  76 Batch  436/769   train_loss = 1.338
Epoch  76 Batch  446/769   train_loss = 1.316
Epoch  76 Batch  456/769   train_loss = 1.396
Epoch  76 Batch  466/769   train_loss = 1.355
Epoch  76 Batch  476/769   train_loss = 1.315
Epoch  76 Batch  486/769   train_loss = 1.373
Epoch  76 Batch  496/769   train_loss = 1.304
Epoch  76 Batch  506/769   train_loss = 1.381
Epoch  76 Batch  516/769   train_loss = 1.283
Epoch  76 Batch  526/769   train_loss = 1.245
Epoch  76 Batch  536/769   train_loss = 1.313
Epoch  76 Batch  546/769   train_loss = 1.320
Epoch  76 Batch  556/769   train_loss = 1.368
Epoch  76 Batch  566/769   train_loss = 1.403
Epoch  76 Batch  576/769   train_loss = 1.277
Epoch  76 Batch  586/769   train_loss = 1.265
Epoch  76 Batch  596/769   train_loss = 1.359
Epoch  76 Batch  606/769   train_loss = 1.339
Epoch  76 Batch  616/769   train_loss = 1.290
Epoch  76 Batch  626/769   train_loss = 1.323
Epoch  76 Batch  636/769   train_loss = 1.275
Epoch  76 Batch  646/769   train_loss = 1.385
Epoch  76 Batch  656/769   train_loss = 1.279
Epoch  76 Batch  666/769   train_loss = 1.356
Epoch  76 Batch  676/769   train_loss = 1.353
Epoch  76 Batch  686/769   train_loss = 1.316
Epoch  76 Batch  696/769   train_loss = 1.272
Epoch  76 Batch  706/769   train_loss = 1.357
Epoch  76 Batch  716/769   train_loss = 1.301
Epoch  76 Batch  726/769   train_loss = 1.305
Epoch  76 Batch  736/769   train_loss = 1.340
Epoch  76 Batch  746/769   train_loss = 1.285
Epoch  76 Batch  756/769   train_loss = 1.302
Epoch  76 Batch  766/769   train_loss = 1.342
Epoch  77 Batch    7/769   train_loss = 1.368
Epoch  77 Batch   17/769   train_loss = 1.355
Epoch  77 Batch   27/769   train_loss = 1.302
Epoch  77 Batch   37/769   train_loss = 1.304
Epoch  77 Batch   47/769   train_loss = 1.315
Epoch  77 Batch   57/769   train_loss = 1.390
Epoch  77 Batch   67/769   train_loss = 1.338
Epoch  77 Batch   77/769   train_loss = 1.358
Epoch  77 Batch   87/769   train_loss = 1.364
Epoch  77 Batch   97/769   train_loss = 1.309
Epoch  77 Batch  107/769   train_loss = 1.308
Epoch  77 Batch  117/769   train_loss = 1.298
Epoch  77 Batch  127/769   train_loss = 1.391
Epoch  77 Batch  137/769   train_loss = 1.287
Epoch  77 Batch  147/769   train_loss = 1.315
Epoch  77 Batch  157/769   train_loss = 1.401
Epoch  77 Batch  167/769   train_loss = 1.282
Epoch  77 Batch  177/769   train_loss = 1.266
Epoch  77 Batch  187/769   train_loss = 1.369
Epoch  77 Batch  197/769   train_loss = 1.289
Epoch  77 Batch  207/769   train_loss = 1.241
Epoch  77 Batch  217/769   train_loss = 1.321
Epoch  77 Batch  227/769   train_loss = 1.305
Epoch  77 Batch  237/769   train_loss = 1.270
Epoch  77 Batch  247/769   train_loss = 1.302
Epoch  77 Batch  257/769   train_loss = 1.306
Epoch  77 Batch  267/769   train_loss = 1.313
Epoch  77 Batch  277/769   train_loss = 1.314
Epoch  77 Batch  287/769   train_loss = 1.291
Epoch  77 Batch  297/769   train_loss = 1.298
Epoch  77 Batch  307/769   train_loss = 1.376
Epoch  77 Batch  317/769   train_loss = 1.217
Epoch  77 Batch  327/769   train_loss = 1.324
Epoch  77 Batch  337/769   train_loss = 1.325
Epoch  77 Batch  347/769   train_loss = 1.307
Epoch  77 Batch  357/769   train_loss = 1.292
Epoch  77 Batch  367/769   train_loss = 1.320
Epoch  77 Batch  377/769   train_loss = 1.323
Epoch  77 Batch  387/769   train_loss = 1.369
Epoch  77 Batch  397/769   train_loss = 1.274
Epoch  77 Batch  407/769   train_loss = 1.348
Epoch  77 Batch  417/769   train_loss = 1.350
Epoch  77 Batch  427/769   train_loss = 1.344
Epoch  77 Batch  437/769   train_loss = 1.364
Epoch  77 Batch  447/769   train_loss = 1.353
Epoch  77 Batch  457/769   train_loss = 1.286
Epoch  77 Batch  467/769   train_loss = 1.333
Epoch  77 Batch  477/769   train_loss = 1.346
Epoch  77 Batch  487/769   train_loss = 1.313
Epoch  77 Batch  497/769   train_loss = 1.323
Epoch  77 Batch  507/769   train_loss = 1.300
Epoch  77 Batch  517/769   train_loss = 1.346
Epoch  77 Batch  527/769   train_loss = 1.296
Epoch  77 Batch  537/769   train_loss = 1.373
Epoch  77 Batch  547/769   train_loss = 1.381
Epoch  77 Batch  557/769   train_loss = 1.301
Epoch  77 Batch  567/769   train_loss = 1.325
Epoch  77 Batch  577/769   train_loss = 1.370
Epoch  77 Batch  587/769   train_loss = 1.289
Epoch  77 Batch  597/769   train_loss = 1.213
Epoch  77 Batch  607/769   train_loss = 1.297
Epoch  77 Batch  617/769   train_loss = 1.322
Epoch  77 Batch  627/769   train_loss = 1.357
Epoch  77 Batch  637/769   train_loss = 1.251
Epoch  77 Batch  647/769   train_loss = 1.361
Epoch  77 Batch  657/769   train_loss = 1.270
Epoch  77 Batch  667/769   train_loss = 1.368
Epoch  77 Batch  677/769   train_loss = 1.282
Epoch  77 Batch  687/769   train_loss = 1.338
Epoch  77 Batch  697/769   train_loss = 1.363
Epoch  77 Batch  707/769   train_loss = 1.322
Epoch  77 Batch  717/769   train_loss = 1.326
Epoch  77 Batch  727/769   train_loss = 1.340
Epoch  77 Batch  737/769   train_loss = 1.394
Epoch  77 Batch  747/769   train_loss = 1.290
Epoch  77 Batch  757/769   train_loss = 1.356
Epoch  77 Batch  767/769   train_loss = 1.338
Epoch  78 Batch    8/769   train_loss = 1.337
Epoch  78 Batch   18/769   train_loss = 1.328
Epoch  78 Batch   28/769   train_loss = 1.334
Epoch  78 Batch   38/769   train_loss = 1.390
Epoch  78 Batch   48/769   train_loss = 1.369
Epoch  78 Batch   58/769   train_loss = 1.339
Epoch  78 Batch   68/769   train_loss = 1.397
Epoch  78 Batch   78/769   train_loss = 1.338
Epoch  78 Batch   88/769   train_loss = 1.328
Epoch  78 Batch   98/769   train_loss = 1.279
Epoch  78 Batch  108/769   train_loss = 1.370
Epoch  78 Batch  118/769   train_loss = 1.335
Epoch  78 Batch  128/769   train_loss = 1.350
Epoch  78 Batch  138/769   train_loss = 1.268
Epoch  78 Batch  148/769   train_loss = 1.313
Epoch  78 Batch  158/769   train_loss = 1.362
Epoch  78 Batch  168/769   train_loss = 1.232
Epoch  78 Batch  178/769   train_loss = 1.346
Epoch  78 Batch  188/769   train_loss = 1.312
Epoch  78 Batch  198/769   train_loss = 1.310
Epoch  78 Batch  208/769   train_loss = 1.324
Epoch  78 Batch  218/769   train_loss = 1.328
Epoch  78 Batch  228/769   train_loss = 1.310
Epoch  78 Batch  238/769   train_loss = 1.319
Epoch  78 Batch  248/769   train_loss = 1.319
Epoch  78 Batch  258/769   train_loss = 1.285
Epoch  78 Batch  268/769   train_loss = 1.315
Epoch  78 Batch  278/769   train_loss = 1.346
Epoch  78 Batch  288/769   train_loss = 1.287
Epoch  78 Batch  298/769   train_loss = 1.361
Epoch  78 Batch  308/769   train_loss = 1.337
Epoch  78 Batch  318/769   train_loss = 1.264
Epoch  78 Batch  328/769   train_loss = 1.302
Epoch  78 Batch  338/769   train_loss = 1.279
Epoch  78 Batch  348/769   train_loss = 1.264
Epoch  78 Batch  358/769   train_loss = 1.318
Epoch  78 Batch  368/769   train_loss = 1.322
Epoch  78 Batch  378/769   train_loss = 1.310
Epoch  78 Batch  388/769   train_loss = 1.281
Epoch  78 Batch  398/769   train_loss = 1.369
Epoch  78 Batch  408/769   train_loss = 1.317
Epoch  78 Batch  418/769   train_loss = 1.297
Epoch  78 Batch  428/769   train_loss = 1.276
Epoch  78 Batch  438/769   train_loss = 1.258
Epoch  78 Batch  448/769   train_loss = 1.355
Epoch  78 Batch  458/769   train_loss = 1.405
Epoch  78 Batch  468/769   train_loss = 1.335
Epoch  78 Batch  478/769   train_loss = 1.247
Epoch  78 Batch  488/769   train_loss = 1.328
Epoch  78 Batch  498/769   train_loss = 1.297
Epoch  78 Batch  508/769   train_loss = 1.286
Epoch  78 Batch  518/769   train_loss = 1.356
Epoch  78 Batch  528/769   train_loss = 1.271
Epoch  78 Batch  538/769   train_loss = 1.307
Epoch  78 Batch  548/769   train_loss = 1.277
Epoch  78 Batch  558/769   train_loss = 1.343
Epoch  78 Batch  568/769   train_loss = 1.310
Epoch  78 Batch  578/769   train_loss = 1.208
Epoch  78 Batch  588/769   train_loss = 1.228
Epoch  78 Batch  598/769   train_loss = 1.300
Epoch  78 Batch  608/769   train_loss = 1.253
Epoch  78 Batch  618/769   train_loss = 1.255
Epoch  78 Batch  628/769   train_loss = 1.327
Epoch  78 Batch  638/769   train_loss = 1.257
Epoch  78 Batch  648/769   train_loss = 1.342
Epoch  78 Batch  658/769   train_loss = 1.377
Epoch  78 Batch  668/769   train_loss = 1.313
Epoch  78 Batch  678/769   train_loss = 1.309
Epoch  78 Batch  688/769   train_loss = 1.306
Epoch  78 Batch  698/769   train_loss = 1.296
Epoch  78 Batch  708/769   train_loss = 1.304
Epoch  78 Batch  718/769   train_loss = 1.275
Epoch  78 Batch  728/769   train_loss = 1.319
Epoch  78 Batch  738/769   train_loss = 1.338
Epoch  78 Batch  748/769   train_loss = 1.338
Epoch  78 Batch  758/769   train_loss = 1.270
Epoch  78 Batch  768/769   train_loss = 1.312
Epoch  79 Batch    9/769   train_loss = 1.333
Epoch  79 Batch   19/769   train_loss = 1.402
Epoch  79 Batch   29/769   train_loss = 1.343
Epoch  79 Batch   39/769   train_loss = 1.282
Epoch  79 Batch   49/769   train_loss = 1.320
Epoch  79 Batch   59/769   train_loss = 1.325
Epoch  79 Batch   69/769   train_loss = 1.340
Epoch  79 Batch   79/769   train_loss = 1.323
Epoch  79 Batch   89/769   train_loss = 1.263
Epoch  79 Batch   99/769   train_loss = 1.348
Epoch  79 Batch  109/769   train_loss = 1.329
Epoch  79 Batch  119/769   train_loss = 1.265
Epoch  79 Batch  129/769   train_loss = 1.258
Epoch  79 Batch  139/769   train_loss = 1.311
Epoch  79 Batch  149/769   train_loss = 1.302
Epoch  79 Batch  159/769   train_loss = 1.267
Epoch  79 Batch  169/769   train_loss = 1.329
Epoch  79 Batch  179/769   train_loss = 1.305
Epoch  79 Batch  189/769   train_loss = 1.322
Epoch  79 Batch  199/769   train_loss = 1.318
Epoch  79 Batch  209/769   train_loss = 1.300
Epoch  79 Batch  219/769   train_loss = 1.289
Epoch  79 Batch  229/769   train_loss = 1.328
Epoch  79 Batch  239/769   train_loss = 1.283
Epoch  79 Batch  249/769   train_loss = 1.275
Epoch  79 Batch  259/769   train_loss = 1.266
Epoch  79 Batch  269/769   train_loss = 1.310
Epoch  79 Batch  279/769   train_loss = 1.308
Epoch  79 Batch  289/769   train_loss = 1.238
Epoch  79 Batch  299/769   train_loss = 1.401
Epoch  79 Batch  309/769   train_loss = 1.335
Epoch  79 Batch  319/769   train_loss = 1.370
Epoch  79 Batch  329/769   train_loss = 1.297
Epoch  79 Batch  339/769   train_loss = 1.287
Epoch  79 Batch  349/769   train_loss = 1.273
Epoch  79 Batch  359/769   train_loss = 1.347
Epoch  79 Batch  369/769   train_loss = 1.270
Epoch  79 Batch  379/769   train_loss = 1.217
Epoch  79 Batch  389/769   train_loss = 1.216
Epoch  79 Batch  399/769   train_loss = 1.325
Epoch  79 Batch  409/769   train_loss = 1.360
Epoch  79 Batch  419/769   train_loss = 1.399
Epoch  79 Batch  429/769   train_loss = 1.295
Epoch  79 Batch  439/769   train_loss = 1.351
Epoch  79 Batch  449/769   train_loss = 1.343
Epoch  79 Batch  459/769   train_loss = 1.340
Epoch  79 Batch  469/769   train_loss = 1.260
Epoch  79 Batch  479/769   train_loss = 1.329
Epoch  79 Batch  489/769   train_loss = 1.399
Epoch  79 Batch  499/769   train_loss = 1.338
Epoch  79 Batch  509/769   train_loss = 1.266
Epoch  79 Batch  519/769   train_loss = 1.361
Epoch  79 Batch  529/769   train_loss = 1.392
Epoch  79 Batch  539/769   train_loss = 1.377
Epoch  79 Batch  549/769   train_loss = 1.369
Epoch  79 Batch  559/769   train_loss = 1.343
Epoch  79 Batch  569/769   train_loss = 1.260
Epoch  79 Batch  579/769   train_loss = 1.220
Epoch  79 Batch  589/769   train_loss = 1.299
Epoch  79 Batch  599/769   train_loss = 1.270
Epoch  79 Batch  609/769   train_loss = 1.315
Epoch  79 Batch  619/769   train_loss = 1.342
Epoch  79 Batch  629/769   train_loss = 1.285
Epoch  79 Batch  639/769   train_loss = 1.265
Epoch  79 Batch  649/769   train_loss = 1.269
Epoch  79 Batch  659/769   train_loss = 1.304
Epoch  79 Batch  669/769   train_loss = 1.308
Epoch  79 Batch  679/769   train_loss = 1.253
Epoch  79 Batch  689/769   train_loss = 1.349
Epoch  79 Batch  699/769   train_loss = 1.282
Epoch  79 Batch  709/769   train_loss = 1.338
Epoch  79 Batch  719/769   train_loss = 1.297
Epoch  79 Batch  729/769   train_loss = 1.279
Epoch  79 Batch  739/769   train_loss = 1.257
Epoch  79 Batch  749/769   train_loss = 1.260
Epoch  79 Batch  759/769   train_loss = 1.296
Epoch  80 Batch    0/769   train_loss = 1.254
Epoch  80 Batch   10/769   train_loss = 1.349
Epoch  80 Batch   20/769   train_loss = 1.316
Epoch  80 Batch   30/769   train_loss = 1.282
Epoch  80 Batch   40/769   train_loss = 1.302
Epoch  80 Batch   50/769   train_loss = 1.299
Epoch  80 Batch   60/769   train_loss = 1.429
Epoch  80 Batch   70/769   train_loss = 1.344
Epoch  80 Batch   80/769   train_loss = 1.283
Epoch  80 Batch   90/769   train_loss = 1.335
Epoch  80 Batch  100/769   train_loss = 1.313
Epoch  80 Batch  110/769   train_loss = 1.308
Epoch  80 Batch  120/769   train_loss = 1.242
Epoch  80 Batch  130/769   train_loss = 1.279
Epoch  80 Batch  140/769   train_loss = 1.357
Epoch  80 Batch  150/769   train_loss = 1.290
Epoch  80 Batch  160/769   train_loss = 1.297
Epoch  80 Batch  170/769   train_loss = 1.249
Epoch  80 Batch  180/769   train_loss = 1.271
Epoch  80 Batch  190/769   train_loss = 1.257
Epoch  80 Batch  200/769   train_loss = 1.338
Epoch  80 Batch  210/769   train_loss = 1.240
Epoch  80 Batch  220/769   train_loss = 1.236
Epoch  80 Batch  230/769   train_loss = 1.253
Epoch  80 Batch  240/769   train_loss = 1.237
Epoch  80 Batch  250/769   train_loss = 1.231
Epoch  80 Batch  260/769   train_loss = 1.288
Epoch  80 Batch  270/769   train_loss = 1.260
Epoch  80 Batch  280/769   train_loss = 1.294
Epoch  80 Batch  290/769   train_loss = 1.292
Epoch  80 Batch  300/769   train_loss = 1.322
Epoch  80 Batch  310/769   train_loss = 1.251
Epoch  80 Batch  320/769   train_loss = 1.283
Epoch  80 Batch  330/769   train_loss = 1.326
Epoch  80 Batch  340/769   train_loss = 1.225
Epoch  80 Batch  350/769   train_loss = 1.215
Epoch  80 Batch  360/769   train_loss = 1.248
Epoch  80 Batch  370/769   train_loss = 1.227
Epoch  80 Batch  380/769   train_loss = 1.284
Epoch  80 Batch  390/769   train_loss = 1.229
Epoch  80 Batch  400/769   train_loss = 1.278
Epoch  80 Batch  410/769   train_loss = 1.316
Epoch  80 Batch  420/769   train_loss = 1.318
Epoch  80 Batch  430/769   train_loss = 1.305
Epoch  80 Batch  440/769   train_loss = 1.341
Epoch  80 Batch  450/769   train_loss = 1.273
Epoch  80 Batch  460/769   train_loss = 1.418
Epoch  80 Batch  470/769   train_loss = 1.302
Epoch  80 Batch  480/769   train_loss = 1.279
Epoch  80 Batch  490/769   train_loss = 1.285
Epoch  80 Batch  500/769   train_loss = 1.302
Epoch  80 Batch  510/769   train_loss = 1.327
Epoch  80 Batch  520/769   train_loss = 1.228
Epoch  80 Batch  530/769   train_loss = 1.345
Epoch  80 Batch  540/769   train_loss = 1.366
Epoch  80 Batch  550/769   train_loss = 1.242
Epoch  80 Batch  560/769   train_loss = 1.350
Epoch  80 Batch  570/769   train_loss = 1.284
Epoch  80 Batch  580/769   train_loss = 1.226
Epoch  80 Batch  590/769   train_loss = 1.242
Epoch  80 Batch  600/769   train_loss = 1.269
Epoch  80 Batch  610/769   train_loss = 1.262
Epoch  80 Batch  620/769   train_loss = 1.321
Epoch  80 Batch  630/769   train_loss = 1.350
Epoch  80 Batch  640/769   train_loss = 1.207
Epoch  80 Batch  650/769   train_loss = 1.275
Epoch  80 Batch  660/769   train_loss = 1.306
Epoch  80 Batch  670/769   train_loss = 1.306
Epoch  80 Batch  680/769   train_loss = 1.328
Epoch  80 Batch  690/769   train_loss = 1.230
Epoch  80 Batch  700/769   train_loss = 1.292
Epoch  80 Batch  710/769   train_loss = 1.267
Epoch  80 Batch  720/769   train_loss = 1.249
Epoch  80 Batch  730/769   train_loss = 1.257
Epoch  80 Batch  740/769   train_loss = 1.297
Epoch  80 Batch  750/769   train_loss = 1.254
Epoch  80 Batch  760/769   train_loss = 1.302
Epoch  81 Batch    1/769   train_loss = 1.299
Epoch  81 Batch   11/769   train_loss = 1.276
Epoch  81 Batch   21/769   train_loss = 1.287
Epoch  81 Batch   31/769   train_loss = 1.278
Epoch  81 Batch   41/769   train_loss = 1.345
Epoch  81 Batch   51/769   train_loss = 1.334
Epoch  81 Batch   61/769   train_loss = 1.401
Epoch  81 Batch   71/769   train_loss = 1.295
Epoch  81 Batch   81/769   train_loss = 1.407
Epoch  81 Batch   91/769   train_loss = 1.303
Epoch  81 Batch  101/769   train_loss = 1.276
Epoch  81 Batch  111/769   train_loss = 1.299
Epoch  81 Batch  121/769   train_loss = 1.328
Epoch  81 Batch  131/769   train_loss = 1.259
Epoch  81 Batch  141/769   train_loss = 1.309
Epoch  81 Batch  151/769   train_loss = 1.301
Epoch  81 Batch  161/769   train_loss = 1.341
Epoch  81 Batch  171/769   train_loss = 1.301
Epoch  81 Batch  181/769   train_loss = 1.310
Epoch  81 Batch  191/769   train_loss = 1.237
Epoch  81 Batch  201/769   train_loss = 1.293
Epoch  81 Batch  211/769   train_loss = 1.266
Epoch  81 Batch  221/769   train_loss = 1.216
Epoch  81 Batch  231/769   train_loss = 1.278
Epoch  81 Batch  241/769   train_loss = 1.317
Epoch  81 Batch  251/769   train_loss = 1.304
Epoch  81 Batch  261/769   train_loss = 1.226
Epoch  81 Batch  271/769   train_loss = 1.260
Epoch  81 Batch  281/769   train_loss = 1.304
Epoch  81 Batch  291/769   train_loss = 1.310
Epoch  81 Batch  301/769   train_loss = 1.287
Epoch  81 Batch  311/769   train_loss = 1.320
Epoch  81 Batch  321/769   train_loss = 1.301
Epoch  81 Batch  331/769   train_loss = 1.296
Epoch  81 Batch  341/769   train_loss = 1.216
Epoch  81 Batch  351/769   train_loss = 1.341
Epoch  81 Batch  361/769   train_loss = 1.233
Epoch  81 Batch  371/769   train_loss = 1.275
Epoch  81 Batch  381/769   train_loss = 1.200
Epoch  81 Batch  391/769   train_loss = 1.239
Epoch  81 Batch  401/769   train_loss = 1.247
Epoch  81 Batch  411/769   train_loss = 1.260
Epoch  81 Batch  421/769   train_loss = 1.323
Epoch  81 Batch  431/769   train_loss = 1.313
Epoch  81 Batch  441/769   train_loss = 1.238
Epoch  81 Batch  451/769   train_loss = 1.258
Epoch  81 Batch  461/769   train_loss = 1.265
Epoch  81 Batch  471/769   train_loss = 1.288
Epoch  81 Batch  481/769   train_loss = 1.314
Epoch  81 Batch  491/769   train_loss = 1.337
Epoch  81 Batch  501/769   train_loss = 1.353
Epoch  81 Batch  511/769   train_loss = 1.308
Epoch  81 Batch  521/769   train_loss = 1.289
Epoch  81 Batch  531/769   train_loss = 1.288
Epoch  81 Batch  541/769   train_loss = 1.305
Epoch  81 Batch  551/769   train_loss = 1.339
Epoch  81 Batch  561/769   train_loss = 1.366
Epoch  81 Batch  571/769   train_loss = 1.304
Epoch  81 Batch  581/769   train_loss = 1.193
Epoch  81 Batch  591/769   train_loss = 1.253
Epoch  81 Batch  601/769   train_loss = 1.289
Epoch  81 Batch  611/769   train_loss = 1.278
Epoch  81 Batch  621/769   train_loss = 1.277
Epoch  81 Batch  631/769   train_loss = 1.323
Epoch  81 Batch  641/769   train_loss = 1.297
Epoch  81 Batch  651/769   train_loss = 1.232
Epoch  81 Batch  661/769   train_loss = 1.255
Epoch  81 Batch  671/769   train_loss = 1.335
Epoch  81 Batch  681/769   train_loss = 1.256
Epoch  81 Batch  691/769   train_loss = 1.286
Epoch  81 Batch  701/769   train_loss = 1.262
Epoch  81 Batch  711/769   train_loss = 1.316
Epoch  81 Batch  721/769   train_loss = 1.328
Epoch  81 Batch  731/769   train_loss = 1.314
Epoch  81 Batch  741/769   train_loss = 1.251
Epoch  81 Batch  751/769   train_loss = 1.281
Epoch  81 Batch  761/769   train_loss = 1.249
Epoch  82 Batch    2/769   train_loss = 1.332
Epoch  82 Batch   12/769   train_loss = 1.237
Epoch  82 Batch   22/769   train_loss = 1.323
Epoch  82 Batch   32/769   train_loss = 1.385
Epoch  82 Batch   42/769   train_loss = 1.416
Epoch  82 Batch   52/769   train_loss = 1.275
Epoch  82 Batch   62/769   train_loss = 1.338
Epoch  82 Batch   72/769   train_loss = 1.359
Epoch  82 Batch   82/769   train_loss = 1.371
Epoch  82 Batch   92/769   train_loss = 1.330
Epoch  82 Batch  102/769   train_loss = 1.329
Epoch  82 Batch  112/769   train_loss = 1.254
Epoch  82 Batch  122/769   train_loss = 1.254
Epoch  82 Batch  132/769   train_loss = 1.249
Epoch  82 Batch  142/769   train_loss = 1.263
Epoch  82 Batch  152/769   train_loss = 1.226
Epoch  82 Batch  162/769   train_loss = 1.344
Epoch  82 Batch  172/769   train_loss = 1.359
Epoch  82 Batch  182/769   train_loss = 1.241
Epoch  82 Batch  192/769   train_loss = 1.268
Epoch  82 Batch  202/769   train_loss = 1.274
Epoch  82 Batch  212/769   train_loss = 1.236
Epoch  82 Batch  222/769   train_loss = 1.310
Epoch  82 Batch  232/769   train_loss = 1.250
Epoch  82 Batch  242/769   train_loss = 1.317
Epoch  82 Batch  252/769   train_loss = 1.262
Epoch  82 Batch  262/769   train_loss = 1.259
Epoch  82 Batch  272/769   train_loss = 1.290
Epoch  82 Batch  282/769   train_loss = 1.327
Epoch  82 Batch  292/769   train_loss = 1.255
Epoch  82 Batch  302/769   train_loss = 1.285
Epoch  82 Batch  312/769   train_loss = 1.282
Epoch  82 Batch  322/769   train_loss = 1.304
Epoch  82 Batch  332/769   train_loss = 1.289
Epoch  82 Batch  342/769   train_loss = 1.245
Epoch  82 Batch  352/769   train_loss = 1.243
Epoch  82 Batch  362/769   train_loss = 1.253
Epoch  82 Batch  372/769   train_loss = 1.301
Epoch  82 Batch  382/769   train_loss = 1.238
Epoch  82 Batch  392/769   train_loss = 1.299
Epoch  82 Batch  402/769   train_loss = 1.358
Epoch  82 Batch  412/769   train_loss = 1.263
Epoch  82 Batch  422/769   train_loss = 1.214
Epoch  82 Batch  432/769   train_loss = 1.270
Epoch  82 Batch  442/769   train_loss = 1.209
Epoch  82 Batch  452/769   train_loss = 1.258
Epoch  82 Batch  462/769   train_loss = 1.350
Epoch  82 Batch  472/769   train_loss = 1.262
Epoch  82 Batch  482/769   train_loss = 1.281
Epoch  82 Batch  492/769   train_loss = 1.344
Epoch  82 Batch  502/769   train_loss = 1.293
Epoch  82 Batch  512/769   train_loss = 1.279
Epoch  82 Batch  522/769   train_loss = 1.271
Epoch  82 Batch  532/769   train_loss = 1.267
Epoch  82 Batch  542/769   train_loss = 1.284
Epoch  82 Batch  552/769   train_loss = 1.280
Epoch  82 Batch  562/769   train_loss = 1.185
Epoch  82 Batch  572/769   train_loss = 1.266
Epoch  82 Batch  582/769   train_loss = 1.299
Epoch  82 Batch  592/769   train_loss = 1.246
Epoch  82 Batch  602/769   train_loss = 1.262
Epoch  82 Batch  612/769   train_loss = 1.259
Epoch  82 Batch  622/769   train_loss = 1.198
Epoch  82 Batch  632/769   train_loss = 1.282
Epoch  82 Batch  642/769   train_loss = 1.302
Epoch  82 Batch  652/769   train_loss = 1.300
Epoch  82 Batch  662/769   train_loss = 1.279
Epoch  82 Batch  672/769   train_loss = 1.261
Epoch  82 Batch  682/769   train_loss = 1.333
Epoch  82 Batch  692/769   train_loss = 1.263
Epoch  82 Batch  702/769   train_loss = 1.331
Epoch  82 Batch  712/769   train_loss = 1.289
Epoch  82 Batch  722/769   train_loss = 1.303
Epoch  82 Batch  732/769   train_loss = 1.242
Epoch  82 Batch  742/769   train_loss = 1.306
Epoch  82 Batch  752/769   train_loss = 1.347
Epoch  82 Batch  762/769   train_loss = 1.257
Epoch  83 Batch    3/769   train_loss = 1.327
Epoch  83 Batch   13/769   train_loss = 1.344
Epoch  83 Batch   23/769   train_loss = 1.259
Epoch  83 Batch   33/769   train_loss = 1.276
Epoch  83 Batch   43/769   train_loss = 1.332
Epoch  83 Batch   53/769   train_loss = 1.302
Epoch  83 Batch   63/769   train_loss = 1.196
Epoch  83 Batch   73/769   train_loss = 1.373
Epoch  83 Batch   83/769   train_loss = 1.265
Epoch  83 Batch   93/769   train_loss = 1.295
Epoch  83 Batch  103/769   train_loss = 1.274
Epoch  83 Batch  113/769   train_loss = 1.246
Epoch  83 Batch  123/769   train_loss = 1.338
Epoch  83 Batch  133/769   train_loss = 1.275
Epoch  83 Batch  143/769   train_loss = 1.281
Epoch  83 Batch  153/769   train_loss = 1.282
Epoch  83 Batch  163/769   train_loss = 1.297
Epoch  83 Batch  173/769   train_loss = 1.282
Epoch  83 Batch  183/769   train_loss = 1.342
Epoch  83 Batch  193/769   train_loss = 1.248
Epoch  83 Batch  203/769   train_loss = 1.246
Epoch  83 Batch  213/769   train_loss = 1.212
Epoch  83 Batch  223/769   train_loss = 1.281
Epoch  83 Batch  233/769   train_loss = 1.236
Epoch  83 Batch  243/769   train_loss = 1.298
Epoch  83 Batch  253/769   train_loss = 1.240
Epoch  83 Batch  263/769   train_loss = 1.324
Epoch  83 Batch  273/769   train_loss = 1.269
Epoch  83 Batch  283/769   train_loss = 1.318
Epoch  83 Batch  293/769   train_loss = 1.261
Epoch  83 Batch  303/769   train_loss = 1.289
Epoch  83 Batch  313/769   train_loss = 1.328
Epoch  83 Batch  323/769   train_loss = 1.310
Epoch  83 Batch  333/769   train_loss = 1.311
Epoch  83 Batch  343/769   train_loss = 1.267
Epoch  83 Batch  353/769   train_loss = 1.315
Epoch  83 Batch  363/769   train_loss = 1.289
Epoch  83 Batch  373/769   train_loss = 1.362
Epoch  83 Batch  383/769   train_loss = 1.281
Epoch  83 Batch  393/769   train_loss = 1.309
Epoch  83 Batch  403/769   train_loss = 1.363
Epoch  83 Batch  413/769   train_loss = 1.338
Epoch  83 Batch  423/769   train_loss = 1.248
Epoch  83 Batch  433/769   train_loss = 1.271
Epoch  83 Batch  443/769   train_loss = 1.265
Epoch  83 Batch  453/769   train_loss = 1.306
Epoch  83 Batch  463/769   train_loss = 1.255
Epoch  83 Batch  473/769   train_loss = 1.246
Epoch  83 Batch  483/769   train_loss = 1.247
Epoch  83 Batch  493/769   train_loss = 1.292
Epoch  83 Batch  503/769   train_loss = 1.355
Epoch  83 Batch  513/769   train_loss = 1.318
Epoch  83 Batch  523/769   train_loss = 1.221
Epoch  83 Batch  533/769   train_loss = 1.299
Epoch  83 Batch  543/769   train_loss = 1.297
Epoch  83 Batch  553/769   train_loss = 1.260
Epoch  83 Batch  563/769   train_loss = 1.278
Epoch  83 Batch  573/769   train_loss = 1.259
Epoch  83 Batch  583/769   train_loss = 1.266
Epoch  83 Batch  593/769   train_loss = 1.322
Epoch  83 Batch  603/769   train_loss = 1.214
Epoch  83 Batch  613/769   train_loss = 1.250
Epoch  83 Batch  623/769   train_loss = 1.253
Epoch  83 Batch  633/769   train_loss = 1.235
Epoch  83 Batch  643/769   train_loss = 1.165
Epoch  83 Batch  653/769   train_loss = 1.274
Epoch  83 Batch  663/769   train_loss = 1.243
Epoch  83 Batch  673/769   train_loss = 1.296
Epoch  83 Batch  683/769   train_loss = 1.284
Epoch  83 Batch  693/769   train_loss = 1.279
Epoch  83 Batch  703/769   train_loss = 1.244
Epoch  83 Batch  713/769   train_loss = 1.285
Epoch  83 Batch  723/769   train_loss = 1.234
Epoch  83 Batch  733/769   train_loss = 1.287
Epoch  83 Batch  743/769   train_loss = 1.303
Epoch  83 Batch  753/769   train_loss = 1.265
Epoch  83 Batch  763/769   train_loss = 1.325
Epoch  84 Batch    4/769   train_loss = 1.319
Epoch  84 Batch   14/769   train_loss = 1.292
Epoch  84 Batch   24/769   train_loss = 1.288
Epoch  84 Batch   34/769   train_loss = 1.266
Epoch  84 Batch   44/769   train_loss = 1.291
Epoch  84 Batch   54/769   train_loss = 1.264
Epoch  84 Batch   64/769   train_loss = 1.304
Epoch  84 Batch   74/769   train_loss = 1.354
Epoch  84 Batch   84/769   train_loss = 1.292
Epoch  84 Batch   94/769   train_loss = 1.254
Epoch  84 Batch  104/769   train_loss = 1.365
Epoch  84 Batch  114/769   train_loss = 1.287
Epoch  84 Batch  124/769   train_loss = 1.260
Epoch  84 Batch  134/769   train_loss = 1.242
Epoch  84 Batch  144/769   train_loss = 1.253
Epoch  84 Batch  154/769   train_loss = 1.263
Epoch  84 Batch  164/769   train_loss = 1.309
Epoch  84 Batch  174/769   train_loss = 1.274
Epoch  84 Batch  184/769   train_loss = 1.290
Epoch  84 Batch  194/769   train_loss = 1.166
Epoch  84 Batch  204/769   train_loss = 1.267
Epoch  84 Batch  214/769   train_loss = 1.295
Epoch  84 Batch  224/769   train_loss = 1.263
Epoch  84 Batch  234/769   train_loss = 1.304
Epoch  84 Batch  244/769   train_loss = 1.331
Epoch  84 Batch  254/769   train_loss = 1.212
Epoch  84 Batch  264/769   train_loss = 1.261
Epoch  84 Batch  274/769   train_loss = 1.240
Epoch  84 Batch  284/769   train_loss = 1.204
Epoch  84 Batch  294/769   train_loss = 1.217
Epoch  84 Batch  304/769   train_loss = 1.273
Epoch  84 Batch  314/769   train_loss = 1.271
Epoch  84 Batch  324/769   train_loss = 1.270
Epoch  84 Batch  334/769   train_loss = 1.264
Epoch  84 Batch  344/769   train_loss = 1.278
Epoch  84 Batch  354/769   train_loss = 1.344
Epoch  84 Batch  364/769   train_loss = 1.233
Epoch  84 Batch  374/769   train_loss = 1.243
Epoch  84 Batch  384/769   train_loss = 1.213
Epoch  84 Batch  394/769   train_loss = 1.275
Epoch  84 Batch  404/769   train_loss = 1.359
Epoch  84 Batch  414/769   train_loss = 1.269
Epoch  84 Batch  424/769   train_loss = 1.183
Epoch  84 Batch  434/769   train_loss = 1.280
Epoch  84 Batch  444/769   train_loss = 1.300
Epoch  84 Batch  454/769   train_loss = 1.283
Epoch  84 Batch  464/769   train_loss = 1.360
Epoch  84 Batch  474/769   train_loss = 1.277
Epoch  84 Batch  484/769   train_loss = 1.240
Epoch  84 Batch  494/769   train_loss = 1.268
Epoch  84 Batch  504/769   train_loss = 1.366
Epoch  84 Batch  514/769   train_loss = 1.306
Epoch  84 Batch  524/769   train_loss = 1.424
Epoch  84 Batch  534/769   train_loss = 1.256
Epoch  84 Batch  544/769   train_loss = 1.264
Epoch  84 Batch  554/769   train_loss = 1.219
Epoch  84 Batch  564/769   train_loss = 1.240
Epoch  84 Batch  574/769   train_loss = 1.249
Epoch  84 Batch  584/769   train_loss = 1.189
Epoch  84 Batch  594/769   train_loss = 1.209
Epoch  84 Batch  604/769   train_loss = 1.242
Epoch  84 Batch  614/769   train_loss = 1.217
Epoch  84 Batch  624/769   train_loss = 1.300
Epoch  84 Batch  634/769   train_loss = 1.225
Epoch  84 Batch  644/769   train_loss = 1.214
Epoch  84 Batch  654/769   train_loss = 1.304
Epoch  84 Batch  664/769   train_loss = 1.285
Epoch  84 Batch  674/769   train_loss = 1.250
Epoch  84 Batch  684/769   train_loss = 1.267
Epoch  84 Batch  694/769   train_loss = 1.280
Epoch  84 Batch  704/769   train_loss = 1.285
Epoch  84 Batch  714/769   train_loss = 1.247
Epoch  84 Batch  724/769   train_loss = 1.256
Epoch  84 Batch  734/769   train_loss = 1.338
Epoch  84 Batch  744/769   train_loss = 1.251
Epoch  84 Batch  754/769   train_loss = 1.263
Epoch  84 Batch  764/769   train_loss = 1.226
Epoch  85 Batch    5/769   train_loss = 1.301
Epoch  85 Batch   15/769   train_loss = 1.249
Epoch  85 Batch   25/769   train_loss = 1.343
Epoch  85 Batch   35/769   train_loss = 1.302
Epoch  85 Batch   45/769   train_loss = 1.290
Epoch  85 Batch   55/769   train_loss = 1.317
Epoch  85 Batch   65/769   train_loss = 1.277
Epoch  85 Batch   75/769   train_loss = 1.355
Epoch  85 Batch   85/769   train_loss = 1.337
Epoch  85 Batch   95/769   train_loss = 1.274
Epoch  85 Batch  105/769   train_loss = 1.303
Epoch  85 Batch  115/769   train_loss = 1.295
Epoch  85 Batch  125/769   train_loss = 1.190
Epoch  85 Batch  135/769   train_loss = 1.284
Epoch  85 Batch  145/769   train_loss = 1.286
Epoch  85 Batch  155/769   train_loss = 1.308
Epoch  85 Batch  165/769   train_loss = 1.257
Epoch  85 Batch  175/769   train_loss = 1.194
Epoch  85 Batch  185/769   train_loss = 1.264
Epoch  85 Batch  195/769   train_loss = 1.268
Epoch  85 Batch  205/769   train_loss = 1.335
Epoch  85 Batch  215/769   train_loss = 1.281
Epoch  85 Batch  225/769   train_loss = 1.205
Epoch  85 Batch  235/769   train_loss = 1.250
Epoch  85 Batch  245/769   train_loss = 1.274
Epoch  85 Batch  255/769   train_loss = 1.293
Epoch  85 Batch  265/769   train_loss = 1.199
Epoch  85 Batch  275/769   train_loss = 1.217
Epoch  85 Batch  285/769   train_loss = 1.342
Epoch  85 Batch  295/769   train_loss = 1.267
Epoch  85 Batch  305/769   train_loss = 1.258
Epoch  85 Batch  315/769   train_loss = 1.234
Epoch  85 Batch  325/769   train_loss = 1.245
Epoch  85 Batch  335/769   train_loss = 1.284
Epoch  85 Batch  345/769   train_loss = 1.220
Epoch  85 Batch  355/769   train_loss = 1.218
Epoch  85 Batch  365/769   train_loss = 1.282
Epoch  85 Batch  375/769   train_loss = 1.302
Epoch  85 Batch  385/769   train_loss = 1.156
Epoch  85 Batch  395/769   train_loss = 1.318
Epoch  85 Batch  405/769   train_loss = 1.296
Epoch  85 Batch  415/769   train_loss = 1.157
Epoch  85 Batch  425/769   train_loss = 1.204
Epoch  85 Batch  435/769   train_loss = 1.225
Epoch  85 Batch  445/769   train_loss = 1.282
Epoch  85 Batch  455/769   train_loss = 1.311
Epoch  85 Batch  465/769   train_loss = 1.353
Epoch  85 Batch  475/769   train_loss = 1.200
Epoch  85 Batch  485/769   train_loss = 1.256
Epoch  85 Batch  495/769   train_loss = 1.303
Epoch  85 Batch  505/769   train_loss = 1.299
Epoch  85 Batch  515/769   train_loss = 1.328
Epoch  85 Batch  525/769   train_loss = 1.239
Epoch  85 Batch  535/769   train_loss = 1.259
Epoch  85 Batch  545/769   train_loss = 1.304
Epoch  85 Batch  555/769   train_loss = 1.290
Epoch  85 Batch  565/769   train_loss = 1.287
Epoch  85 Batch  575/769   train_loss = 1.291
Epoch  85 Batch  585/769   train_loss = 1.234
Epoch  85 Batch  595/769   train_loss = 1.215
Epoch  85 Batch  605/769   train_loss = 1.252
Epoch  85 Batch  615/769   train_loss = 1.267
Epoch  85 Batch  625/769   train_loss = 1.236
Epoch  85 Batch  635/769   train_loss = 1.228
Epoch  85 Batch  645/769   train_loss = 1.193
Epoch  85 Batch  655/769   train_loss = 1.271
Epoch  85 Batch  665/769   train_loss = 1.246
Epoch  85 Batch  675/769   train_loss = 1.289
Epoch  85 Batch  685/769   train_loss = 1.286
Epoch  85 Batch  695/769   train_loss = 1.328
Epoch  85 Batch  705/769   train_loss = 1.286
Epoch  85 Batch  715/769   train_loss = 1.290
Epoch  85 Batch  725/769   train_loss = 1.190
Epoch  85 Batch  735/769   train_loss = 1.253
Epoch  85 Batch  745/769   train_loss = 1.253
Epoch  85 Batch  755/769   train_loss = 1.222
Epoch  85 Batch  765/769   train_loss = 1.240
Epoch  86 Batch    6/769   train_loss = 1.227
Epoch  86 Batch   16/769   train_loss = 1.248
Epoch  86 Batch   26/769   train_loss = 1.264
Epoch  86 Batch   36/769   train_loss = 1.298
Epoch  86 Batch   46/769   train_loss = 1.324
Epoch  86 Batch   56/769   train_loss = 1.290
Epoch  86 Batch   66/769   train_loss = 1.313
Epoch  86 Batch   76/769   train_loss = 1.348
Epoch  86 Batch   86/769   train_loss = 1.296
Epoch  86 Batch   96/769   train_loss = 1.248
Epoch  86 Batch  106/769   train_loss = 1.298
Epoch  86 Batch  116/769   train_loss = 1.243
Epoch  86 Batch  126/769   train_loss = 1.284
Epoch  86 Batch  136/769   train_loss = 1.236
Epoch  86 Batch  146/769   train_loss = 1.281
Epoch  86 Batch  156/769   train_loss = 1.266
Epoch  86 Batch  166/769   train_loss = 1.221
Epoch  86 Batch  176/769   train_loss = 1.220
Epoch  86 Batch  186/769   train_loss = 1.240
Epoch  86 Batch  196/769   train_loss = 1.248
Epoch  86 Batch  206/769   train_loss = 1.272
Epoch  86 Batch  216/769   train_loss = 1.228
Epoch  86 Batch  226/769   train_loss = 1.255
Epoch  86 Batch  236/769   train_loss = 1.275
Epoch  86 Batch  246/769   train_loss = 1.273
Epoch  86 Batch  256/769   train_loss = 1.268
Epoch  86 Batch  266/769   train_loss = 1.273
Epoch  86 Batch  276/769   train_loss = 1.257
Epoch  86 Batch  286/769   train_loss = 1.236
Epoch  86 Batch  296/769   train_loss = 1.193
Epoch  86 Batch  306/769   train_loss = 1.243
Epoch  86 Batch  316/769   train_loss = 1.222
Epoch  86 Batch  326/769   train_loss = 1.332
Epoch  86 Batch  336/769   train_loss = 1.274
Epoch  86 Batch  346/769   train_loss = 1.257
Epoch  86 Batch  356/769   train_loss = 1.236
Epoch  86 Batch  366/769   train_loss = 1.244
Epoch  86 Batch  376/769   train_loss = 1.229
Epoch  86 Batch  386/769   train_loss = 1.246
Epoch  86 Batch  396/769   train_loss = 1.321
Epoch  86 Batch  406/769   train_loss = 1.216
Epoch  86 Batch  416/769   train_loss = 1.250
Epoch  86 Batch  426/769   train_loss = 1.316
Epoch  86 Batch  436/769   train_loss = 1.251
Epoch  86 Batch  446/769   train_loss = 1.261
Epoch  86 Batch  456/769   train_loss = 1.325
Epoch  86 Batch  466/769   train_loss = 1.289
Epoch  86 Batch  476/769   train_loss = 1.239
Epoch  86 Batch  486/769   train_loss = 1.276
Epoch  86 Batch  496/769   train_loss = 1.238
Epoch  86 Batch  506/769   train_loss = 1.286
Epoch  86 Batch  516/769   train_loss = 1.229
Epoch  86 Batch  526/769   train_loss = 1.210
Epoch  86 Batch  536/769   train_loss = 1.227
Epoch  86 Batch  546/769   train_loss = 1.249
Epoch  86 Batch  556/769   train_loss = 1.324
Epoch  86 Batch  566/769   train_loss = 1.342
Epoch  86 Batch  576/769   train_loss = 1.210
Epoch  86 Batch  586/769   train_loss = 1.228
Epoch  86 Batch  596/769   train_loss = 1.306
Epoch  86 Batch  606/769   train_loss = 1.259
Epoch  86 Batch  616/769   train_loss = 1.224
Epoch  86 Batch  626/769   train_loss = 1.244
Epoch  86 Batch  636/769   train_loss = 1.210
Epoch  86 Batch  646/769   train_loss = 1.325
Epoch  86 Batch  656/769   train_loss = 1.216
Epoch  86 Batch  666/769   train_loss = 1.270
Epoch  86 Batch  676/769   train_loss = 1.286
Epoch  86 Batch  686/769   train_loss = 1.279
Epoch  86 Batch  696/769   train_loss = 1.230
Epoch  86 Batch  706/769   train_loss = 1.302
Epoch  86 Batch  716/769   train_loss = 1.254
Epoch  86 Batch  726/769   train_loss = 1.252
Epoch  86 Batch  736/769   train_loss = 1.291
Epoch  86 Batch  746/769   train_loss = 1.235
Epoch  86 Batch  756/769   train_loss = 1.234
Epoch  86 Batch  766/769   train_loss = 1.288
Epoch  87 Batch    7/769   train_loss = 1.278
Epoch  87 Batch   17/769   train_loss = 1.270
Epoch  87 Batch   27/769   train_loss = 1.218
Epoch  87 Batch   37/769   train_loss = 1.255
Epoch  87 Batch   47/769   train_loss = 1.241
Epoch  87 Batch   57/769   train_loss = 1.336
Epoch  87 Batch   67/769   train_loss = 1.287
Epoch  87 Batch   77/769   train_loss = 1.302
Epoch  87 Batch   87/769   train_loss = 1.287
Epoch  87 Batch   97/769   train_loss = 1.248
Epoch  87 Batch  107/769   train_loss = 1.264
Epoch  87 Batch  117/769   train_loss = 1.239
Epoch  87 Batch  127/769   train_loss = 1.318
Epoch  87 Batch  137/769   train_loss = 1.221
Epoch  87 Batch  147/769   train_loss = 1.262
Epoch  87 Batch  157/769   train_loss = 1.314
Epoch  87 Batch  167/769   train_loss = 1.251
Epoch  87 Batch  177/769   train_loss = 1.209
Epoch  87 Batch  187/769   train_loss = 1.293
Epoch  87 Batch  197/769   train_loss = 1.215
Epoch  87 Batch  207/769   train_loss = 1.175
Epoch  87 Batch  217/769   train_loss = 1.256
Epoch  87 Batch  227/769   train_loss = 1.236
Epoch  87 Batch  237/769   train_loss = 1.225
Epoch  87 Batch  247/769   train_loss = 1.263
Epoch  87 Batch  257/769   train_loss = 1.230
Epoch  87 Batch  267/769   train_loss = 1.263
Epoch  87 Batch  277/769   train_loss = 1.246
Epoch  87 Batch  287/769   train_loss = 1.247
Epoch  87 Batch  297/769   train_loss = 1.234
Epoch  87 Batch  307/769   train_loss = 1.295
Epoch  87 Batch  317/769   train_loss = 1.165
Epoch  87 Batch  327/769   train_loss = 1.282
Epoch  87 Batch  337/769   train_loss = 1.279
Epoch  87 Batch  347/769   train_loss = 1.240
Epoch  87 Batch  357/769   train_loss = 1.255
Epoch  87 Batch  367/769   train_loss = 1.283
Epoch  87 Batch  377/769   train_loss = 1.248
Epoch  87 Batch  387/769   train_loss = 1.314
Epoch  87 Batch  397/769   train_loss = 1.208
Epoch  87 Batch  407/769   train_loss = 1.275
Epoch  87 Batch  417/769   train_loss = 1.267
Epoch  87 Batch  427/769   train_loss = 1.281
Epoch  87 Batch  437/769   train_loss = 1.329
Epoch  87 Batch  447/769   train_loss = 1.304
Epoch  87 Batch  457/769   train_loss = 1.239
Epoch  87 Batch  467/769   train_loss = 1.268
Epoch  87 Batch  477/769   train_loss = 1.266
Epoch  87 Batch  487/769   train_loss = 1.240
Epoch  87 Batch  497/769   train_loss = 1.282
Epoch  87 Batch  507/769   train_loss = 1.256
Epoch  87 Batch  517/769   train_loss = 1.251
Epoch  87 Batch  527/769   train_loss = 1.233
Epoch  87 Batch  537/769   train_loss = 1.313
Epoch  87 Batch  547/769   train_loss = 1.298
Epoch  87 Batch  557/769   train_loss = 1.266
Epoch  87 Batch  567/769   train_loss = 1.274
Epoch  87 Batch  577/769   train_loss = 1.298
Epoch  87 Batch  587/769   train_loss = 1.249
Epoch  87 Batch  597/769   train_loss = 1.161
Epoch  87 Batch  607/769   train_loss = 1.240
Epoch  87 Batch  617/769   train_loss = 1.256
Epoch  87 Batch  627/769   train_loss = 1.297
Epoch  87 Batch  637/769   train_loss = 1.208
Epoch  87 Batch  647/769   train_loss = 1.295
Epoch  87 Batch  657/769   train_loss = 1.210
Epoch  87 Batch  667/769   train_loss = 1.286
Epoch  87 Batch  677/769   train_loss = 1.235
Epoch  87 Batch  687/769   train_loss = 1.311
Epoch  87 Batch  697/769   train_loss = 1.314
Epoch  87 Batch  707/769   train_loss = 1.271
Epoch  87 Batch  717/769   train_loss = 1.264
Epoch  87 Batch  727/769   train_loss = 1.268
Epoch  87 Batch  737/769   train_loss = 1.319
Epoch  87 Batch  747/769   train_loss = 1.210
Epoch  87 Batch  757/769   train_loss = 1.280
Epoch  87 Batch  767/769   train_loss = 1.281
Epoch  88 Batch    8/769   train_loss = 1.257
Epoch  88 Batch   18/769   train_loss = 1.262
Epoch  88 Batch   28/769   train_loss = 1.265
Epoch  88 Batch   38/769   train_loss = 1.273
Epoch  88 Batch   48/769   train_loss = 1.299
Epoch  88 Batch   58/769   train_loss = 1.268
Epoch  88 Batch   68/769   train_loss = 1.360
Epoch  88 Batch   78/769   train_loss = 1.264
Epoch  88 Batch   88/769   train_loss = 1.258
Epoch  88 Batch   98/769   train_loss = 1.229
Epoch  88 Batch  108/769   train_loss = 1.305
Epoch  88 Batch  118/769   train_loss = 1.263
Epoch  88 Batch  128/769   train_loss = 1.283
Epoch  88 Batch  138/769   train_loss = 1.219
Epoch  88 Batch  148/769   train_loss = 1.279
Epoch  88 Batch  158/769   train_loss = 1.297
Epoch  88 Batch  168/769   train_loss = 1.173
Epoch  88 Batch  178/769   train_loss = 1.296
Epoch  88 Batch  188/769   train_loss = 1.252
Epoch  88 Batch  198/769   train_loss = 1.252
Epoch  88 Batch  208/769   train_loss = 1.257
Epoch  88 Batch  218/769   train_loss = 1.272
Epoch  88 Batch  228/769   train_loss = 1.229
Epoch  88 Batch  238/769   train_loss = 1.267
Epoch  88 Batch  248/769   train_loss = 1.298
Epoch  88 Batch  258/769   train_loss = 1.246
Epoch  88 Batch  268/769   train_loss = 1.255
Epoch  88 Batch  278/769   train_loss = 1.292
Epoch  88 Batch  288/769   train_loss = 1.247
Epoch  88 Batch  298/769   train_loss = 1.293
Epoch  88 Batch  308/769   train_loss = 1.265
Epoch  88 Batch  318/769   train_loss = 1.234
Epoch  88 Batch  328/769   train_loss = 1.246
Epoch  88 Batch  338/769   train_loss = 1.246
Epoch  88 Batch  348/769   train_loss = 1.215
Epoch  88 Batch  358/769   train_loss = 1.277
Epoch  88 Batch  368/769   train_loss = 1.286
Epoch  88 Batch  378/769   train_loss = 1.247
Epoch  88 Batch  388/769   train_loss = 1.221
Epoch  88 Batch  398/769   train_loss = 1.298
Epoch  88 Batch  408/769   train_loss = 1.274
Epoch  88 Batch  418/769   train_loss = 1.240
Epoch  88 Batch  428/769   train_loss = 1.225
Epoch  88 Batch  438/769   train_loss = 1.210
Epoch  88 Batch  448/769   train_loss = 1.293
Epoch  88 Batch  458/769   train_loss = 1.336
Epoch  88 Batch  468/769   train_loss = 1.258
Epoch  88 Batch  478/769   train_loss = 1.165
Epoch  88 Batch  488/769   train_loss = 1.266
Epoch  88 Batch  498/769   train_loss = 1.262
Epoch  88 Batch  508/769   train_loss = 1.254
Epoch  88 Batch  518/769   train_loss = 1.281
Epoch  88 Batch  528/769   train_loss = 1.200
Epoch  88 Batch  538/769   train_loss = 1.259
Epoch  88 Batch  548/769   train_loss = 1.250
Epoch  88 Batch  558/769   train_loss = 1.271
Epoch  88 Batch  568/769   train_loss = 1.239
Epoch  88 Batch  578/769   train_loss = 1.138
Epoch  88 Batch  588/769   train_loss = 1.185
Epoch  88 Batch  598/769   train_loss = 1.229
Epoch  88 Batch  608/769   train_loss = 1.191
Epoch  88 Batch  618/769   train_loss = 1.182
Epoch  88 Batch  628/769   train_loss = 1.264
Epoch  88 Batch  638/769   train_loss = 1.234
Epoch  88 Batch  648/769   train_loss = 1.252
Epoch  88 Batch  658/769   train_loss = 1.316
Epoch  88 Batch  668/769   train_loss = 1.238
Epoch  88 Batch  678/769   train_loss = 1.243
Epoch  88 Batch  688/769   train_loss = 1.265
Epoch  88 Batch  698/769   train_loss = 1.226
Epoch  88 Batch  708/769   train_loss = 1.246
Epoch  88 Batch  718/769   train_loss = 1.213
Epoch  88 Batch  728/769   train_loss = 1.258
Epoch  88 Batch  738/769   train_loss = 1.268
Epoch  88 Batch  748/769   train_loss = 1.277
Epoch  88 Batch  758/769   train_loss = 1.246
Epoch  88 Batch  768/769   train_loss = 1.263
Epoch  89 Batch    9/769   train_loss = 1.252
Epoch  89 Batch   19/769   train_loss = 1.325
Epoch  89 Batch   29/769   train_loss = 1.254
Epoch  89 Batch   39/769   train_loss = 1.230
Epoch  89 Batch   49/769   train_loss = 1.230
Epoch  89 Batch   59/769   train_loss = 1.263
Epoch  89 Batch   69/769   train_loss = 1.283
Epoch  89 Batch   79/769   train_loss = 1.268
Epoch  89 Batch   89/769   train_loss = 1.211
Epoch  89 Batch   99/769   train_loss = 1.275
Epoch  89 Batch  109/769   train_loss = 1.295
Epoch  89 Batch  119/769   train_loss = 1.202
Epoch  89 Batch  129/769   train_loss = 1.182
Epoch  89 Batch  139/769   train_loss = 1.262
Epoch  89 Batch  149/769   train_loss = 1.246
Epoch  89 Batch  159/769   train_loss = 1.213
Epoch  89 Batch  169/769   train_loss = 1.270
Epoch  89 Batch  179/769   train_loss = 1.227
Epoch  89 Batch  189/769   train_loss = 1.247
Epoch  89 Batch  199/769   train_loss = 1.276
Epoch  89 Batch  209/769   train_loss = 1.244
Epoch  89 Batch  219/769   train_loss = 1.229
Epoch  89 Batch  229/769   train_loss = 1.248
Epoch  89 Batch  239/769   train_loss = 1.202
Epoch  89 Batch  249/769   train_loss = 1.190
Epoch  89 Batch  259/769   train_loss = 1.197
Epoch  89 Batch  269/769   train_loss = 1.230
Epoch  89 Batch  279/769   train_loss = 1.265
Epoch  89 Batch  289/769   train_loss = 1.221
Epoch  89 Batch  299/769   train_loss = 1.326
Epoch  89 Batch  309/769   train_loss = 1.262
Epoch  89 Batch  319/769   train_loss = 1.300
Epoch  89 Batch  329/769   train_loss = 1.273
Epoch  89 Batch  339/769   train_loss = 1.246
Epoch  89 Batch  349/769   train_loss = 1.225
Epoch  89 Batch  359/769   train_loss = 1.290
Epoch  89 Batch  369/769   train_loss = 1.223
Epoch  89 Batch  379/769   train_loss = 1.178
Epoch  89 Batch  389/769   train_loss = 1.162
Epoch  89 Batch  399/769   train_loss = 1.279
Epoch  89 Batch  409/769   train_loss = 1.274
Epoch  89 Batch  419/769   train_loss = 1.349
Epoch  89 Batch  429/769   train_loss = 1.221
Epoch  89 Batch  439/769   train_loss = 1.288
Epoch  89 Batch  449/769   train_loss = 1.282
Epoch  89 Batch  459/769   train_loss = 1.255
Epoch  89 Batch  469/769   train_loss = 1.192
Epoch  89 Batch  479/769   train_loss = 1.272
Epoch  89 Batch  489/769   train_loss = 1.335
Epoch  89 Batch  499/769   train_loss = 1.295
Epoch  89 Batch  509/769   train_loss = 1.240
Epoch  89 Batch  519/769   train_loss = 1.295
Epoch  89 Batch  529/769   train_loss = 1.312
Epoch  89 Batch  539/769   train_loss = 1.299
Epoch  89 Batch  549/769   train_loss = 1.304
Epoch  89 Batch  559/769   train_loss = 1.286
Epoch  89 Batch  569/769   train_loss = 1.176
Epoch  89 Batch  579/769   train_loss = 1.186
Epoch  89 Batch  589/769   train_loss = 1.237
Epoch  89 Batch  599/769   train_loss = 1.215
Epoch  89 Batch  609/769   train_loss = 1.235
Epoch  89 Batch  619/769   train_loss = 1.287
Epoch  89 Batch  629/769   train_loss = 1.236
Epoch  89 Batch  639/769   train_loss = 1.195
Epoch  89 Batch  649/769   train_loss = 1.191
Epoch  89 Batch  659/769   train_loss = 1.219
Epoch  89 Batch  669/769   train_loss = 1.250
Epoch  89 Batch  679/769   train_loss = 1.177
Epoch  89 Batch  689/769   train_loss = 1.295
Epoch  89 Batch  699/769   train_loss = 1.210
Epoch  89 Batch  709/769   train_loss = 1.290
Epoch  89 Batch  719/769   train_loss = 1.252
Epoch  89 Batch  729/769   train_loss = 1.222
Epoch  89 Batch  739/769   train_loss = 1.212
Epoch  89 Batch  749/769   train_loss = 1.213
Epoch  89 Batch  759/769   train_loss = 1.233
Model Trained and Saved
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Save-Parameters">Save Parameters<a class="anchor-link" href="#Save-Parameters">&#182;</a></h2><p>Save <code>seq_length</code> and <code>save_dir</code> for generating a new TV script.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[31]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="c1"># Save parameters for checkpoint</span>
<span class="n">helper</span><span class="o">.</span><span class="n">save_params</span><span class="p">((</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Checkpoint">Checkpoint<a class="anchor-link" href="#Checkpoint">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[32]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">helper</span>
<span class="kn">import</span> <span class="nn">problem_unittests</span> <span class="k">as</span> <span class="nn">tests</span>

<span class="n">_</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">,</span> <span class="n">int_to_vocab</span><span class="p">,</span> <span class="n">token_dict</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">load_preprocess</span><span class="p">()</span>
<span class="n">seq_length</span><span class="p">,</span> <span class="n">load_dir</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">load_params</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Implement-Generate-Functions">Implement Generate Functions<a class="anchor-link" href="#Implement-Generate-Functions">&#182;</a></h2><h3 id="Get-Tensors">Get Tensors<a class="anchor-link" href="#Get-Tensors">&#182;</a></h3><p>Get tensors from <code>loaded_graph</code> using the function <a href="https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name"><code>get_tensor_by_name()</code></a>.  Get the tensors using the following names:</p>
<ul>
<li>"input:0"</li>
<li>"initial_state:0"</li>
<li>"final_state:0"</li>
<li>"probs:0"</li>
</ul>
<p>Return the tensors in the following tuple <code>(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[33]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_tensors</span><span class="p">(</span><span class="n">loaded_graph</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get input, initial state, final state, and probabilities tensor from &lt;loaded_graph&gt;</span>
<span class="sd">    :param loaded_graph: TensorFlow graph loaded from file</span>
<span class="sd">    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
          
    <span class="n">InputTensor</span> <span class="o">=</span> <span class="n">loaded_graph</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s1">&#39;input:0&#39;</span><span class="p">)</span>
    <span class="n">InitialStateTensor</span> <span class="o">=</span> <span class="n">loaded_graph</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s1">&#39;initial_state:0&#39;</span><span class="p">)</span>
    <span class="n">FinalStateTensor</span> <span class="o">=</span> <span class="n">loaded_graph</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s1">&#39;final_state:0&#39;</span><span class="p">)</span>
    <span class="n">ProbsTensor</span> <span class="o">=</span> <span class="n">loaded_graph</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s1">&#39;probs:0&#39;</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">InputTensor</span><span class="p">,</span> <span class="n">InitialStateTensor</span><span class="p">,</span> <span class="n">FinalStateTensor</span><span class="p">,</span> <span class="n">ProbsTensor</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_get_tensors</span><span class="p">(</span><span class="n">get_tensors</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Choose-Word">Choose Word<a class="anchor-link" href="#Choose-Word">&#182;</a></h3><p>Implement the <code>pick_word()</code> function to select the next word using <code>probabilities</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[34]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">pick_word</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">int_to_vocab</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pick the next word in the generated text</span>
<span class="sd">    :param probabilitieprobabilitiess: Probabilites of the next word</span>
<span class="sd">    :param int_to_vocab: Dictionary of word ids as the keys and words as the values</span>
<span class="sd">    :return: String of the predicted word</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
    
       
    <span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">int_to_vocab</span><span class="p">)),</span> <span class="n">p</span><span class="o">=</span><span class="n">probabilities</span><span class="p">)</span>
    <span class="c1"># print(index)</span>
   
    <span class="k">return</span> <span class="n">int_to_vocab</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_pick_word</span><span class="p">(</span><span class="n">pick_word</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Generate-TV-Script">Generate TV Script<a class="anchor-link" href="#Generate-TV-Script">&#182;</a></h2><p>This will generate the TV script for you.  Set <code>gen_length</code> to the length of TV script you want to generate.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[50]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">gen_length</span> <span class="o">=</span> <span class="mi">500</span>
<span class="c1"># homer_simpson, moe_szyslak, or Barney_Gumble</span>
<span class="n">prime_word</span> <span class="o">=</span> <span class="s1">&#39;sideshow_bob&#39;</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">loaded_graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">loaded_graph</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="c1"># Load saved model</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">import_meta_graph</span><span class="p">(</span><span class="n">load_dir</span> <span class="o">+</span> <span class="s1">&#39;.meta&#39;</span><span class="p">)</span>
    <span class="n">loader</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">load_dir</span><span class="p">)</span>

    <span class="c1"># Get Tensors from loaded model</span>
    <span class="n">input_text</span><span class="p">,</span> <span class="n">initial_state</span><span class="p">,</span> <span class="n">final_state</span><span class="p">,</span> <span class="n">probs</span> <span class="o">=</span> <span class="n">get_tensors</span><span class="p">(</span><span class="n">loaded_graph</span><span class="p">)</span>

    <span class="c1"># Sentences generation setup</span>
    <span class="n">gen_sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">prime_word</span> <span class="o">+</span> <span class="s1">&#39;:&#39;</span><span class="p">]</span>
    <span class="n">prev_state</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">initial_state</span><span class="p">,</span> <span class="p">{</span><span class="n">input_text</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])})</span>

    <span class="c1"># Generate sentences</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gen_length</span><span class="p">):</span>
        <span class="c1"># Dynamic Input</span>
        <span class="n">dyn_input</span> <span class="o">=</span> <span class="p">[[</span><span class="n">vocab_to_int</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">gen_sentences</span><span class="p">[</span><span class="o">-</span><span class="n">seq_length</span><span class="p">:]]]</span>
        <span class="n">dyn_seq_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dyn_input</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="c1"># Get Prediction</span>
        <span class="n">probabilities</span><span class="p">,</span> <span class="n">prev_state</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
            <span class="p">[</span><span class="n">probs</span><span class="p">,</span> <span class="n">final_state</span><span class="p">],</span>
            <span class="p">{</span><span class="n">input_text</span><span class="p">:</span> <span class="n">dyn_input</span><span class="p">,</span> <span class="n">initial_state</span><span class="p">:</span> <span class="n">prev_state</span><span class="p">})</span>
        
        <span class="n">pred_word</span> <span class="o">=</span> <span class="n">pick_word</span><span class="p">(</span><span class="n">probabilities</span><span class="p">[</span><span class="n">dyn_seq_length</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">int_to_vocab</span><span class="p">)</span>

        <span class="n">gen_sentences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_word</span><span class="p">)</span>
    
    <span class="c1"># Remove tokens</span>
    <span class="n">tv_script</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">gen_sentences</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">token_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">ending</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span> <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;(&#39;</span><span class="p">,</span> <span class="s1">&#39;&quot;&#39;</span><span class="p">]</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="n">tv_script</span> <span class="o">=</span> <span class="n">tv_script</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">token</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="n">key</span><span class="p">)</span>
    <span class="n">tv_script</span> <span class="o">=</span> <span class="n">tv_script</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> &#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">tv_script</span> <span class="o">=</span> <span class="n">tv_script</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;( &#39;</span><span class="p">,</span> <span class="s1">&#39;(&#39;</span><span class="p">)</span>
        
    <span class="nb">print</span><span class="p">(</span><span class="n">tv_script</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>sideshow_bob: oh, yeah. hey, i could be flying there.
martin_prince:(flipping through six) this means is this with first time at the whole school...
barney_gumble: weee!
isabel_gutierrez: hm, but uh...
homer_simpson:(impulsively) now, where can i go or something?
female_scientist:(suspicious noise)... you&#39;re welcome... all for comin&#39; under the office.
selma_bouvier:(ecstatic) this is where the end is chocolate!
female_prisoners:(hospital)--(screams)
c. _montgomery_burns:(anguished noise) we should drive to that number. just look at everything about you, the&#34; wow,&#34; and i know it&#39;s good tonight. if i wanted to tell&#34; a&#34; burns in your mouth.&#34; you know&#34; what a wife has to do&#34; h. b.&#34; wonder what else?&#34; what?
professor_jonathan_frink: i would love this
&#34; but&#34; the year a ticket is the perfect choice&#34;, i&#39;m supposed to help me say my name isn&#39;t&#34; i.&#34; so you could turn to the ball with selma and picture?
sideshow_bob: no i don&#39;t. because we&#39;re about to break loose. when i ordered a woman take my heart, which happens to himself.
homer_simpson: haw haw, you&#39;re right!
chief_wiggum: i&#39;m sorry, kitty. the only reason i would are not in the pink abandoned card in our town.
kent_brockman: a sign of rock di
c. _montgomery_burns: oh, look at this place, you can be friends from fast as or when you see it?
homer_simpson:(from inside clothes) uh-huh.
marge_simpson: sweetie, you were unlucky in your first last supper. i am a doctor! he couldn&#39;t find a house in my life(mimes first) of course.&#34;
waylon_smithers: yes sir.
c. _montgomery_burns: just take the business pills.
lisa_simpson: no, bart.(sits down) just a second, honey.(sighs)
simpson_home: int. simpson house - attic - day)
marge_simpson: well, desserts is who you gave yourself inside the entire lottery of an beautiful dream i gave us this way?
comic_book_guy: we are all gonna enjoy the car and make it another... i...
homer_simpson: can&#39;t you do it, dr. hibbert?
homer_simpson: probably play the best.(loud scream you&#39;re a little upset mean


homer_simpson: dad.
homer_simpson: oh, yes, well... but i was laying into the dentist...
homer_simpson:(sighs, gets to remember)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="The-TV-Script-is-Nonsensical">The TV Script is Nonsensical<a class="anchor-link" href="#The-TV-Script-is-Nonsensical">&#182;</a></h1><p>It's ok if the TV script doesn't make any sense.  We trained on less than a megabyte of text.  In order to get good results, you'll have to use a smaller vocabulary or get more data.  Luckly there's more data!  As we mentioned in the begging of this project, this is a subset of <a href="https://www.kaggle.com/wcukierski/the-simpsons-by-the-data">another dataset</a>.  We didn't have you train on all the data, because that would take too long.  However, you are free to train your neural network on all the data.  After you complete the project, of course.</p>
<h1 id="Submitting-This-Project">Submitting This Project<a class="anchor-link" href="#Submitting-This-Project">&#182;</a></h1><p>When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as "dlnd_tv_script_generation.ipynb" and save it as a HTML file under "File" -&gt; "Download as". Include the "helper.py" and "problem_unittests.py" files in your submission.</p>

</div>
</div>
</div>
    </div>
  </div>
</body>

 


</html>
